{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(**Click the icon below to open this notebook in Colab**)\n",
    "\n",
    "[![Open InColab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/xiangshiyin/machine-learning-for-actuarial-science/blob/main/2025-spring/week15/notebook/demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/xiangshiyin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/xiangshiyin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/xiangshiyin/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\n",
      "drwxr-xr-x  6 xiangshiyin  staff  192 Apr 13 23:47 \u001b[34mcorpora\u001b[m\u001b[m\n",
      "drwxr-xr-x  6 xiangshiyin  staff  192 Apr 12 17:38 \u001b[34mtokenizers\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "ls -l /Users/xiangshiyin/nltk_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "data = \"This is a simple example to demonstrate removing stopwords using NLTK.\"\n",
    "stopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = word_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: This is a simple example to demonstrate removing stopwords using NLTK.\n",
      "Tokenized text: This|is|a|simple|example|to|demonstrate|removing|stopwords|using|NLTK|.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original text: {data}\")\n",
    "print(f\"Tokenized text: {\"|\".join(tokenized_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: This is a simple example to demonstrate removing stopwords using NLTK. I'll be using Python to do the data processing. Please check the output.\n",
      "Tokenized text: This|is|a|simple|example|to|demonstrate|removing|stopwords|using|NLTK|.|I|'ll|be|using|Python|to|do|the|data|processing|.|Please|check|the|output|.\n"
     ]
    }
   ],
   "source": [
    "data2 = \"This is a simple example to demonstrate removing stopwords using NLTK. I'll be using Python to do the data processing. Please check the output.\"\n",
    "tokenized_data2 = word_tokenize(data2)\n",
    "print(f\"Original text: {data2}\")\n",
    "print(f\"Tokenized text: {\"|\".join(tokenized_data2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing stopwords: ['This', 'simple', 'example', 'demonstrate', 'removing', 'stopwords', 'using', 'NLTK', '.']\n"
     ]
    }
   ],
   "source": [
    "filtered_tokenized_data = [\n",
    "    w\n",
    "    for w in tokenized_data\n",
    "    if w not in stopWords\n",
    "]\n",
    "print(f\"After removing stopwords: {filtered_tokenized_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: This is a simple example to demonstrate removing stopwords using NLTK.\n",
      "Tokenized text: This|is|a|simple|example|to|demonstrate|removing|stopwords|using|NLTK|.\n",
      "After removing stopwords: This|simple|example|demonstrate|removing|stopwords|using|NLTK|.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original text: {data}\")\n",
    "print(f\"Tokenized text: {\"|\".join(tokenized_data)}\")\n",
    "print(f\"After removing stopwords: {\"|\".join(filtered_tokenized_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names (Vocabulary):\n",
      "['all' 'amazing' 'at' 'ever' 'experience' 'happy' 'hate' 'is' 'love' 'not'\n",
      " 'product' 'result' 'satisfied' 'the' 'this' 'very' 'with' 'worst']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Sample dataset\n",
    "texts = [\n",
    "    \"I love this product product product\",         # positive\n",
    "    \"This is amazing\",             # positive\n",
    "    \"Very happy with the result\",  # positive\n",
    "    \"I hate this\",                 # negative\n",
    "    \"Worst experience ever\",       # negative\n",
    "    \"Not satisfied at all\"         # negative\n",
    "]\n",
    "\n",
    "labels = [1, 1, 1, 0, 0, 0]  # 1 = positive, 0 = negative\n",
    "\n",
    "# 2. Convert text to bag-of-words vectors\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# 3. Show feature names\n",
    "print(\"Feature Names (Vocabulary):\")\n",
    "print(vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 18)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/xiangshiyin/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8275\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.81      0.82       199\n",
      "           1       0.82      0.84      0.83       201\n",
      "\n",
      "    accuracy                           0.83       400\n",
      "   macro avg       0.83      0.83      0.83       400\n",
      "weighted avg       0.83      0.83      0.83       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import nltk\n",
    "\n",
    "# Download NLTK movie_reviews data\n",
    "nltk.download('movie_reviews')\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# Prepare dataset\n",
    "docs = []\n",
    "labels = []\n",
    "\n",
    "for fileid in movie_reviews.fileids():\n",
    "    docs.append(movie_reviews.raw(fileid))\n",
    "    labels.append(movie_reviews.categories(fileid)[0])  # 'pos' or 'neg'\n",
    "\n",
    "# Convert labels to binary format\n",
    "y = [1 if label == 'pos' else 0 for label in labels]\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(docs, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train classifier\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00' '000' '0009f' ... 'zwigoff' 'zycie' 'zzzzzzz']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(feature_names)\n",
    "# feature_names[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '0009f', '007', '00s', '03', '04', '05', '05425',\n",
       "       '10', '100', '1000', '10000', '100m', '101', '102', '103', '104',\n",
       "       '105', '106'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35940"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plot : two teen couples go to a church party , drink and then drive . \\nthey get into an accident . \\none of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . \\nwhat\\'s the deal ? \\nwatch the movie and \" sorta \" find out . . . \\ncritique : a mind-fuck movie for the teen generation that touches on a very cool idea , but presents it in a very bad package . \\nwhich is what makes this review an even harder one to write , since i generally applaud films which attempt to break the mold , mess with your head and such ( lost highway & memento ) , but there are good and bad ways of making all types of films , and these folks just didn\\'t snag this one correctly . \\nthey seem to have taken this pretty neat concept , but executed it terribly . \\nso what are the problems with the movie ? \\nwell , its main problem is that it\\'s simply too jumbled . \\nit starts off \" normal \" but then downshifts into this \" fantasy \" world in which you , as an audience member , have no idea what\\'s going on . \\nthere are dreams , there are characters coming back from the dead , there are others who look like the dead , there are strange apparitions , there are disappearances , there are a looooot of chase scenes , there are tons of weird things that happen , and most of it is simply not explained . \\nnow i personally don\\'t mind trying to unravel a film every now and then , but when all it does is give me the same clue over and over again , i get kind of fed up after a while , which is this film\\'s biggest problem . \\nit\\'s obviously got this big secret to hide , but it seems to want to hide it completely until its final five minutes . \\nand do they make things entertaining , thrilling or even engaging , in the meantime ? \\nnot really . \\nthe sad part is that the arrow and i both dig on flicks like this , so we actually figured most of it out by the half-way point , so all of the strangeness after that did start to make a little bit of sense , but it still didn\\'t the make the film all that more entertaining . \\ni guess the bottom line with movies like this is that you should always make sure that the audience is \" into it \" even before they are given the secret password to enter your world of understanding . \\ni mean , showing melissa sagemiller running away from visions for about 20 minutes throughout the movie is just plain lazy ! ! \\nokay , we get it . . . there \\nare people chasing her and we don\\'t know who they are . \\ndo we really need to see it over and over again ? \\nhow about giving us different scenes offering further insight into all of the strangeness going down in the movie ? \\napparently , the studio took this film away from its director and chopped it up themselves , and it shows . \\nthere might\\'ve been a pretty decent teen mind-fuck movie in here somewhere , but i guess \" the suits \" decided that turning it into a music video with little edge , would make more sense . \\nthe actors are pretty good for the most part , although wes bentley just seemed to be playing the exact same character that he did in american beauty , only in a new neighborhood . \\nbut my biggest kudos go out to sagemiller , who holds her own throughout the entire film , and actually has you feeling her character\\'s unraveling . \\noverall , the film doesn\\'t stick because it doesn\\'t entertain , it\\'s confusing , it rarely excites and it feels pretty redundant for most of its runtime , despite a pretty cool ending and explanation to all of the craziness that came before it . \\noh , and by the way , this is not a horror or teen slasher flick . . . it\\'s \\njust packaged to look that way because someone is apparently assuming that the genre is still hot with the kids . \\nit also wrapped production two years ago and has been sitting on the shelves ever since . \\nwhatever . . . skip \\nit ! \\nwhere\\'s joblo coming from ? \\na nightmare of elm street 3 ( 7/10 ) - blair witch 2 ( 7/10 ) - the crow ( 9/10 ) - the crow : salvation ( 4/10 ) - lost highway ( 10/10 ) - memento ( 10/10 ) - the others ( 9/10 ) - stir of echoes ( 8/10 ) \\n']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Choose a sample document from the test set\n",
    "sample_idx = 0\n",
    "sample_vector = X_test_tfidf[sample_idx]\n",
    "\n",
    "# Convert sparse vector to dense and create DataFrame\n",
    "df_features = pd.DataFrame(\n",
    "    data=sample_vector.toarray()[0],\n",
    "    index=feature_names,\n",
    "    columns=[\"tfidf\"]\n",
    ")\n",
    "\n",
    "# Filter non-zero features and sort\n",
    "df_nonzero = df_features[df_features.tfidf > 0].sort_values(by=\"tfidf\", ascending=False)\n",
    "\n",
    "# Show top 15 features by TF-IDF weight\n",
    "print(\"\\nTop TF-IDF features in sample test document:\")\n",
    "print(df_nonzero.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hand-craft implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Sample corpus\n",
    "corpus = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# Preprocessing: Tokenization and vocabulary building\n",
    "tokens = re.findall(r'\\b\\w+\\b', corpus.lower())\n",
    "vocab = set(tokens)\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lazy': 0,\n",
       " 'jumps': 1,\n",
       " 'the': 2,\n",
       " 'over': 3,\n",
       " 'dog': 4,\n",
       " 'fox': 5,\n",
       " 'brown': 6,\n",
       " 'quick': 7}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'lazy',\n",
       " 1: 'jumps',\n",
       " 2: 'the',\n",
       " 3: 'over',\n",
       " 4: 'dog',\n",
       " 5: 'fox',\n",
       " 6: 'brown',\n",
       " 7: 'quick'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data\n",
    "def generate_training_data(tokens, window_size):\n",
    "    training_data = []\n",
    "    for idx, target_word in enumerate(tokens):\n",
    "        target_idx = word_to_idx[target_word]\n",
    "        context_range = list(range(max(0, idx - window_size), idx)) + \\\n",
    "                        list(range(idx + 1, min(len(tokens), idx + window_size + 1)))\n",
    "        for context_idx in context_range:\n",
    "            context_word = tokens[context_idx]\n",
    "            context_word_idx = word_to_idx[context_word]\n",
    "            training_data.append((target_idx, context_word_idx))\n",
    "    return training_data\n",
    "\n",
    "window_size = 2\n",
    "training_data = generate_training_data(tokens, window_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: The quick brown fox jumps over the lazy dog\n",
      "[('the', 'quick'), ('the', 'brown'), ('quick', 'the'), ('quick', 'brown'), ('quick', 'fox'), ('brown', 'the'), ('brown', 'quick'), ('brown', 'fox'), ('brown', 'jumps'), ('fox', 'quick'), ('fox', 'brown'), ('fox', 'jumps'), ('fox', 'over'), ('jumps', 'brown'), ('jumps', 'fox'), ('jumps', 'over'), ('jumps', 'the'), ('over', 'fox'), ('over', 'jumps'), ('over', 'the'), ('over', 'lazy'), ('the', 'jumps'), ('the', 'over'), ('the', 'lazy'), ('the', 'dog'), ('lazy', 'over'), ('lazy', 'the'), ('lazy', 'dog'), ('dog', 'the'), ('dog', 'lazy')]\n"
     ]
    }
   ],
   "source": [
    "# Inspect the training data\n",
    "print(f\"Corpus: {corpus}\")\n",
    "print([\n",
    "    (idx_to_word[t[0]], idx_to_word[t[1]])\n",
    "    for t in training_data\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 45.3056\n",
      "Epoch 200, Loss: 43.4334\n",
      "Epoch 300, Loss: 44.7213\n",
      "Epoch 400, Loss: 43.2369\n",
      "Epoch 500, Loss: 37.0080\n",
      "Epoch 600, Loss: 45.4319\n",
      "Epoch 700, Loss: 39.8265\n",
      "Epoch 800, Loss: 40.0149\n",
      "Epoch 900, Loss: 39.4416\n",
      "Epoch 1000, Loss: 37.7593\n"
     ]
    }
   ],
   "source": [
    "# Initialize parameters\n",
    "embedding_dim = 10\n",
    "W1 = np.random.randn(vocab_size, embedding_dim)\n",
    "W2 = np.random.randn(embedding_dim, vocab_size)\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Training parameters\n",
    "epochs = 1000\n",
    "learning_rate = 0.01\n",
    "num_negative_samples = 2\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    for target_idx, context_idx in training_data:\n",
    "        # Positive sample\n",
    "        h = W1[target_idx]\n",
    "        u = np.dot(h, W2[:, context_idx])\n",
    "        pred = sigmoid(u)\n",
    "        error = pred - 1\n",
    "        loss += -np.log(pred + 1e-7)\n",
    "        # Gradients\n",
    "        grad_W2 = error * h\n",
    "        grad_W1 = error * W2[:, context_idx]\n",
    "        # Update weights\n",
    "        W2[:, context_idx] -= learning_rate * grad_W2\n",
    "        W1[target_idx] -= learning_rate * grad_W1\n",
    "\n",
    "        # Negative sampling\n",
    "        negative_samples = random.sample([i for i in range(vocab_size) if i != context_idx], num_negative_samples)\n",
    "        for neg_idx in negative_samples:\n",
    "            u_neg = np.dot(h, W2[:, neg_idx])\n",
    "            pred_neg = sigmoid(u_neg)\n",
    "            error_neg = pred_neg\n",
    "            loss += -np.log(1 - pred_neg + 1e-7)\n",
    "            # Gradients\n",
    "            grad_W2_neg = error_neg * h\n",
    "            grad_W1_neg = error_neg * W2[:, neg_idx]\n",
    "            # Update weights\n",
    "            W2[:, neg_idx] -= learning_rate * grad_W2_neg\n",
    "            W1[target_idx] -= learning_rate * grad_W1_neg\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Words similar to 'fox':\n",
      "brown: 0.3096\n",
      "the: 0.2008\n",
      "jumps: 0.1507\n"
     ]
    }
   ],
   "source": [
    "# Retrieve word embeddings\n",
    "word_embeddings = W1\n",
    "\n",
    "# Example: Find similar words\n",
    "def find_similar(word, top_n=3):\n",
    "    if word not in word_to_idx:\n",
    "        print(f\"'{word}' not in vocabulary.\")\n",
    "        return\n",
    "    idx = word_to_idx[word]\n",
    "    vec = word_embeddings[idx]\n",
    "    similarities = []\n",
    "    for i in range(vocab_size):\n",
    "        if i == idx:\n",
    "            continue\n",
    "        sim = np.dot(vec, word_embeddings[i]) / (np.linalg.norm(vec) * np.linalg.norm(word_embeddings[i]))\n",
    "        similarities.append((idx_to_word[i], sim))\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    for word, sim in similarities[:top_n]:\n",
    "        print(f\"{word}: {sim:.4f}\")\n",
    "\n",
    "# Test the model\n",
    "print(\"\\nWords similar to 'fox':\")\n",
    "find_similar('fox')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With `Gensim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample corpus\n",
    "sentences = [\n",
    "    [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"],\n",
    "    [\"i\", \"love\", \"natural\", \"language\", \"processing\"],\n",
    "    [\"word2vec\", \"is\", \"a\", \"technique\", \"for\", \"natural\", \"language\", \"processing\"],\n",
    "    [\"the\", \"dog\", \"is\", \"lazy\", \"but\", \"the\", \"brown\", \"fox\", \"is\", \"quick\"]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the model\n",
    "model = Word2Vec(\n",
    "    sentences,\n",
    "    vector_size=100,  # Dimensionality of the word vectors\n",
    "    window=5,         # Maximum distance between the current and predicted word\n",
    "    min_count=1,      # Ignores all words with total frequency lower than this\n",
    "    workers=4,        # Use these many worker threads to train the model\n",
    "    sg=1              # 1 for Skip-gram; 0 for CBOW\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('over', 0.16699621081352234), ('brown', 0.1388736069202423), ('quick', 0.13150502741336823)]\n"
     ]
    }
   ],
   "source": [
    "# Find most similar words\n",
    "similar_words = model.wv.most_similar(\"fox\", topn=3)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'dog' and 'fox': -0.1052\n"
     ]
    }
   ],
   "source": [
    "# Compute similarity between two words\n",
    "similarity = model.wv.similarity(\"dog\", \"fox\")\n",
    "print(f\"Similarity between 'dog' and 'fox': {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"\"\"\n",
    "Mr. Dursley was the director of a firm called Grunnings, which made\n",
    "drills. He was a big, beefy man with hardly any neck, although he did\n",
    "have a very large mustache. Mrs. Dursley was thin and blonde and had\n",
    "nearly twice the usual amount of neck, which came in very useful as she\n",
    "spent so much of her time craning over garden fences, spying on the\n",
    "neighbors. The Dursleys had a small son called Dudley and in their\n",
    "opinion there was no finer boy anywhere.\n",
    "\n",
    "\n",
    "The Dursleys had everything they wanted, but they also had a secret, and\n",
    "their greatest fear was that somebody would discover it. They didn't\n",
    "think they could bear it if anyone found out about the Potters. Mrs.\n",
    "Potter was Mrs. Dursley's sister, but they hadn't met for several years;\n",
    "in fact, Mrs. Dursley pretended she didn't have a sister, because her\n",
    "sister and her good-for-nothing husband were as unDursleyish as it was\n",
    "possible to be. The Dursleys shuddered to think what the neighbors would\n",
    "say if the Potters arrived in the street. The Dursleys knew that the\n",
    "Potters had a small son, too, but they had never even seen him. This boy\n",
    "was another good reason for keeping the Potters away; they didn't want\n",
    "Dudley mixing with a child like that.\n",
    "\n",
    "\n",
    "When Mr. and Mrs. Dursley woke up on the dull, gray Tuesday our story\n",
    "starts, there was nothing about the cloudy sky outside to suggest that\n",
    "strange and mysterious things would soon be happening all over the\n",
    "country. Mr. Dursley hummed as he picked out his most boring tie for\n",
    "work, and Mrs. Dursley gossiped away happily as she wrestled a screaming\n",
    "Dudley into his high chair.\n",
    "\"\"\"\n",
    "\n",
    "sentences = [\n",
    "    gensim.utils.simple_preprocess(sentence)\n",
    "    for sentence in sample.split(\"\\n\\n\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(\n",
    "    sentences,\n",
    "    vector_size=100,  # Dimensionality of the word vectors\n",
    "    window=5,         # Maximum distance between the current and predicted word\n",
    "    min_count=1,      # Ignores all words with total frequency lower than this\n",
    "    workers=4,        # Use these many worker threads to train the model\n",
    "    sg=1              # 1 for Skip-gram; 0 for CBOW\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dursley', 0.2640940845012665), ('on', 0.2523151636123657), ('starts', 0.24110931158065796)]\n"
     ]
    }
   ],
   "source": [
    "# Find most similar words\n",
    "similar_words = model.wv.most_similar(\"potter\", topn=3)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('want', 0.2976841926574707), ('suggest', 0.2530568242073059), ('she', 0.23463937640190125)]\n"
     ]
    }
   ],
   "source": [
    "similar_words = model.wv.most_similar(\"somebody\", topn=3)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('which', 0.21421189606189728), ('large', 0.19235055148601532), ('arrived', 0.19228415191173553)]\n"
     ]
    }
   ],
   "source": [
    "similar_words = model.wv.most_similar(\"sister\", topn=3)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `GLoVE`\n",
    "\n",
    "- https://nlp.stanford.edu/projects/glove/\n",
    "- https://radimrehurek.com/gensim/models/word2vec.html#pretrained-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasttext-wiki-news-subwords-300\n",
      "conceptnet-numberbatch-17-06-300\n",
      "word2vec-ruscorpora-300\n",
      "word2vec-google-news-300\n",
      "glove-wiki-gigaword-50\n",
      "glove-wiki-gigaword-100\n",
      "glove-wiki-gigaword-200\n",
      "glove-wiki-gigaword-300\n",
      "glove-twitter-25\n",
      "glove-twitter-50\n",
      "glove-twitter-100\n",
      "glove-twitter-200\n",
      "__testing_word2vec-matrix-synopsis\n"
     ]
    }
   ],
   "source": [
    "# All available models in gensim-data\n",
    "for model in gensim.downloader.info()['models'].keys():\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors = gensim.downloader.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('facebook', 0.948005199432373),\n",
       " ('tweet', 0.9403423070907593),\n",
       " ('fb', 0.9342358708381653),\n",
       " ('instagram', 0.9104824066162109),\n",
       " ('chat', 0.8964964747428894),\n",
       " ('hashtag', 0.8885937333106995),\n",
       " ('tweets', 0.8878158330917358),\n",
       " ('tl', 0.8778461217880249),\n",
       " ('link', 0.8778210878372192),\n",
       " ('internet', 0.8753897547721863),\n",
       " ('bio', 0.8740679621696472),\n",
       " ('skype', 0.8711126446723938),\n",
       " ('youtube', 0.8707534074783325),\n",
       " ('spam', 0.8684024214744568),\n",
       " ('tumblr', 0.8668119311332703),\n",
       " ('ex', 0.8645952939987183),\n",
       " ('ask', 0.8644779920578003),\n",
       " ('dm', 0.8439710736274719),\n",
       " ('insta', 0.8426101207733154),\n",
       " ('post', 0.8411487340927124)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vectors.most_similar('twitter', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barack', 0.9471943378448486),\n",
       " ('obama', 0.9400959014892578),\n",
       " ('clinton', 0.9378828406333923),\n",
       " ('former', 0.9294927716255188),\n",
       " ('minister', 0.9137527346611023),\n",
       " ('romney', 0.9051568508148193),\n",
       " ('pope', 0.9035295248031616),\n",
       " ('senator', 0.8977937698364258),\n",
       " ('kerry', 0.8958768844604492),\n",
       " ('hillary', 0.893998920917511),\n",
       " ('potus', 0.8929537534713745),\n",
       " ('bill', 0.8894913196563721),\n",
       " ('says', 0.8860845565795898),\n",
       " ('candidate', 0.8827615976333618),\n",
       " ('justice', 0.882584273815155),\n",
       " ('gov', 0.8798654675483704),\n",
       " ('leader', 0.8764994144439697),\n",
       " ('labour', 0.8748924732208252),\n",
       " ('claims', 0.8715708255767822),\n",
       " ('reagan', 0.8713157176971436)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vectors.most_similar('president', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('china', 0.8639861941337585),\n",
       " ('local', 0.8453366756439209),\n",
       " ('capital', 0.8419684767723083),\n",
       " ('base', 0.8405494689941406),\n",
       " ('a', 0.8330598473548889),\n",
       " ('fox', 0.8293148279190063),\n",
       " ('sub', 0.8233862519264221),\n",
       " ('america', 0.8221979737281799),\n",
       " ('union', 0.8131723403930664),\n",
       " ('media', 0.8116157650947571),\n",
       " ('club', 0.8025693297386169),\n",
       " ('pro', 0.8019703030586243),\n",
       " ('central', 0.8011414408683777),\n",
       " ('uk', 0.7979711294174194),\n",
       " ('red', 0.794592559337616),\n",
       " ('dc', 0.7931137681007385),\n",
       " ('top', 0.791972279548645),\n",
       " ('rock', 0.7895259261131287),\n",
       " ('no', 0.786898672580719),\n",
       " ('york', 0.7861546874046326)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vectors.most_similar('usa', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.74501 , -0.11992 ,  0.37329 ,  0.36847 , -0.4472  , -0.2288  ,\n",
       "        0.70118 ,  0.82872 ,  0.39486 , -0.58347 ,  0.41488 ,  0.37074 ,\n",
       "       -3.6906  , -0.20101 ,  0.11472 , -0.34661 ,  0.36208 ,  0.095679,\n",
       "       -0.01765 ,  0.68498 , -0.049013,  0.54049 , -0.21005 , -0.65397 ,\n",
       "        0.64556 ], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vectors.get_vector('king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "king = glove_vectors.get_vector('king')\n",
    "queen = glove_vectors.get_vector('queen')\n",
    "man = glove_vectors.get_vector('man')\n",
    "woman = glove_vectors.get_vector('woman')\n",
    "\n",
    "res = king - man + woman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.04041   , -0.06222999,  0.07362202,  1.1453301 ,  0.3944    ,\n",
       "       -0.72558004,  1.8081    ,  0.11692998,  0.79493   , -0.66673994,\n",
       "        0.99141   ,  0.54456997, -3.1602    , -0.00692701, -0.68719   ,\n",
       "       -0.71597195, -0.13448006, -0.077546  ,  1.42316   ,  1.05583   ,\n",
       "        0.720557  , -0.25537002, -0.4989    , -2.1607199 , -0.56942   ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25,)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between queen and res: 0.7530912756919861\n"
     ]
    }
   ],
   "source": [
    "# calculate the cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity = cosine_similarity(res.reshape(1, -1), queen.reshape(1, -1))\n",
    "print(f\"Similarity between queen and res: {similarity[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the cosine similarity of two vectors following the linear algebra formula\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity2(v1, v2):\n",
    "    return np.dot(v1, v2.T) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between queen and res: 0.7530912756919861\n"
     ]
    }
   ],
   "source": [
    "similarity2 = cosine_similarity2(res.reshape(1, -1), queen.reshape(1, -1))\n",
    "print(f\"Similarity between queen and res: {similarity2[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 1\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv(), override=True) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"what is the capital of France?\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There would be 1 apple remaining.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"If there are 3 apples and you take away 2, how many in total?\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion for Text-to-Step transformation:\n",
      "\n",
      "Step 1 - Buy some tofu.\n",
      "Step 2 - Heat some oil in a pan.\n",
      "Step 3 - Add the tofu to the pan.\n",
      "Step 4 - Cook the tofu.\n",
      "Step 5 - Add seasoning to the tofu.\n",
      "Step 6 - Optional: Cook some ground beef and then add it to the tofu.\n",
      "Step 7 - Enjoy your delicious tofu.\n"
     ]
    }
   ],
   "source": [
    "text = f\"\"\"\n",
    "Cooking ma po tofu is easy. First, you need to buy some tofu. Then you need to heat some oil in a pan.\n",
    "After that, you need to add the tofu to the pan. Then you need to cook the tofu. After that, you need \n",
    "to add some seasoning to the tofu. Some people might first cook some ground beef and then add the tofu.\n",
    "And that's it! You have cooked some delicious tofu. Enjoy!\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You will be provided with text delimited by triple quotes. If the content contains a sequence of instructions,\n",
    "re-write those instructions in the following format:\n",
    "\n",
    "Step 1 - ...\n",
    "Step 2 - ...\n",
    "...\n",
    "Step N - ...\n",
    "If the content does not contain a sequence of instructions, then simply write \\\"No steps provided.\\\"\n",
    "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(\"Completion for Text-to-Step transformation:\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[83, 1609, 5963, 374, 2294, 0]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('tiktoken is great!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, model_name: str = \"gpt-3.5-turbo\") -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    tokenizer = tiktoken.encoding_for_model(model_name)\n",
    "    num_tokens = len(tokenizer.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_from_string(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_from_string(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tiktoken is great!'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turn tokens into text\n",
    "tokenizer.decode([83, 1609, 5963, 374, 2294, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoid prompt injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion for Text-to-Step transformation:\n",
      "{\n",
      "    \"step_numbers\": [\"Step 1\", \"Step 2\", \"Step 3\", \"Step 4\", \"Step 5\", \"Step 6\"],\n",
      "    \"steps\": [\n",
      "        \"Buy some tofu.\",\n",
      "        \"Heat some oil in a pan.\",\n",
      "        \"Add the tofu to the pan.\",\n",
      "        \"Cook the tofu.\",\n",
      "        \"Add some seasoning to the tofu.\",\n",
      "        \"Cook ground beef and then add the tofu.\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "text = f\"\"\"\n",
    "Cooking ma po tofu is easy. First, you need to buy some tofu. Then you need to heat some oil in a pan.\n",
    "After that, you need to add the tofu to the pan. Then you need to cook the tofu. After that, you need \n",
    "to add some seasoning to the tofu. Some people might first cook some ground beef and then add the tofu.\n",
    "And that's it! You have cooked some delicious tofu. Enjoy!\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You will be provided with text delimited by triple quotes. If the content contains a sequence of instructions,\n",
    "re-write those instructions in the following format:\n",
    "\n",
    "Step 1 - ...\n",
    "Step 2 - ...\n",
    "...\n",
    "Step N - ...\n",
    "If the content does not contain a sequence of instructions, then simply write \\\"No steps provided.\\\"\n",
    "Please provide the response in JSON format with the following keys:\n",
    "step_numbers, steps\n",
    "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(\"Completion for Text-to-Step transformation:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if condition is met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control the length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion for Text-to-Step transformation:\n",
      "{\n",
      "    \"step_numbers\": \"6\",\n",
      "    \"steps\": \"Buy tofu, heat oil, cook tofu, add seasoning, cook ground beef (optional), enjoy\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "text = f\"\"\"\n",
    "Cooking ma po tofu is easy. First, you need to buy some tofu. Then you need to heat some oil in a pan.\n",
    "After that, you need to add the tofu to the pan. Then you need to cook the tofu. After that, you need \n",
    "to add some seasoning to the tofu. Some people might first cook some ground beef and then add the tofu.\n",
    "And that's it! You have cooked some delicious tofu. Enjoy!\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You will be provided with text delimited by triple quotes. If the content contains a sequence of instructions,\n",
    "Summarize the information about cooking ma po tofu, make sure the summarization is less than 20 tokens.\n",
    "\n",
    "If the content does not contain a sequence of instructions, then simply write \\\"No steps provided.\\\"\n",
    "Please provide the response in JSON format with the following keys:\n",
    "step_numbers, steps\n",
    "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(\"Completion for Text-to-Step transformation:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: To become a good leader, you must lead by example, inspire others, communicate effectively, make decisions with confidence, and always strive to improve yourself and those around you. It's important to listen to feedback, be open-minded, and show empathy towards others. Remember, a good leader is someone who empowers and motivates their team to achieve success.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Please answer questions in a consistent style.\n",
    "\n",
    "Q: How can I become a kungfu master?\n",
    "A: Empty your mind, be formless. Shapeless, like water. If you put water into a cup, it becomes the cup. You put water into a bottle and it becomes the bottle. You put it in a teapot, it becomes the teapot. Now, water can flow or it can crash. Be water, my friend.\n",
    "Q: How can I become a good leader?\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A math problem (coursera example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The student's solution is incorrect. The total cost for the first year of operations should be calculated as follows:\n",
      "\n",
      "Total cost = Land cost + Solar panel cost + Maintenance cost\n",
      "Total cost = 100x + 250x + (100,000 + 10x)\n",
      "Total cost = 350x + 100,000\n",
      "\n",
      "Therefore, the correct total cost for the first year of operations as a function of the number of square feet is 350x + 100,000.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Determine if the student's solution is correct or not.\n",
    "\n",
    "Question:\n",
    "I'm building a solar power installation and I need\n",
    " help working out the financials. \n",
    "- Land costs $100 / square foot\n",
    "- I can buy solar panels for $250 / square foot\n",
    "- I negotiated a contract for maintenance that will cost \n",
    "me a flat $100k per year, and an additional $10 / square\n",
    "foot\n",
    "What is the total cost for the first year of operations \n",
    "as a function of the number of square feet.\n",
    "\n",
    "Student's Solution:\n",
    "Let x be the size of the installation in square feet.\n",
    "Costs:\n",
    "1. Land cost: 100x\n",
    "2. Solar panel cost: 250x\n",
    "3. Maintenance cost: 100,000 + 100x\n",
    "Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hallucinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xiangshi Yin is a traditional Chinese architectural style that originated in the Song Dynasty (960-1279) and reached its peak during the Ming (1368-1644) and Qing (1644-1912) dynasties. It is characterized by its intricate wooden structures, curved eaves, and elaborate decorations.\n",
      "\n",
      "One of the key features of Xiangshi Yin architecture is the use of dougong, a unique structural element that consists of interlocking wooden brackets that support the roof. Dougong allows for large, open interior spaces without the need for columns or other supports, giving Xiangshi Yin buildings a sense of lightness and elegance.\n",
      "\n",
      "Xiangshi Yin buildings are typically constructed using traditional Chinese building techniques, such as mortise and tenon joints, and feature intricate carvings and paintings on the walls, doors, and ceilings. The roofs are often adorned with colorful glazed tiles and ornate dragon motifs, symbolizing power and protection.\n",
      "\n",
      "Xiangshi Yin architecture is often found in temples, palaces, and other important buildings, where it is used to convey a sense of grandeur and importance. Today, Xiangshi Yin architecture continues to be celebrated for its beauty and craftsmanship, and many historic buildings have been preserved and restored to showcase this traditional Chinese architectural style.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Tell me about the architecture Xiangshi Yin\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interative Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Tell me about the self-attention mechanism in transformers.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"\"\"\n",
    "First year changing from Milorganite.....Have already spread over 3 acres once, waiting on my next shipment. This stuff spreads super easy and I am glad they increased the size of bags. We only had one issue and it is when my daughter had her basketball team over, it is good for PGF and bad.....maybe. I decided to use the spreading of fertilizer as conditioning drill, which seemed like a win/win. All the girls did well until it got to KOBI. Kobi started sprinting with the spreader and I was attempting to get her to slow down. By the time I got to her to explain why I needed an even spread on my beautiful lawn, she slipped in a giant St. Bernard turd. Unfortunately she was wearing her basketball shoes for some reason so they got ruined....but that is not the worst part. When she fell she went forward and landed face first in the spreader with all the fertilizer. Which would not have been a problem but Kobi is the most out of shape person on the team and when she began sprinting with the spreader she immediately broke out in a sweat. The PGF fertilizer stuck to her face and when she looked up it looked like a young version of the bearded lady from the carnival. After I quit laughing, I attempted to help her wash off her face. Well actually I asked her to pick up all the fertilizer she spilled first because this stuff is not cheap. Then we washed off her face but she screamed and screamed. I thought it was because of her prepubescent acne face. However, I now believe it was because PGF started to work instantly, with the moisture on her face. That was late February and her mother just called me in April and stated her daughter is growing a full beard and mustache. It was unfortunate because we had a basketball tournament and they would not let her play because they did not believe she was a girl. The hair on her face is thick and rich, which makes me think this is a great product. But you might want to keep it away from the Kobi's of the world.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferring\n",
    "- Sentiment (positive/negative)\n",
    "- Identify types of emotions\n",
    "- Identify the subject of the text\n",
    "- Identify the entities (product and company)\n",
    "- Multiple tasks at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Help me identify the entities and relations present in the following product review:\n",
    "Review:\n",
    "```\n",
    "{review}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Help me identify the entities (product, company, etc.) and relations present in the following product review:\n",
    "Review:\n",
    "```\n",
    "{review}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Translate the following English text to Chinese:\n",
    "\n",
    "Hi, I would like to order a ma po tofu.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tone transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spellchecks (example from coursera) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [ \n",
    "  \"The girl with the black and white puppies have a ball.\",  # The girl has a ball.\n",
    "  \"Yolanda has her notebook.\", # ok\n",
    "  \"Its going to be a long day. Does the car need its oil changed?\",  # Homonyms\n",
    "  \"Their goes my freedom. There going to bring theyre suitcases.\",  # Homonyms\n",
    "  \"Your going to need youre notebook.\",  # Homonyms\n",
    "  \"That medicine effects my ability to sleep. Have you heard of the butterfly affect?\", # Homonyms\n",
    "  \"This phrase is to cherck chatGPT for speling abilitty\"  # spelling\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reply to Customer Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are a customer service AI assistant.\n",
    "Your task is to send an email reply to a valued customer.\n",
    "Given the customer email delimited by ```, \\\n",
    "Generate a reply to thank the customer for their review.\n",
    "If the sentiment is positive or neutral, thank them for \\\n",
    "their review.\n",
    "If the sentiment is negative, apologize and suggest that \\\n",
    "they can reach out to customer service. \n",
    "Make sure to use specific details from the review.\n",
    "Write in a concise and professional tone.\n",
    "Sign the email as `AI customer agent`.\n",
    "Customer review: ```{review}```\n",
    "Review sentiment: {sentiment}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages =  [  \n",
    "{'role':'system', 'content':'You are an assistant that speaks like Shakespeare.'},    \n",
    "{'role':'user', 'content':'tell me a joke'},   \n",
    "{'role':'assistant', 'content':'Why did the chicken cross the road'},   \n",
    "{'role':'user', 'content':'I don\\'t know'}  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = input(\"Tell me a joke: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
