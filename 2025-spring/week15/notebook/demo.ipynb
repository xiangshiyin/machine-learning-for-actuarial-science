{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(**Click the icon below to open this notebook in Colab**)\n",
    "\n",
    "[![Open InColab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/xiangshiyin/machine-learning-for-actuarial-science/blob/main/2025-spring/week15/notebook/demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/xiangshiyin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/xiangshiyin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/xiangshiyin/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "data = \"This is a simple example to demonstrate removing stopwords using NLTK.\"\n",
    "stopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = word_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: This is a simple example to demonstrate removing stopwords using NLTK.\n",
      "Tokenized text: This|is|a|simple|example|to|demonstrate|removing|stopwords|using|NLTK|.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original text: {data}\")\n",
    "print(f\"Tokenized text: {\"|\".join(tokenized_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing stopwords: ['This', 'simple', 'example', 'demonstrate', 'removing', 'stopwords', 'using', 'NLTK', '.']\n"
     ]
    }
   ],
   "source": [
    "filtered_tokenized_data = [\n",
    "    w\n",
    "    for w in tokenized_data\n",
    "    if w not in stopWords\n",
    "]\n",
    "print(f\"After removing stopwords: {filtered_tokenized_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: This is a simple example to demonstrate removing stopwords using NLTK.\n",
      "Tokenized text: This|is|a|simple|example|to|demonstrate|removing|stopwords|using|NLTK|.\n",
      "After removing stopwords: This|simple|example|demonstrate|removing|stopwords|using|NLTK|.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original text: {data}\")\n",
    "print(f\"Tokenized text: {\"|\".join(tokenized_data)}\")\n",
    "print(f\"After removing stopwords: {\"|\".join(filtered_tokenized_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names (Vocabulary):\n",
      "['all' 'amazing' 'at' 'ever' 'experience' 'happy' 'hate' 'is' 'love' 'not'\n",
      " 'product' 'result' 'satisfied' 'the' 'this' 'very' 'with' 'worst']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Sample dataset\n",
    "texts = [\n",
    "    \"I love this product\",         # positive\n",
    "    \"This is amazing\",             # positive\n",
    "    \"Very happy with the result\",  # positive\n",
    "    \"I hate this\",                 # negative\n",
    "    \"Worst experience ever\",       # negative\n",
    "    \"Not satisfied at all\"         # negative\n",
    "]\n",
    "\n",
    "labels = [1, 1, 1, 0, 0, 0]  # 1 = positive, 0 = negative\n",
    "\n",
    "# 2. Convert text to bag-of-words vectors\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# 3. Show feature names\n",
    "print(\"Feature Names (Vocabulary):\")\n",
    "print(vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 18)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/xiangshiyin/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8275\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.81      0.82       199\n",
      "           1       0.82      0.84      0.83       201\n",
      "\n",
      "    accuracy                           0.83       400\n",
      "   macro avg       0.83      0.83      0.83       400\n",
      "weighted avg       0.83      0.83      0.83       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import nltk\n",
    "\n",
    "# Download NLTK movie_reviews data\n",
    "nltk.download('movie_reviews')\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# Prepare dataset\n",
    "docs = []\n",
    "labels = []\n",
    "\n",
    "for fileid in movie_reviews.fileids():\n",
    "    docs.append(movie_reviews.raw(fileid))\n",
    "    labels.append(movie_reviews.categories(fileid)[0])  # 'pos' or 'neg'\n",
    "\n",
    "# Convert labels to binary format\n",
    "y = [1 if label == 'pos' else 0 for label in labels]\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(docs, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train classifier\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '0009f', '007', '00s', '03', '04', '05', '05425',\n",
       "       '10', '100', '1000', '10000', '100m', '101', '102', '103', '104',\n",
       "       '105', '106'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "feature_names[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top TF-IDF features in sample test document:\n",
      "                 tfidf\n",
      "bates         0.401204\n",
      "annie         0.253364\n",
      "caan          0.169454\n",
      "kathy         0.148969\n",
      "sledgehammer  0.127790\n",
      "realises      0.117562\n",
      "french        0.114832\n",
      "nerve         0.107711\n",
      "cake          0.104894\n",
      "misery        0.102454\n",
      "masterful     0.100301\n",
      "basically     0.093828\n",
      "prisoner      0.092226\n",
      "arm           0.088148\n",
      "tense         0.086170\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Choose a sample document from the test set\n",
    "sample_idx = 0\n",
    "sample_vector = X_test_tfidf[sample_idx]\n",
    "\n",
    "# Convert sparse vector to dense and create DataFrame\n",
    "df_features = pd.DataFrame(\n",
    "    data=sample_vector.toarray()[0],\n",
    "    index=feature_names,\n",
    "    columns=[\"tfidf\"]\n",
    ")\n",
    "\n",
    "# Filter non-zero features and sort\n",
    "df_nonzero = df_features[df_features.tfidf > 0].sort_values(by=\"tfidf\", ascending=False)\n",
    "\n",
    "# Show top 15 features by TF-IDF weight\n",
    "print(\"\\nTop TF-IDF features in sample test document:\")\n",
    "print(df_nonzero.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hand-craft implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Sample corpus\n",
    "corpus = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# Preprocessing: Tokenization and vocabulary building\n",
    "tokens = re.findall(r'\\b\\w+\\b', corpus.lower())\n",
    "vocab = set(tokens)\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quick': 0,\n",
       " 'the': 1,\n",
       " 'brown': 2,\n",
       " 'dog': 3,\n",
       " 'fox': 4,\n",
       " 'over': 5,\n",
       " 'jumps': 6,\n",
       " 'lazy': 7}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'quick',\n",
       " 1: 'the',\n",
       " 2: 'brown',\n",
       " 3: 'dog',\n",
       " 4: 'fox',\n",
       " 5: 'over',\n",
       " 6: 'jumps',\n",
       " 7: 'lazy'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data\n",
    "def generate_training_data(tokens, window_size):\n",
    "    training_data = []\n",
    "    for idx, target_word in enumerate(tokens):\n",
    "        target_idx = word_to_idx[target_word]\n",
    "        context_range = list(range(max(0, idx - window_size), idx)) + \\\n",
    "                        list(range(idx + 1, min(len(tokens), idx + window_size + 1)))\n",
    "        for context_idx in context_range:\n",
    "            context_word = tokens[context_idx]\n",
    "            context_word_idx = word_to_idx[context_word]\n",
    "            training_data.append((target_idx, context_word_idx))\n",
    "    return training_data\n",
    "\n",
    "window_size = 2\n",
    "training_data = generate_training_data(tokens, window_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: The quick brown fox jumps over the lazy dog\n",
      "[('the', 'quick'), ('the', 'brown'), ('quick', 'the'), ('quick', 'brown'), ('quick', 'fox'), ('brown', 'the'), ('brown', 'quick'), ('brown', 'fox'), ('brown', 'jumps'), ('fox', 'quick'), ('fox', 'brown'), ('fox', 'jumps'), ('fox', 'over'), ('jumps', 'brown'), ('jumps', 'fox'), ('jumps', 'over'), ('jumps', 'the'), ('over', 'fox'), ('over', 'jumps'), ('over', 'the'), ('over', 'lazy'), ('the', 'jumps'), ('the', 'over'), ('the', 'lazy'), ('the', 'dog'), ('lazy', 'over'), ('lazy', 'the'), ('lazy', 'dog'), ('dog', 'the'), ('dog', 'lazy')]\n"
     ]
    }
   ],
   "source": [
    "# Inspect the training data\n",
    "print(f\"Corpus: {corpus}\")\n",
    "print([\n",
    "    (idx_to_word[t[0]], idx_to_word[t[1]])\n",
    "    for t in training_data\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 43.5765\n",
      "Epoch 200, Loss: 39.8485\n",
      "Epoch 300, Loss: 37.8640\n",
      "Epoch 400, Loss: 39.2224\n",
      "Epoch 500, Loss: 41.6311\n",
      "Epoch 600, Loss: 39.8883\n",
      "Epoch 700, Loss: 40.8639\n",
      "Epoch 800, Loss: 41.8159\n",
      "Epoch 900, Loss: 41.0794\n",
      "Epoch 1000, Loss: 35.5038\n"
     ]
    }
   ],
   "source": [
    "# Initialize parameters\n",
    "embedding_dim = 10\n",
    "W1 = np.random.randn(vocab_size, embedding_dim)\n",
    "W2 = np.random.randn(embedding_dim, vocab_size)\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Training parameters\n",
    "epochs = 1000\n",
    "learning_rate = 0.01\n",
    "num_negative_samples = 2\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    for target_idx, context_idx in training_data:\n",
    "        # Positive sample\n",
    "        h = W1[target_idx]\n",
    "        u = np.dot(h, W2[:, context_idx])\n",
    "        pred = sigmoid(u)\n",
    "        error = pred - 1\n",
    "        loss += -np.log(pred + 1e-7)\n",
    "        # Gradients\n",
    "        grad_W2 = error * h\n",
    "        grad_W1 = error * W2[:, context_idx]\n",
    "        # Update weights\n",
    "        W2[:, context_idx] -= learning_rate * grad_W2\n",
    "        W1[target_idx] -= learning_rate * grad_W1\n",
    "\n",
    "        # Negative sampling\n",
    "        negative_samples = random.sample([i for i in range(vocab_size) if i != context_idx], num_negative_samples)\n",
    "        for neg_idx in negative_samples:\n",
    "            u_neg = np.dot(h, W2[:, neg_idx])\n",
    "            pred_neg = sigmoid(u_neg)\n",
    "            error_neg = pred_neg\n",
    "            loss += -np.log(1 - pred_neg + 1e-7)\n",
    "            # Gradients\n",
    "            grad_W2_neg = error_neg * h\n",
    "            grad_W1_neg = error_neg * W2[:, neg_idx]\n",
    "            # Update weights\n",
    "            W2[:, neg_idx] -= learning_rate * grad_W2_neg\n",
    "            W1[target_idx] -= learning_rate * grad_W1_neg\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Words similar to 'fox':\n",
      "jumps: 0.4163\n",
      "brown: 0.3140\n",
      "quick: 0.3060\n"
     ]
    }
   ],
   "source": [
    "# Retrieve word embeddings\n",
    "word_embeddings = W1\n",
    "\n",
    "# Example: Find similar words\n",
    "def find_similar(word, top_n=3):\n",
    "    if word not in word_to_idx:\n",
    "        print(f\"'{word}' not in vocabulary.\")\n",
    "        return\n",
    "    idx = word_to_idx[word]\n",
    "    vec = word_embeddings[idx]\n",
    "    similarities = []\n",
    "    for i in range(vocab_size):\n",
    "        if i == idx:\n",
    "            continue\n",
    "        sim = np.dot(vec, word_embeddings[i]) / (np.linalg.norm(vec) * np.linalg.norm(word_embeddings[i]))\n",
    "        similarities.append((idx_to_word[i], sim))\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    for word, sim in similarities[:top_n]:\n",
    "        print(f\"{word}: {sim:.4f}\")\n",
    "\n",
    "# Test the model\n",
    "print(\"\\nWords similar to 'fox':\")\n",
    "find_similar('fox')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With `Gensim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample corpus\n",
    "sentences = [\n",
    "    [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"],\n",
    "    [\"i\", \"love\", \"natural\", \"language\", \"processing\"],\n",
    "    [\"word2vec\", \"is\", \"a\", \"technique\", \"for\", \"natural\", \"language\", \"processing\"],\n",
    "    [\"the\", \"dog\", \"is\", \"lazy\", \"but\", \"the\", \"brown\", \"fox\", \"is\", \"quick\"]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the model\n",
    "model = Word2Vec(\n",
    "    sentences,\n",
    "    vector_size=100,  # Dimensionality of the word vectors\n",
    "    window=5,         # Maximum distance between the current and predicted word\n",
    "    min_count=1,      # Ignores all words with total frequency lower than this\n",
    "    workers=4,        # Use these many worker threads to train the model\n",
    "    sg=1              # 1 for Skip-gram; 0 for CBOW\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('over', 0.16699621081352234), ('brown', 0.1388736069202423), ('quick', 0.13150502741336823)]\n",
      "Similarity between 'dog' and 'fox': -0.1052\n"
     ]
    }
   ],
   "source": [
    "# Find most similar words\n",
    "similar_words = model.wv.most_similar(\"fox\", topn=3)\n",
    "print(similar_words)\n",
    "\n",
    "# Compute similarity between two words\n",
    "similarity = model.wv.similarity(\"dog\", \"fox\")\n",
    "print(f\"Similarity between 'dog' and 'fox': {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"\"\"\n",
    "Mr. Dursley was the director of a firm called Grunnings, which made\n",
    "drills. He was a big, beefy man with hardly any neck, although he did\n",
    "have a very large mustache. Mrs. Dursley was thin and blonde and had\n",
    "nearly twice the usual amount of neck, which came in very useful as she\n",
    "spent so much of her time craning over garden fences, spying on the\n",
    "neighbors. The Dursleys had a small son called Dudley and in their\n",
    "opinion there was no finer boy anywhere.\n",
    "\n",
    "\n",
    "The Dursleys had everything they wanted, but they also had a secret, and\n",
    "their greatest fear was that somebody would discover it. They didn't\n",
    "think they could bear it if anyone found out about the Potters. Mrs.\n",
    "Potter was Mrs. Dursley's sister, but they hadn't met for several years;\n",
    "in fact, Mrs. Dursley pretended she didn't have a sister, because her\n",
    "sister and her good-for-nothing husband were as unDursleyish as it was\n",
    "possible to be. The Dursleys shuddered to think what the neighbors would\n",
    "say if the Potters arrived in the street. The Dursleys knew that the\n",
    "Potters had a small son, too, but they had never even seen him. This boy\n",
    "was another good reason for keeping the Potters away; they didn't want\n",
    "Dudley mixing with a child like that.\n",
    "\n",
    "\n",
    "When Mr. and Mrs. Dursley woke up on the dull, gray Tuesday our story\n",
    "starts, there was nothing about the cloudy sky outside to suggest that\n",
    "strange and mysterious things would soon be happening all over the\n",
    "country. Mr. Dursley hummed as he picked out his most boring tie for\n",
    "work, and Mrs. Dursley gossiped away happily as she wrestled a screaming\n",
    "Dudley into his high chair.\n",
    "\"\"\"\n",
    "\n",
    "sentences = [\n",
    "    gensim.utils.simple_preprocess(sentence)\n",
    "    for sentence in sample.split(\"\\n\\n\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(\n",
    "    sentences,\n",
    "    vector_size=100,  # Dimensionality of the word vectors\n",
    "    window=5,         # Maximum distance between the current and predicted word\n",
    "    min_count=1,      # Ignores all words with total frequency lower than this\n",
    "    workers=4,        # Use these many worker threads to train the model\n",
    "    sg=1              # 1 for Skip-gram; 0 for CBOW\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dursley', 0.2640940845012665), ('on', 0.2523151636123657), ('starts', 0.24110931158065796)]\n"
     ]
    }
   ],
   "source": [
    "# Find most similar words\n",
    "similar_words = model.wv.most_similar(\"potter\", topn=3)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `GLoVE`\n",
    "\n",
    "- https://nlp.stanford.edu/projects/glove/\n",
    "- https://radimrehurek.com/gensim/models/word2vec.html#pretrained-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasttext-wiki-news-subwords-300\n",
      "conceptnet-numberbatch-17-06-300\n",
      "word2vec-ruscorpora-300\n",
      "word2vec-google-news-300\n",
      "glove-wiki-gigaword-50\n",
      "glove-wiki-gigaword-100\n",
      "glove-wiki-gigaword-200\n",
      "glove-wiki-gigaword-300\n",
      "glove-twitter-25\n",
      "glove-twitter-50\n",
      "glove-twitter-100\n",
      "glove-twitter-200\n",
      "__testing_word2vec-matrix-synopsis\n"
     ]
    }
   ],
   "source": [
    "# All available models in gensim-data\n",
    "for model in gensim.downloader.info()['models'].keys():\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors = gensim.downloader.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('facebook', 0.948005199432373),\n",
       " ('tweet', 0.9403423070907593),\n",
       " ('fb', 0.9342358708381653),\n",
       " ('instagram', 0.9104824066162109),\n",
       " ('chat', 0.8964964747428894),\n",
       " ('hashtag', 0.8885937333106995),\n",
       " ('tweets', 0.8878158330917358),\n",
       " ('tl', 0.8778461217880249),\n",
       " ('link', 0.8778210878372192),\n",
       " ('internet', 0.8753897547721863),\n",
       " ('bio', 0.8740679621696472),\n",
       " ('skype', 0.8711126446723938),\n",
       " ('youtube', 0.8707534074783325),\n",
       " ('spam', 0.8684024214744568),\n",
       " ('tumblr', 0.8668119311332703),\n",
       " ('ex', 0.8645952939987183),\n",
       " ('ask', 0.8644779920578003),\n",
       " ('dm', 0.8439710736274719),\n",
       " ('insta', 0.8426101207733154),\n",
       " ('post', 0.8411487340927124)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vectors.most_similar('twitter', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barack', 0.9471943378448486),\n",
       " ('obama', 0.9400959014892578),\n",
       " ('clinton', 0.9378828406333923),\n",
       " ('former', 0.9294927716255188),\n",
       " ('minister', 0.9137527346611023),\n",
       " ('romney', 0.9051568508148193),\n",
       " ('pope', 0.9035295248031616),\n",
       " ('senator', 0.8977937698364258),\n",
       " ('kerry', 0.8958768844604492),\n",
       " ('hillary', 0.893998920917511),\n",
       " ('potus', 0.8929537534713745),\n",
       " ('bill', 0.8894913196563721),\n",
       " ('says', 0.8860845565795898),\n",
       " ('candidate', 0.8827615976333618),\n",
       " ('justice', 0.882584273815155),\n",
       " ('gov', 0.8798654675483704),\n",
       " ('leader', 0.8764994144439697),\n",
       " ('labour', 0.8748924732208252),\n",
       " ('claims', 0.8715708255767822),\n",
       " ('reagan', 0.8713157176971436)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vectors.most_similar('president', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('china', 0.8639861941337585),\n",
       " ('local', 0.8453366756439209),\n",
       " ('capital', 0.8419684767723083),\n",
       " ('base', 0.8405494689941406),\n",
       " ('a', 0.8330598473548889),\n",
       " ('fox', 0.8293148279190063),\n",
       " ('sub', 0.8233862519264221),\n",
       " ('america', 0.8221979737281799),\n",
       " ('union', 0.8131723403930664),\n",
       " ('media', 0.8116157650947571),\n",
       " ('club', 0.8025693297386169),\n",
       " ('pro', 0.8019703030586243),\n",
       " ('central', 0.8011414408683777),\n",
       " ('uk', 0.7979711294174194),\n",
       " ('red', 0.794592559337616),\n",
       " ('dc', 0.7931137681007385),\n",
       " ('top', 0.791972279548645),\n",
       " ('rock', 0.7895259261131287),\n",
       " ('no', 0.786898672580719),\n",
       " ('york', 0.7861546874046326)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vectors.most_similar('usa', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.74501 , -0.11992 ,  0.37329 ,  0.36847 , -0.4472  , -0.2288  ,\n",
       "        0.70118 ,  0.82872 ,  0.39486 , -0.58347 ,  0.41488 ,  0.37074 ,\n",
       "       -3.6906  , -0.20101 ,  0.11472 , -0.34661 ,  0.36208 ,  0.095679,\n",
       "       -0.01765 ,  0.68498 , -0.049013,  0.54049 , -0.21005 , -0.65397 ,\n",
       "        0.64556 ], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vectors.get_vector('king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "king = glove_vectors.get_vector('king')\n",
    "queen = glove_vectors.get_vector('queen')\n",
    "man = glove_vectors.get_vector('man')\n",
    "woman = glove_vectors.get_vector('woman')\n",
    "\n",
    "res = king - man + woman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between queen and res: 0.7530912756919861\n"
     ]
    }
   ],
   "source": [
    "# calculate the cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity = cosine_similarity(res.reshape(1, -1), queen.reshape(1, -1))\n",
    "print(f\"Similarity between queen and res: {similarity[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the cosine similarity of two vectors following the linear algebra formula\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 1\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv(), override=True) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion for Text-to-Step transformation:\n",
      "\n",
      "Step 1 - Buy some tofu.\n",
      "Step 2 - Heat some oil in a pan.\n",
      "Step 3 - Add the tofu to the pan.\n",
      "Step 4 - Cook the tofu.\n",
      "Step 5 - Add seasoning to the tofu.\n",
      "Step 6 - Optionally, cook some ground beef before adding the tofu.\n",
      "Step 7 - Enjoy your delicious tofu.\n"
     ]
    }
   ],
   "source": [
    "text = f\"\"\"\n",
    "Cooking ma po tofu is easy. First, you need to buy some tofu. Then you need to heat some oil in a pan.\n",
    "After that, you need to add the tofu to the pan. Then you need to cook the tofu. After that, you need \n",
    "to add some seasoning to the tofu. Some people might first cook some ground beef and then add the tofu.\n",
    "And that's it! You have cooked some delicious tofu. Enjoy!\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You will be provided with text delimited by triple quotes. If the content contains a sequence of instructions,\n",
    "re-write those instructions in the following format:\n",
    "\n",
    "Step 1 - ...\n",
    "Step 2 - ...\n",
    "...\n",
    "Step N - ...\n",
    "If the content does not contain a sequence of instructions, then simply write \\\"No steps provided.\\\"\n",
    "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(\"Completion for Text-to-Step transformation:\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[83, 1609, 5963, 374, 2294, 0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('tiktoken is great!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, model_name: str = \"gpt-3.5-turbo\") -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    tokenizer = tiktoken.encoding_for_model(model_name)\n",
    "    num_tokens = len(tokenizer.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_from_string(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_from_string(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tiktoken is great!'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turn tokens into text\n",
    "tokenizer.decode([83, 1609, 5963, 374, 2294, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion for Text-to-Step transformation:\n",
      "{\n",
      "    \"step_numbers\": \"1, 2, 3, 4, 5, 6\",\n",
      "    \"steps\": [\n",
      "        \"Step 1 - Buy some tofu.\",\n",
      "        \"Step 2 - Heat some oil in a pan.\",\n",
      "        \"Step 3 - Add the tofu to the pan.\",\n",
      "        \"Step 4 - Cook the tofu.\",\n",
      "        \"Step 5 - Add some seasoning to the tofu.\",\n",
      "        \"Step 6 - Optionally, cook some ground beef before adding the tofu.\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "text = f\"\"\"\n",
    "Cooking ma po tofu is easy. First, you need to buy some tofu. Then you need to heat some oil in a pan.\n",
    "After that, you need to add the tofu to the pan. Then you need to cook the tofu. After that, you need \n",
    "to add some seasoning to the tofu. Some people might first cook some ground beef and then add the tofu.\n",
    "And that's it! You have cooked some delicious tofu. Enjoy!\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You will be provided with text delimited by triple quotes. If the content contains a sequence of instructions,\n",
    "re-write those instructions in the following format:\n",
    "\n",
    "Step 1 - ...\n",
    "Step 2 - ...\n",
    "...\n",
    "Step N - ...\n",
    "If the content does not contain a sequence of instructions, then simply write \\\"No steps provided.\\\"\n",
    "Please provide the response in JSON format with the following keys:\n",
    "step_numbers, steps\n",
    "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(\"Completion for Text-to-Step transformation:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
