{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(**Click the icon below to open this notebook in Colab**)\n",
    "\n",
    "[![Open InColab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/xiangshiyin/machine-learning-for-actuarial-science/blob/main/2025-spring/week15/notebook/demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "data = \"This is a simple example to demonstrate removing stopwords using NLTK.\"\n",
    "stopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.words('english')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = word_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original text: {data}\")\n",
    "print(f\"Tokenized text: {\"|\".join(tokenized_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokenized_data = [\n",
    "    w\n",
    "    for w in tokenized_data\n",
    "    if w not in stopWords\n",
    "]\n",
    "print(f\"After removing stopwords: {filtered_tokenized_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original text: {data}\")\n",
    "print(f\"Tokenized text: {\"|\".join(tokenized_data)}\")\n",
    "print(f\"After removing stopwords: {\"|\".join(filtered_tokenized_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names (Vocabulary):\n",
      "['all' 'amazing' 'at' 'ever' 'experience' 'happy' 'hate' 'is' 'love' 'not'\n",
      " 'product' 'result' 'satisfied' 'the' 'this' 'very' 'with' 'worst']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Sample dataset\n",
    "texts = [\n",
    "    \"I love this product\",         # positive\n",
    "    \"This is amazing\",             # positive\n",
    "    \"Very happy with the result\",  # positive\n",
    "    \"I hate this\",                 # negative\n",
    "    \"Worst experience ever\",       # negative\n",
    "    \"Not satisfied at all\"         # negative\n",
    "]\n",
    "\n",
    "labels = [1, 1, 1, 0, 0, 0]  # 1 = positive, 0 = negative\n",
    "\n",
    "# 2. Convert text to bag-of-words vectors\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# 3. Show feature names\n",
    "print(\"Feature Names (Vocabulary):\")\n",
    "print(vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 18)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/xiangshiyin/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8275\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.81      0.82       199\n",
      "           1       0.82      0.84      0.83       201\n",
      "\n",
      "    accuracy                           0.83       400\n",
      "   macro avg       0.83      0.83      0.83       400\n",
      "weighted avg       0.83      0.83      0.83       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import nltk\n",
    "\n",
    "# Download NLTK movie_reviews data\n",
    "nltk.download('movie_reviews')\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# Prepare dataset\n",
    "docs = []\n",
    "labels = []\n",
    "\n",
    "for fileid in movie_reviews.fileids():\n",
    "    docs.append(movie_reviews.raw(fileid))\n",
    "    labels.append(movie_reviews.categories(fileid)[0])  # 'pos' or 'neg'\n",
    "\n",
    "# Convert labels to binary format\n",
    "y = [1 if label == 'pos' else 0 for label in labels]\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(docs, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train classifier\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '0009f', '007', '00s', '03', '04', '05', '05425',\n",
       "       '10', '100', '1000', '10000', '100m', '101', '102', '103', '104',\n",
       "       '105', '106'], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "feature_names[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top TF-IDF features in sample test document:\n",
      "                 tfidf\n",
      "bates         0.401204\n",
      "annie         0.253364\n",
      "caan          0.169454\n",
      "kathy         0.148969\n",
      "sledgehammer  0.127790\n",
      "realises      0.117562\n",
      "french        0.114832\n",
      "nerve         0.107711\n",
      "cake          0.104894\n",
      "misery        0.102454\n",
      "masterful     0.100301\n",
      "basically     0.093828\n",
      "prisoner      0.092226\n",
      "arm           0.088148\n",
      "tense         0.086170\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Choose a sample document from the test set\n",
    "sample_idx = 0\n",
    "sample_vector = X_test_tfidf[sample_idx]\n",
    "\n",
    "# Convert sparse vector to dense and create DataFrame\n",
    "df_features = pd.DataFrame(\n",
    "    data=sample_vector.toarray()[0],\n",
    "    index=feature_names,\n",
    "    columns=[\"tfidf\"]\n",
    ")\n",
    "\n",
    "# Filter non-zero features and sort\n",
    "df_nonzero = df_features[df_features.tfidf > 0].sort_values(by=\"tfidf\", ascending=False)\n",
    "\n",
    "# Show top 15 features by TF-IDF weight\n",
    "print(\"\\nTop TF-IDF features in sample test document:\")\n",
    "print(df_nonzero.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hand-craft implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Sample corpus\n",
    "corpus = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# Preprocessing: Tokenization and vocabulary building\n",
    "tokens = re.findall(r'\\b\\w+\\b', corpus.lower())\n",
    "vocab = set(tokens)\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data\n",
    "def generate_training_data(tokens, window_size):\n",
    "    training_data = []\n",
    "    for idx, target_word in enumerate(tokens):\n",
    "        target_idx = word_to_idx[target_word]\n",
    "        context_range = list(range(max(0, idx - window_size), idx)) + \\\n",
    "                        list(range(idx + 1, min(len(tokens), idx + window_size + 1)))\n",
    "        for context_idx in context_range:\n",
    "            context_word = tokens[context_idx]\n",
    "            context_word_idx = word_to_idx[context_word]\n",
    "            training_data.append((target_idx, context_word_idx))\n",
    "    return training_data\n",
    "\n",
    "window_size = 2\n",
    "training_data = generate_training_data(tokens, window_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the training data\n",
    "print(f\"Corpus: {corpus}\")\n",
    "print([\n",
    "    (idx_to_word[t[0]], idx_to_word[t[1]])\n",
    "    for t in training_data\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "embedding_dim = 10\n",
    "W1 = np.random.randn(vocab_size, embedding_dim)\n",
    "W2 = np.random.randn(embedding_dim, vocab_size)\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Training parameters\n",
    "epochs = 1000\n",
    "learning_rate = 0.01\n",
    "num_negative_samples = 2\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    for target_idx, context_idx in training_data:\n",
    "        # Positive sample\n",
    "        h = W1[target_idx]\n",
    "        u = np.dot(h, W2[:, context_idx])\n",
    "        pred = sigmoid(u)\n",
    "        error = pred - 1\n",
    "        loss += -np.log(pred + 1e-7)\n",
    "        # Gradients\n",
    "        grad_W2 = error * h\n",
    "        grad_W1 = error * W2[:, context_idx]\n",
    "        # Update weights\n",
    "        W2[:, context_idx] -= learning_rate * grad_W2\n",
    "        W1[target_idx] -= learning_rate * grad_W1\n",
    "\n",
    "        # Negative sampling\n",
    "        negative_samples = random.sample([i for i in range(vocab_size) if i != context_idx], num_negative_samples)\n",
    "        for neg_idx in negative_samples:\n",
    "            u_neg = np.dot(h, W2[:, neg_idx])\n",
    "            pred_neg = sigmoid(u_neg)\n",
    "            error_neg = pred_neg\n",
    "            loss += -np.log(1 - pred_neg + 1e-7)\n",
    "            # Gradients\n",
    "            grad_W2_neg = error_neg * h\n",
    "            grad_W1_neg = error_neg * W2[:, neg_idx]\n",
    "            # Update weights\n",
    "            W2[:, neg_idx] -= learning_rate * grad_W2_neg\n",
    "            W1[target_idx] -= learning_rate * grad_W1_neg\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve word embeddings\n",
    "word_embeddings = W1\n",
    "\n",
    "# Example: Find similar words\n",
    "def find_similar(word, top_n=3):\n",
    "    if word not in word_to_idx:\n",
    "        print(f\"'{word}' not in vocabulary.\")\n",
    "        return\n",
    "    idx = word_to_idx[word]\n",
    "    vec = word_embeddings[idx]\n",
    "    similarities = []\n",
    "    for i in range(vocab_size):\n",
    "        if i == idx:\n",
    "            continue\n",
    "        sim = np.dot(vec, word_embeddings[i]) / (np.linalg.norm(vec) * np.linalg.norm(word_embeddings[i]))\n",
    "        similarities.append((idx_to_word[i], sim))\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    for word, sim in similarities[:top_n]:\n",
    "        print(f\"{word}: {sim:.4f}\")\n",
    "\n",
    "# Test the model\n",
    "print(\"\\nWords similar to 'fox':\")\n",
    "find_similar('fox')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With `Gensim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample corpus\n",
    "sentences = [\n",
    "    [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"],\n",
    "    [\"i\", \"love\", \"natural\", \"language\", \"processing\"],\n",
    "    [\"word2vec\", \"is\", \"a\", \"technique\", \"for\", \"natural\", \"language\", \"processing\"],\n",
    "    [\"the\", \"dog\", \"is\", \"lazy\", \"but\", \"the\", \"brown\", \"fox\", \"is\", \"quick\"]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the model\n",
    "model = Word2Vec(\n",
    "    sentences,\n",
    "    vector_size=100,  # Dimensionality of the word vectors\n",
    "    window=5,         # Maximum distance between the current and predicted word\n",
    "    min_count=1,      # Ignores all words with total frequency lower than this\n",
    "    workers=4,        # Use these many worker threads to train the model\n",
    "    sg=1              # 1 for Skip-gram; 0 for CBOW\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most similar words\n",
    "similar_words = model.wv.most_similar(\"fox\", topn=3)\n",
    "print(similar_words)\n",
    "\n",
    "# Compute similarity between two words\n",
    "similarity = model.wv.similarity(\"dog\", \"fox\")\n",
    "print(f\"Similarity between 'dog' and 'fox': {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"\"\"\n",
    "Mr. Dursley was the director of a firm called Grunnings, which made\n",
    "drills. He was a big, beefy man with hardly any neck, although he did\n",
    "have a very large mustache. Mrs. Dursley was thin and blonde and had\n",
    "nearly twice the usual amount of neck, which came in very useful as she\n",
    "spent so much of her time craning over garden fences, spying on the\n",
    "neighbors. The Dursleys had a small son called Dudley and in their\n",
    "opinion there was no finer boy anywhere.\n",
    "\n",
    "\n",
    "The Dursleys had everything they wanted, but they also had a secret, and\n",
    "their greatest fear was that somebody would discover it. They didn't\n",
    "think they could bear it if anyone found out about the Potters. Mrs.\n",
    "Potter was Mrs. Dursley's sister, but they hadn't met for several years;\n",
    "in fact, Mrs. Dursley pretended she didn't have a sister, because her\n",
    "sister and her good-for-nothing husband were as unDursleyish as it was\n",
    "possible to be. The Dursleys shuddered to think what the neighbors would\n",
    "say if the Potters arrived in the street. The Dursleys knew that the\n",
    "Potters had a small son, too, but they had never even seen him. This boy\n",
    "was another good reason for keeping the Potters away; they didn't want\n",
    "Dudley mixing with a child like that.\n",
    "\n",
    "\n",
    "When Mr. and Mrs. Dursley woke up on the dull, gray Tuesday our story\n",
    "starts, there was nothing about the cloudy sky outside to suggest that\n",
    "strange and mysterious things would soon be happening all over the\n",
    "country. Mr. Dursley hummed as he picked out his most boring tie for\n",
    "work, and Mrs. Dursley gossiped away happily as she wrestled a screaming\n",
    "Dudley into his high chair.\n",
    "\"\"\"\n",
    "\n",
    "sentences = [\n",
    "    gensim.utils.simple_preprocess(sentence)\n",
    "    for sentence in sample.split(\"\\n\\n\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(\n",
    "    sentences,\n",
    "    vector_size=100,  # Dimensionality of the word vectors\n",
    "    window=5,         # Maximum distance between the current and predicted word\n",
    "    min_count=1,      # Ignores all words with total frequency lower than this\n",
    "    workers=4,        # Use these many worker threads to train the model\n",
    "    sg=1              # 1 for Skip-gram; 0 for CBOW\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most similar words\n",
    "similar_words = model.wv.most_similar(\"potter\", topn=3)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `GLoVE`\n",
    "\n",
    "- https://nlp.stanford.edu/projects/glove/\n",
    "- https://radimrehurek.com/gensim/models/word2vec.html#pretrained-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All available models in gensim-data\n",
    "for model in gensim.downloader.info()['models'].keys():\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors = gensim.downloader.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors.most_similar('twitter', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors.most_similar('president', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors.most_similar('usa', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors.get_vector('king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "king = glove_vectors.get_vector('king')\n",
    "queen = glove_vectors.get_vector('queen')\n",
    "man = glove_vectors.get_vector('man')\n",
    "woman = glove_vectors.get_vector('woman')\n",
    "\n",
    "res = king - man + woman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity = cosine_similarity(res.reshape(1, -1), queen.reshape(1, -1))\n",
    "print(f\"Similarity between queen and res: {similarity[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the cosine similarity of two vectors following the linear algebra formula\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
