{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "(**Click the icon below to open this notebook in Colab**)\n",
                "\n",
                "[![Open InColab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/xiangshiyin/machine-learning-for-actuarial-science/blob/main/2025-spring/week09/notebook/demo.ipynb)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Imbalanced Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.datasets import make_classification\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import accuracy_score, recall_score\n",
                "\n",
                "def generate_imbalanced_dataset(n_sample=10000, minority_ratio=0.05, random_state=1):\n",
                "    X, y = make_classification(n_samples=n_sample, n_features=2, n_redundant=0, n_clusters_per_class=1,\n",
                "                               weights=[1 - minority_ratio], class_sep=1, flip_y=0, random_state=random_state)\n",
                "    return X, y\n",
                "\n",
                "# Function to plot decision boundaries\n",
                "def plot_decision_boundaries(model, X, y, ax, title):\n",
                "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
                "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
                "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
                "                         np.linspace(y_min, y_max, 100))\n",
                "\n",
                "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
                "    Z = Z.reshape(xx.shape)\n",
                "\n",
                "    ax.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)\n",
                "    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n",
                "    ax.set_title(title)\n",
                "    ax.set_xlim(xx.min(), xx.max())\n",
                "    ax.set_ylim(yy.min(), yy.max())\n",
                "\n",
                "inputs_to_visualize = []\n",
                "for minority_ratio in [0.001, 0.005, 0.01, 0.05, 0.1, 0.5]:\n",
                "    # Generate an imbalanced dataset\n",
                "    X, y = generate_imbalanced_dataset(n_sample=10000, minority_ratio=minority_ratio)\n",
                "    # Split the dataset\n",
                "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12, stratify=y)\n",
                "    # Fit a logistic regression model\n",
                "    model = LogisticRegression()\n",
                "    model.fit(X_train, y_train)\n",
                "    # Check the accuracy\n",
                "    accuracy = accuracy_score(y_test, model.predict(X_test))\n",
                "    # Store the input data for visualization\n",
                "    inputs_to_visualize.append((minority_ratio, model, X_test, y_test, accuracy))\n",
                "\n",
                "# visualize all in a 1x3 grid\n",
                "nrow, ncol = 3, 2\n",
                "fig, axes = plt.subplots(nrow, ncol, figsize=(8, 8))\n",
                "for i, (minority_ratio, model, X_input, y_input, accuracy) in enumerate(inputs_to_visualize):\n",
                "    plot_decision_boundaries(model, X_input, y_input, axes[i//ncol][i%ncol], f\"Ratio: {minority_ratio}, Accuracy: {accuracy:.4f}\")\n",
                "plt.tight_layout()\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.datasets import make_classification\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import accuracy_score, recall_score\n",
                "\n",
                "def generate_imbalanced_dataset(n_sample=10000, minority_ratio=0.05, random_state=1):\n",
                "    X, y = make_classification(n_samples=n_sample, n_features=2, n_redundant=0, n_clusters_per_class=1,\n",
                "                               weights=[1 - minority_ratio], class_sep=1, flip_y=0, random_state=random_state)\n",
                "    return X, y\n",
                "\n",
                "# Function to plot decision boundaries\n",
                "def plot_decision_boundaries(model, X, y, ax, title):\n",
                "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
                "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
                "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
                "                         np.linspace(y_min, y_max, 100))\n",
                "\n",
                "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
                "    Z = Z.reshape(xx.shape)\n",
                "\n",
                "    ax.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)\n",
                "    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n",
                "    ax.set_title(title)\n",
                "    ax.set_xlim(xx.min(), xx.max())\n",
                "    ax.set_ylim(yy.min(), yy.max())\n",
                "\n",
                "inputs_to_visualize = []\n",
                "for minority_ratio in [0.001, 0.005, 0.01, 0.05, 0.1, 0.5]:\n",
                "    # Generate an imbalanced dataset\n",
                "    X, y = generate_imbalanced_dataset(n_sample=10000, minority_ratio=minority_ratio)\n",
                "    # Split the dataset\n",
                "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12, stratify=y)\n",
                "    # Fit a logistic regression model\n",
                "    model = LogisticRegression()\n",
                "    model.fit(X_train, y_train)\n",
                "    # Check the accuracy and recall\n",
                "    accuracy = accuracy_score(y_test, model.predict(X_test))\n",
                "    recall = recall_score(y_test, model.predict(X_test))\n",
                "    # Store the input data for visualization\n",
                "    inputs_to_visualize.append((minority_ratio, model, X_test, y_test, accuracy, recall))\n",
                "\n",
                "# visualize all in a 1x3 grid\n",
                "nrow, ncol = 3, 2\n",
                "fig, axes = plt.subplots(nrow, ncol, figsize=(8, 8))\n",
                "for i, (minority_ratio, model, X_input, y_input, accuracy, recall) in enumerate(inputs_to_visualize):\n",
                "    plot_decision_boundaries(model, X_input, y_input, axes[i//ncol][i%ncol], f\"Ratio: {minority_ratio}, Accuracy: {accuracy:.4f}, Recall: {recall:.4f}\")\n",
                "plt.tight_layout()\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Model Evaluation with Imbalanced Data\n",
                "\n",
                "- Cross validation score prameter [[doc](https://scikit-learn.org/stable/modules/model_evaluation.html#the-scoring-parameter-defining-model-evaluation-rules)]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import cross_val_score\n",
                "from sklearn.model_selection import RepeatedStratifiedKFold\n",
                "\n",
                "X, y = generate_imbalanced_dataset(n_sample=10000, minority_ratio=0.01)\n",
                "lr = LogisticRegression()\n",
                "\n",
                "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
                "roc_aucs = cross_val_score(lr, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
                "accuracies = cross_val_score(lr, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
                "recalls = cross_val_score(lr, X, y, scoring='recall', cv=cv, n_jobs=-1)\n",
                "f1s = cross_val_score(lr, X, y, scoring='f1', cv=cv, n_jobs=-1)\n",
                "\n",
                "print('Mean ROC AUC: %.4f' % np.mean(roc_aucs))\n",
                "print('Mean Accuracy: %.4f' % np.mean(accuracies))\n",
                "print('Mean Recall: %.4f' % np.mean(recalls))\n",
                "print('Mean F1: %.4f' % np.mean(f1s))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ROC-AUC"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# example of a roc curve for a predictive model\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import roc_curve, roc_auc_score\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# generate 2 class dataset\n",
                "X, y = generate_imbalanced_dataset(n_sample=10000, minority_ratio=0.5)\n",
                "# split into train/test sets\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2, stratify=y)\n",
                "# fit a model\n",
                "lr = LogisticRegression()\n",
                "lr.fit(X_train, y_train)\n",
                "# predict probabilities\n",
                "yhat = lr.predict_proba(X_test)\n",
                "# retrieve just the probabilities for the positive class\n",
                "pos_probs = yhat[:, 1]\n",
                "# plot no skill roc curve\n",
                "plt.plot([0, 1], [0, 1], linestyle='--', label='No Skill')\n",
                "# calculate roc curve for the model\n",
                "fpr, tpr, thresholds = roc_curve(y_test, pos_probs)\n",
                "roc_auc_lr = roc_auc_score(y_test, pos_probs)\n",
                "# plot lr roc curve\n",
                "plt.plot(fpr, tpr, marker='.', label='Logistic')\n",
                "# axis labels\n",
                "plt.xlabel('False Positive Rate')\n",
                "plt.ylabel('True Positive Rate')\n",
                "# show the legend\n",
                "plt.legend()\n",
                "# show the plot\n",
                "plt.title(f'ROC curve for Logistic Regression Model. AUC = {roc_auc_lr:.2f}')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.dummy import DummyClassifier\n",
                "\n",
                "dc = DummyClassifier(strategy='stratified')\n",
                "dc.fit(X_train, y_train)\n",
                "yhat_dummy = dc.predict_proba(X_test)\n",
                "pos_probs_dummy = yhat_dummy[:, 1]\n",
                "# calculate roc auc\n",
                "roc_auc_dummy = roc_auc_score(y_test, pos_probs_dummy)\n",
                "print(f'ROC AUC = {roc_auc_dummy:.2f}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pos_probs_dummy = yhat_dummy[:, 1]\n",
                "\n",
                "plt.hist(pos_probs_dummy, bins=10, label='Positive Class Distribution', density=True)\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pos_probs = yhat[:, 1]\n",
                "neg_probs = yhat[:, 0]\n",
                "\n",
                "plt.hist(pos_probs, bins=100, label='Positive Class Distribution', density=True)\n",
                "plt.axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='Threshold = 0.5')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "recall = recall_score(y_test, lr.predict(X_test))\n",
                "print(f\"Recall: {recall}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "np.sum(pos_probs>=0.5), np.sum(pos_probs<0.5)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
                "\n",
                "\n",
                "|               | Negative Prediction | Positive Prediction |\n",
                "|--------------|--------------------|--------------------|\n",
                "| **Negative Class** | True Negative (TN)  | False Positive (FP) |\n",
                "| **Positive Class** | False Negative (FN) | True Positive (TP) |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import confusion_matrix\n",
                "\n",
                "matrix = confusion_matrix(y_test, lr.predict(X_test))\n",
                "print(matrix)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## PR-AUC"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import precision_recall_curve\n",
                "\n",
                "# retrieve just the probabilities for the positive class\n",
                "pos_probs = yhat[:, 1]\n",
                "# calculate the no skill line as the proportion of the positive class\n",
                "no_skill = len(y[y==1]) / len(y)\n",
                "# plot the no skill precision-recall curve\n",
                "plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
                "# calculate model precision-recall curve\n",
                "precision, recall, _ = precision_recall_curve(y_test, pos_probs)\n",
                "# plot the model precision-recall curve\n",
                "plt.plot(recall, precision, marker='.', label='Logistic')\n",
                "# axis labels\n",
                "plt.xlabel('Recall')\n",
                "plt.ylabel('Precision')\n",
                "# show the legend\n",
                "plt.legend()\n",
                "# show the plot\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import auc\n",
                "\n",
                "auc_score = auc(recall, precision)\n",
                "print('PR AUC: %.3f' % auc_score)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n",
                "precision_dummy, recall_dummy, _ = precision_recall_curve(y_test, pos_probs_dummy)\n",
                "auc_score = auc(recall_dummy, precision_dummy)\n",
                "print('PR AUC: %.3f' % auc_score)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Severly Imbalanced Data\n",
                "\n",
                "### ROC-AUC"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# example of a roc curve for a predictive model\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import roc_curve, roc_auc_score\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# generate 2 class dataset\n",
                "X, y = generate_imbalanced_dataset(n_sample=10000, minority_ratio=0.01)\n",
                "# split into train/test sets\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2, stratify=y)\n",
                "# fit a model\n",
                "lr = LogisticRegression()\n",
                "lr.fit(X_train, y_train)\n",
                "# predict probabilities\n",
                "yhat = lr.predict_proba(X_test)\n",
                "# retrieve just the probabilities for the positive class\n",
                "pos_probs = yhat[:, 1]\n",
                "# plot no skill roc curve\n",
                "plt.plot([0, 1], [0, 1], linestyle='--', label='No Skill')\n",
                "# calculate roc curve for the model\n",
                "fpr, tpr, thresholds = roc_curve(y_test, pos_probs)\n",
                "roc_auc_lr = roc_auc_score(y_test, pos_probs)\n",
                "# plot lr roc curve\n",
                "plt.plot(fpr, tpr, marker='.', label='Logistic')\n",
                "# axis labels\n",
                "plt.xlabel('False Positive Rate')\n",
                "plt.ylabel('True Positive Rate')\n",
                "# show the legend\n",
                "plt.legend()\n",
                "# show the plot\n",
                "plt.title(f'ROC curve for Logistic Regression Model. AUC = {roc_auc_lr:.2f}')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pos_probs = yhat[:, 1]\n",
                "\n",
                "plt.hist(pos_probs, bins=100, label='Positive Class Distribution', density=True)\n",
                "plt.axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='Threshold = 0.5')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "recall = recall_score(y_test, lr.predict(X_test))\n",
                "print(f\"Recall: {recall}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "np.sum(pos_probs>=0.5), np.sum(pos_probs<0.5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import confusion_matrix\n",
                "\n",
                "matrix = confusion_matrix(y_test, lr.predict(X_test))\n",
                "print(matrix)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "7 / 20"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### PR-AUC"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import precision_recall_curve\n",
                "\n",
                "# retrieve just the probabilities for the positive class\n",
                "pos_probs = yhat[:, 1]\n",
                "# calculate the no skill line as the proportion of the positive class\n",
                "no_skill = len(y[y==1]) / len(y)\n",
                "# plot the no skill precision-recall curve\n",
                "plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
                "# calculate model precision-recall curve\n",
                "precision, recall, _ = precision_recall_curve(y_test, pos_probs)\n",
                "# plot the model precision-recall curve\n",
                "plt.plot(recall, precision, marker='.', label='Logistic')\n",
                "# axis labels\n",
                "plt.xlabel('Recall')\n",
                "plt.ylabel('Precision')\n",
                "# show the legend\n",
                "plt.legend()\n",
                "# show the plot\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To explain why the ROC and PR curves tell a different story, recall that the PR curve focuses on the minority class, whereas the ROC curve covers both classes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import auc\n",
                "\n",
                "auc_score = auc(recall, precision)\n",
                "print('PR AUC: %.3f' % auc_score)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# SMOTE (Synthetic Minority Over-sampling Technique)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import imblearn\n",
                "\n",
                "imblearn.__version__"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from collections import Counter\n",
                "\n",
                "# define dataset\n",
                "X, y = generate_imbalanced_dataset(n_sample=10000, minority_ratio=0.01, random_state=1)\n",
                "# summarize class distribution\n",
                "counter = Counter(y)\n",
                "print(counter)\n",
                "\n",
                "for label in counter:\n",
                "    row_ix = np.where(y == label)[0]\n",
                "    plt.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n",
                "    \n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from imblearn.over_sampling import SMOTE\n",
                "\n",
                "X, y = generate_imbalanced_dataset(n_sample=10000, minority_ratio=0.01, random_state=1)\n",
                "# transform the dataset\n",
                "oversample = SMOTE()\n",
                "X, y = oversample.fit_resample(X, y)\n",
                "counter = Counter(y)\n",
                "print(counter)\n",
                "\n",
                "for label in counter:\n",
                "    row_ix = np.where(y == label)[0]\n",
                "    plt.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n",
                "    \n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The original paper on SMOTE suggested combining SMOTE with random undersampling of the majority class."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from imblearn.over_sampling import SMOTE\n",
                "from imblearn.under_sampling import RandomUnderSampler\n",
                "from imblearn.pipeline import Pipeline\n",
                "\n",
                "X, y = generate_imbalanced_dataset(n_sample=10000, minority_ratio=0.01, random_state=1)\n",
                "counter = Counter(y)\n",
                "print(counter)\n",
                "\n",
                "over = SMOTE(sampling_strategy=0.1)\n",
                "X, y = over.fit_resample(X, y)\n",
                "counter = Counter(y)\n",
                "print(counter)\n",
                "\n",
                "for label in counter:\n",
                "    row_ix = np.where(y == label)[0]\n",
                "    plt.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n",
                "    \n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from imblearn.over_sampling import SMOTE\n",
                "from imblearn.under_sampling import RandomUnderSampler\n",
                "from imblearn.pipeline import Pipeline\n",
                "\n",
                "X, y = generate_imbalanced_dataset(n_sample=10000, minority_ratio=0.01, random_state=1)\n",
                "counter = Counter(y)\n",
                "print(counter)\n",
                "\n",
                "over = SMOTE(sampling_strategy=0.1)\n",
                "under = RandomUnderSampler(sampling_strategy=0.5)\n",
                "steps = [('o', over), ('u', under)]\n",
                "pipeline = Pipeline(steps=steps)\n",
                "# transform the dataset\n",
                "X, y = pipeline.fit_resample(X, y)\n",
                "# summarize the new class distribution\n",
                "counter = Counter(y)\n",
                "print(counter)\n",
                "# scatter plot of examples by class label\n",
                "for label in counter:\n",
                "\trow_ix = np.where(y == label)[0]\n",
                "\tplt.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Classification with SMOTE"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import cross_val_score\n",
                "from sklearn.model_selection import RepeatedStratifiedKFold\n",
                "\n",
                "X, y = generate_imbalanced_dataset(n_sample=10000, minority_ratio=0.01)\n",
                "lr = LogisticRegression()\n",
                "\n",
                "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
                "roc_aucs = cross_val_score(lr, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
                "accuracies = cross_val_score(lr, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
                "recalls = cross_val_score(lr, X, y, scoring='recall', cv=cv, n_jobs=-1)\n",
                "f1s = cross_val_score(lr, X, y, scoring='f1', cv=cv, n_jobs=-1)\n",
                "\n",
                "print('Mean ROC AUC: %.4f' % np.mean(roc_aucs))\n",
                "print('Mean Accuracy: %.4f' % np.mean(accuracies))\n",
                "print('Mean Recall: %.4f' % np.mean(recalls))\n",
                "print('Mean F1: %.4f' % np.mean(f1s))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import cross_val_score\n",
                "from sklearn.model_selection import RepeatedStratifiedKFold\n",
                "from imblearn.pipeline import Pipeline\n",
                "\n",
                "X, y = generate_imbalanced_dataset(n_sample=10000, minority_ratio=0.01)\n",
                "lr = LogisticRegression()\n",
                "oversample = SMOTE()\n",
                "\n",
                "steps = [\n",
                "    ('over', oversample),\n",
                "    ('model', lr)\n",
                "]\n",
                "pipeline = Pipeline(steps=steps)\n",
                "\n",
                "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
                "roc_aucs = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
                "accuracies = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
                "recalls = cross_val_score(pipeline, X, y, scoring='recall', cv=cv, n_jobs=-1)\n",
                "f1s = cross_val_score(pipeline, X, y, scoring='f1', cv=cv, n_jobs=-1)\n",
                "\n",
                "print('Mean ROC AUC: %.4f' % np.mean(roc_aucs))\n",
                "print('Mean Accuracy: %.4f' % np.mean(accuracies))\n",
                "print('Mean Recall: %.4f' % np.mean(recalls))\n",
                "print('Mean F1: %.4f' % np.mean(f1s))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import cross_val_score\n",
                "from sklearn.model_selection import RepeatedStratifiedKFold\n",
                "from imblearn.pipeline import Pipeline\n",
                "from imblearn.over_sampling import SMOTE\n",
                "from imblearn.under_sampling import RandomUnderSampler\n",
                "\n",
                "X, y = generate_imbalanced_dataset(n_sample=10000, minority_ratio=0.01)\n",
                "lr = LogisticRegression()\n",
                "oversample = SMOTE(sampling_strategy=0.1)\n",
                "undersample = RandomUnderSampler(sampling_strategy=0.5)\n",
                "\n",
                "steps = [\n",
                "    ('over', oversample),\n",
                "    ('under', undersample),\n",
                "    ('model', lr)\n",
                "]\n",
                "pipeline = Pipeline(steps=steps)\n",
                "\n",
                "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
                "roc_aucs = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
                "accuracies = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
                "recalls = cross_val_score(pipeline, X, y, scoring='recall', cv=cv, n_jobs=-1)\n",
                "precisions = cross_val_score(pipeline, X, y, scoring='precision', cv=cv, n_jobs=-1)\n",
                "f1s = cross_val_score(pipeline, X, y, scoring='f1', cv=cv, n_jobs=-1)\n",
                "\n",
                "print('Mean ROC AUC: %.4f' % np.mean(roc_aucs))\n",
                "print('Mean Accuracy: %.4f' % np.mean(accuracies))\n",
                "print('Mean Recall: %.4f' % np.mean(recalls))\n",
                "print('Mean Precision: %.4f' % np.mean(precisions))\n",
                "print('Mean F1: %.4f' % np.mean(f1s))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import cross_val_score\n",
                "from sklearn.model_selection import RepeatedStratifiedKFold\n",
                "from imblearn.pipeline import Pipeline\n",
                "from imblearn.over_sampling import SMOTE\n",
                "from imblearn.under_sampling import RandomUnderSampler\n",
                "\n",
                "X, y = generate_imbalanced_dataset(n_sample=10000, minority_ratio=0.01)\n",
                "\n",
                "k_values = [1, 2, 3, 4, 5, 6, 7]\n",
                "\n",
                "for k in k_values:\n",
                "    print('Testing k=%d' % k)\n",
                "    lr = LogisticRegression()\n",
                "    oversample = SMOTE(sampling_strategy=0.1, k_neighbors=k)\n",
                "    undersample = RandomUnderSampler(sampling_strategy=0.5)\n",
                "\n",
                "    steps = [\n",
                "        ('over', oversample),\n",
                "        ('under', undersample),\n",
                "        ('model', lr)\n",
                "    ]\n",
                "    pipeline = Pipeline(steps=steps)\n",
                "\n",
                "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
                "    roc_aucs = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
                "    accuracies = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
                "    recalls = cross_val_score(pipeline, X, y, scoring='recall', cv=cv, n_jobs=-1)\n",
                "    precisions = cross_val_score(pipeline, X, y, scoring='precision', cv=cv, n_jobs=-1)\n",
                "    f1s = cross_val_score(pipeline, X, y, scoring='f1', cv=cv, n_jobs=-1)\n",
                "\n",
                "    print('Mean ROC AUC: %.4f' % np.mean(roc_aucs))\n",
                "    print('Mean Accuracy: %.4f' % np.mean(accuracies))\n",
                "    print('Mean Recall: %.4f' % np.mean(recalls))\n",
                "    print('Mean Precision: %.4f' % np.mean(precisions))\n",
                "    print('Mean F1: %.4f' % np.mean(f1s))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Example: German Credit Risk data\n",
                "- https://www.kaggle.com/datasets/uciml/german-credit\n",
                "\n",
                "In this dataset\n",
                "- Each entry represents a person who takes a credit by a bank. \n",
                "- Each person is classified as good or bad credit risks according to the set of attributes.\n",
                "\n",
                "The selected attributes are:\n",
                "- **Age** (numeric)\n",
                "- **Sex** (text: male, female)\n",
                "- **Job** (numeric: 0 - unskilled and non-resident, 1 - unskilled and resident, 2 - skilled, 3 - highly skilled)\n",
                "- **Housing** (text: own, rent, or free)\n",
                "- **Saving accounts** (text - little, moderate, quite rich, rich)\n",
                "- **Checking account** (numeric, in DM - Deutsch Mark)\n",
                "- **Credit amount** (numeric, in DM)\n",
                "- **Duration** (numeric, in month)\n",
                "- **Purpose** (text: car, furniture/equipment, radio/TV, domestic appliances, repairs, education, business, vacation/others)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "credit = pd.read_csv('../data/credit_data_risk.csv')\n",
                "credit.head(3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "credit.drop(columns=['Unnamed: 0'], inplace=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data Exploration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "credit.info()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "credit['Saving accounts'].value_counts()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "credit['Checking account'].value_counts()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "credit['Risk'].value_counts()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "numerical_credit = credit.select_dtypes(exclude='O')\n",
                "numerical_credit.columns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "\n",
                "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
                "sns.histplot(x='Age', data=credit, ax=axes[0, 0])\n",
                "sns.histplot(x='Job', data=credit, ax=axes[0, 1])\n",
                "sns.histplot(x='Credit amount', data=credit, ax=axes[1, 0])\n",
                "sns.histplot(x='Duration', data=credit, ax=axes[1, 1])\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
                "sns.kdeplot(x='Age', hue='Risk', data=credit, ax=axes[0, 0], common_norm=False)\n",
                "sns.kdeplot(x='Job', hue='Risk', data=credit, ax=axes[0, 1], common_norm=False)\n",
                "sns.kdeplot(x='Credit amount', hue='Risk', data=credit, ax=axes[1, 0], common_norm=False)\n",
                "sns.kdeplot(x='Duration', hue='Risk', data=credit, ax=axes[1, 1], common_norm=False)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cat_credit = credit.select_dtypes(include='O')\n",
                "cat_credit.columns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
                "sns.countplot(x='Sex', data=credit, ax=axes[0, 0])\n",
                "sns.countplot(x='Housing', data=credit, ax=axes[0, 1])\n",
                "sns.countplot(x='Saving accounts', data=credit, ax=axes[1, 0])\n",
                "sns.countplot(x='Checking account', data=credit, ax=axes[1, 1])\n",
                "sns.countplot(x='Purpose', data=credit, ax=axes[1, 2])\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.xticks(rotation=45)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
                "sns.countplot(x='Sex', hue='Risk', data=credit, ax=axes[0, 0])\n",
                "sns.countplot(x='Housing', hue='Risk', data=credit, ax=axes[0, 1])\n",
                "sns.countplot(x='Saving accounts', hue='Risk', data=credit, ax=axes[1, 0])\n",
                "sns.countplot(x='Checking account', hue='Risk', data=credit, ax=axes[1, 1])\n",
                "sns.countplot(x='Purpose', hue='Risk', data=credit, ax=axes[1, 2])\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.xticks(rotation=45)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Customer Segmentation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.cluster import KMeans\n",
                "\n",
                "scaler = StandardScaler()\n",
                "scaled_credit = scaler.fit_transform(numerical_credit)\n",
                "\n",
                "## try different k values\n",
                "kmeans_per_k = [KMeans(n_clusters=k).fit(scaled_credit) for k in range(1, 10)]\n",
                "inertias = [model.inertia_ for model in kmeans_per_k]\n",
                "\n",
                "plt.figure(figsize=(8, 3.5))\n",
                "plt.plot(range(1, 10), inertias, \"bo-\")\n",
                "plt.xlabel(\"$k$\", fontsize=14)\n",
                "plt.ylabel(\"Inertia\", fontsize=14)\n",
                "plt.annotate('Elbow',\n",
                "             xy=(4, inertias[3]),\n",
                "             xytext=(0.55, 0.55),\n",
                "             textcoords='figure fraction',\n",
                "             fontsize=16,\n",
                "             arrowprops=dict(facecolor='black', shrink=0.1)\n",
                "            )\n",
                "# plt.ylim(0, 1300)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import silhouette_score\n",
                "\n",
                "silhouette_scores = [silhouette_score(scaled_credit, model.labels_) for model in kmeans_per_k[1:]]\n",
                "plt.figure(figsize=(8, 3))\n",
                "plt.plot(range(2, 10), silhouette_scores, \"bo-\")\n",
                "plt.xlabel(\"$k$\", fontsize=14)\n",
                "plt.ylabel(\"Silhouette score\", fontsize=14)\n",
                "plt.annotate('Optimal K',\n",
                "             xy=(2, silhouette_scores[0]),\n",
                "             xytext=(0.33, 0.33),\n",
                "             textcoords='figure fraction',\n",
                "             fontsize=12,\n",
                "             arrowprops=dict(facecolor='black', shrink=0.1)\n",
                "            )\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### When `k=2`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kmeans = KMeans(n_clusters=2)\n",
                "clusters = kmeans.fit_predict(scaled_credit)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "numerical_credit.head(3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10, 12))\n",
                "plt.subplot(311)\n",
                "plt.scatter(scaled_credit[:, 0], scaled_credit[:, 2], c=kmeans.labels_, cmap='viridis')\n",
                "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 2], s = 80, marker= 'x', color = 'k')\n",
                "plt.xlabel('Age')\n",
                "plt.ylabel('Credit')\n",
                "plt.title('Age vs Credit')\n",
                "\n",
                "plt.subplot(312)\n",
                "plt.scatter(scaled_credit[:, 0], scaled_credit[:, 2], c=kmeans.labels_, cmap='viridis')\n",
                "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 2], s=80, marker='x', color='k')\n",
                "plt.xlabel('Credit')\n",
                "plt.ylabel('Duration')\n",
                "plt.title('Credit vs Duration')\n",
                "\n",
                "plt.subplot(313)\n",
                "plt.scatter(scaled_credit[:, 2], scaled_credit[:, 3], c=kmeans.labels_, cmap='viridis')\n",
                "plt.scatter(kmeans.cluster_centers_[:, 2], kmeans.cluster_centers_[:, 3], s=120, marker='x', color='k')\n",
                "plt.xlabel('Duration')\n",
                "plt.ylabel('Age')\n",
                "plt.title('Age vs Duration')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### When `k=4`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kmeans = KMeans(n_clusters=4)\n",
                "clusters = kmeans.fit_predict(scaled_credit)\n",
                "\n",
                "plt.figure(figsize=(10, 12))\n",
                "plt.subplot(311)\n",
                "plt.scatter(scaled_credit[:, 0], scaled_credit[:, 2], c=kmeans.labels_, cmap='viridis')\n",
                "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 2], s = 80, marker= 'x', color = 'k')\n",
                "plt.xlabel('Age')\n",
                "plt.ylabel('Credit')\n",
                "plt.title('Age vs Credit')\n",
                "\n",
                "plt.subplot(312)\n",
                "plt.scatter(scaled_credit[:, 0], scaled_credit[:, 2], c=kmeans.labels_, cmap='viridis')\n",
                "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 2], s=80, marker='x', color='k')\n",
                "plt.xlabel('Credit')\n",
                "plt.ylabel('Duration')\n",
                "plt.title('Credit vs Duration')\n",
                "\n",
                "plt.subplot(313)\n",
                "plt.scatter(scaled_credit[:, 2], scaled_credit[:, 3], c=kmeans.labels_, cmap='viridis')\n",
                "plt.scatter(kmeans.cluster_centers_[:, 2], kmeans.cluster_centers_[:, 3], s=120, marker='x', color='k')\n",
                "plt.xlabel('Duration')\n",
                "plt.ylabel('Age')\n",
                "plt.title('Age vs Duration')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data Transformations & Model Fit"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# convert all column names to lowercase and remove spaces\n",
                "def clean_column_names(df):\n",
                "    df.columns = df.columns.str.lower()\n",
                "    df.columns = df.columns.str.replace(r'\\s+', '_', regex=True)\n",
                "    return df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "credit = clean_column_names(credit)\n",
                "credit.head(3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "credit.info()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "credit['checking_account'].value_counts()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "credit['saving_accounts'].value_counts()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Append the cluster labels to the original dataframe\n",
                "kmeans = KMeans(n_clusters=2)\n",
                "kmeans.fit(scaled_credit)\n",
                "\n",
                "df = credit.copy()\n",
                "df['cluster'] = kmeans.labels_\n",
                "df.head(3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X = df.drop(columns=['risk'])\n",
                "y = df['risk'].map(lambda x: 1 if x == 'bad' else 0)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y[:3], df['risk'][:3]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Transformation Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X.head(3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
                "from sklearn.impute import SimpleImputer\n",
                "\n",
                "num_features = ['age', 'job', 'credit_amount', 'duration']\n",
                "cat_features = ['sex', 'housing', 'saving_accounts', 'checking_account', 'purpose']\n",
                "\n",
                "num_transformer = Pipeline(steps=[\n",
                "    ('imputer', SimpleImputer(strategy='mean')),  # Fill missing numeric values with mean\n",
                "    ('scaler', StandardScaler())\n",
                "])\n",
                "\n",
                "cat_transformer = Pipeline(steps=[\n",
                "    ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),  # Replace NaN with 'unknown'\n",
                "    ('encoder', OneHotEncoder(handle_unknown='ignore'))  # Encode categorical variables\n",
                "])\n",
                "\n",
                "preprocessor = ColumnTransformer(transformers=[\n",
                "    ('num', num_transformer, num_features),\n",
                "    ('cat', cat_transformer, cat_features)\n",
                "], remainder='passthrough')  # Keep unlisted columns unchanged\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1-Model fit"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import cross_val_score\n",
                "from sklearn.model_selection import RepeatedStratifiedKFold\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "\n",
                "# Apply the transformation\n",
                "X_transformed = preprocessor.fit_transform(X)\n",
                "lr = LogisticRegression()\n",
                "\n",
                "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
                "roc_aucs = cross_val_score(lr, X_transformed, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
                "accuracies = cross_val_score(lr, X_transformed, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
                "recalls = cross_val_score(lr, X_transformed, y, scoring='recall', cv=cv, n_jobs=-1)\n",
                "precisions = cross_val_score(lr, X_transformed, y, scoring='precision', cv=cv, n_jobs=-1)\n",
                "f1s = cross_val_score(lr, X_transformed, y, scoring='f1', cv=cv, n_jobs=-1)\n",
                "\n",
                "print('Mean ROC AUC: %.4f' % np.mean(roc_aucs))\n",
                "print('Mean Accuracy: %.4f' % np.mean(accuracies))\n",
                "print('Mean Recall: %.4f' % np.mean(recalls))\n",
                "print('Mean Precision: %.4f' % np.mean(precisions))\n",
                "print('Mean F1: %.4f' % np.mean(f1s))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.ensemble import BaggingClassifier\n",
                "\n",
                "# Apply the transformation\n",
                "X_transformed = preprocessor.fit_transform(X)\n",
                "\n",
                "# Create a bagging ensemble of logistic regression models\n",
                "base_lr = LogisticRegression(solver='liblinear')  # Use liblinear solver for small datasets\n",
                "bagged_lr = BaggingClassifier(estimator=base_lr, n_estimators=50, random_state=1, max_features=0.8, bootstrap=False, n_jobs=-1)\n",
                "\n",
                "# Define cross-validation\n",
                "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
                "\n",
                "# Evaluate performance\n",
                "roc_aucs = cross_val_score(bagged_lr, X_transformed, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
                "accuracies = cross_val_score(bagged_lr, X_transformed, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
                "recalls = cross_val_score(bagged_lr, X_transformed, y, scoring='recall', cv=cv, n_jobs=-1)\n",
                "precisions = cross_val_score(bagged_lr, X_transformed, y, scoring='precision', cv=cv, n_jobs=-1)\n",
                "f1s = cross_val_score(bagged_lr, X_transformed, y, scoring='f1', cv=cv, n_jobs=-1)\n",
                "\n",
                "# Print results\n",
                "print('Mean ROC AUC: %.4f' % np.mean(roc_aucs))\n",
                "print('Mean Accuracy: %.4f' % np.mean(accuracies))\n",
                "print('Mean Recall: %.4f' % np.mean(recalls))\n",
                "print('Mean Precision: %.4f' % np.mean(precisions))\n",
                "print('Mean F1: %.4f' % np.mean(f1s))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.pipeline import Pipeline\n",
                "\n",
                "# Apply the transformation\n",
                "X_transformed = preprocessor.fit_transform(X)\n",
                "\n",
                "# Define base models\n",
                "base_models = [\n",
                "    ('logreg', LogisticRegression(solver='liblinear')),  # Logistic Regression\n",
                "    ('rf', RandomForestClassifier(n_estimators=100, random_state=1))  # Random Forest\n",
                "]\n",
                "\n",
                "# Define meta-classifier (final model)\n",
                "meta_classifier = LogisticRegression(solver='liblinear')\n",
                "\n",
                "# Create stacking classifier\n",
                "stacked_model = StackingClassifier(estimators=base_models, final_estimator=meta_classifier, n_jobs=-1)\n",
                "\n",
                "# Define cross-validation\n",
                "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
                "\n",
                "# Evaluate performance\n",
                "roc_aucs = cross_val_score(stacked_model, X_transformed, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
                "accuracies = cross_val_score(stacked_model, X_transformed, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
                "recalls = cross_val_score(stacked_model, X_transformed, y, scoring='recall', cv=cv, n_jobs=-1)\n",
                "precisions = cross_val_score(stacked_model, X_transformed, y, scoring='precision', cv=cv, n_jobs=-1)\n",
                "f1s = cross_val_score(stacked_model, X_transformed, y, scoring='f1', cv=cv, n_jobs=-1)\n",
                "\n",
                "# Print results\n",
                "print('Mean ROC AUC: %.4f' % np.mean(roc_aucs))\n",
                "print('Mean Accuracy: %.4f' % np.mean(accuracies))\n",
                "print('Mean Recall: %.4f' % np.mean(recalls))\n",
                "print('Mean Precision: %.4f' % np.mean(precisions))\n",
                "print('Mean F1: %.4f' % np.mean(f1s))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2-Cluster Fit"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_transformed_0 = preprocessor.fit_transform(X[X.cluster == 0])\n",
                "y_0 = y[X.cluster == 0]\n",
                "lr = LogisticRegression()\n",
                "\n",
                "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
                "roc_aucs = cross_val_score(lr, X_transformed_0, y_0, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
                "accuracies = cross_val_score(lr, X_transformed_0, y_0, scoring='accuracy', cv=cv, n_jobs=-1)\n",
                "recalls = cross_val_score(lr, X_transformed_0, y_0, scoring='recall', cv=cv, n_jobs=-1)\n",
                "precisions = cross_val_score(lr, X_transformed_0, y_0, scoring='precision', cv=cv, n_jobs=-1)\n",
                "f1s = cross_val_score(lr, X_transformed_0, y_0, scoring='f1', cv=cv, n_jobs=-1)\n",
                "\n",
                "print('Mean ROC AUC: %.4f' % np.mean(roc_aucs))\n",
                "print('Mean Accuracy: %.4f' % np.mean(accuracies))\n",
                "print('Mean Recall: %.4f' % np.mean(recalls))\n",
                "print('Mean Precision: %.4f' % np.mean(precisions))\n",
                "print('Mean F1: %.4f' % np.mean(f1s))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_transformed_1 = preprocessor.fit_transform(X[X.cluster == 1])\n",
                "y_1 = y[X.cluster == 1]\n",
                "lr = LogisticRegression()\n",
                "\n",
                "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
                "roc_aucs = cross_val_score(lr, X_transformed_1, y_1, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
                "accuracies = cross_val_score(lr, X_transformed_1, y_1, scoring='accuracy', cv=cv, n_jobs=-1)\n",
                "recalls = cross_val_score(lr, X_transformed_1, y_1, scoring='recall', cv=cv, n_jobs=-1)\n",
                "precisions = cross_val_score(lr, X_transformed_1, y_1, scoring='precision', cv=cv, n_jobs=-1)\n",
                "f1s = cross_val_score(lr, X_transformed_1, y_1, scoring='f1', cv=cv, n_jobs=-1)\n",
                "\n",
                "print('Mean ROC AUC: %.4f' % np.mean(roc_aucs))\n",
                "print('Mean Accuracy: %.4f' % np.mean(accuracies))\n",
                "print('Mean Recall: %.4f' % np.mean(recalls))\n",
                "print('Mean Precision: %.4f' % np.mean(precisions))\n",
                "print('Mean F1: %.4f' % np.mean(f1s))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### `cluster_id` as a feature"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.ensemble import RandomForestClassifier\n",
                "\n",
                "# Apply the transformation\n",
                "X_transformed = preprocessor.fit_transform(X)\n",
                "rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=1)\n",
                "\n",
                "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
                "roc_aucs = cross_val_score(rf, X_transformed, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
                "accuracies = cross_val_score(rf, X_transformed, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
                "recalls = cross_val_score(rf, X_transformed, y, scoring='recall', cv=cv, n_jobs=-1)\n",
                "precisions = cross_val_score(rf, X_transformed, y, scoring='precision', cv=cv, n_jobs=-1)\n",
                "f1s = cross_val_score(rf, X_transformed, y, scoring='f1', cv=cv, n_jobs=-1)\n",
                "\n",
                "print('Mean ROC AUC: %.4f' % np.mean(roc_aucs))\n",
                "print('Mean Accuracy: %.4f' % np.mean(accuracies))\n",
                "print('Mean Recall: %.4f' % np.mean(recalls))\n",
                "print('Mean Precision: %.4f' % np.mean(precisions))\n",
                "print('Mean F1: %.4f' % np.mean(f1s))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Grid search\n",
                "\n",
                "from sklearn.model_selection import GridSearchCV\n",
                "\n",
                "# Define the parameter grid for Random Forest hyperparameter tuning\n",
                "param_grid = {\n",
                "    'n_estimators': [100, 200, 300],\n",
                "    'max_depth': [10, 20, None],\n",
                "    'min_samples_split': [2, 5, 10],\n",
                "    'min_samples_leaf': [1, 2, 4],\n",
                "    'max_features': ['auto', 'sqrt', 'log2']\n",
                "}\n",
                "\n",
                "# Define RandomForestClassifier model\n",
                "rf = RandomForestClassifier(random_state=1)\n",
                "\n",
                "# Define GridSearchCV with the parameter grid\n",
                "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, scoring='roc_auc')\n",
                "\n",
                "# Apply the transformation and perform GridSearchCV\n",
                "X_transformed = preprocessor.fit_transform(X)\n",
                "\n",
                "# Fit GridSearchCV with cross-validation\n",
                "grid_search.fit(X_transformed, y)\n",
                "\n",
                "# Print the best parameters found by GridSearchCV\n",
                "print(\"Best Hyperparameters:\", grid_search.best_params_)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.ensemble import RandomForestClassifier\n",
                "\n",
                "# Apply the transformation\n",
                "X_transformed = preprocessor.fit_transform(X)\n",
                "# rf = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=1, max_features='sqrt', min_samples_leaf=4, min_samples_split=2)\n",
                "rf = grid_search.best_estimator_\n",
                "\n",
                "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
                "roc_aucs = cross_val_score(rf, X_transformed, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
                "accuracies = cross_val_score(rf, X_transformed, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
                "recalls = cross_val_score(rf, X_transformed, y, scoring='recall', cv=cv, n_jobs=-1)\n",
                "precisions = cross_val_score(rf, X_transformed, y, scoring='precision', cv=cv, n_jobs=-1)\n",
                "f1s = cross_val_score(rf, X_transformed, y, scoring='f1', cv=cv, n_jobs=-1)\n",
                "\n",
                "print('Mean ROC AUC: %.4f' % np.mean(roc_aucs))\n",
                "print('Mean Accuracy: %.4f' % np.mean(accuracies))\n",
                "print('Mean Recall: %.4f' % np.mean(recalls))\n",
                "print('Mean Precision: %.4f' % np.mean(precisions))\n",
                "print('Mean F1: %.4f' % np.mean(f1s))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Convert transformed array back to DataFrame (optional)\n",
                "output_columns = (\n",
                "    num_features + ['cluster'] + \n",
                "    list(preprocessor.named_transformers_['cat'].named_steps['encoder'].get_feature_names_out(cat_features))\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate feature importance ranking\n",
                "feature_importances = rf.feature_importances_\n",
                "\n",
                "# Create a DataFrame with feature names and their corresponding importance values\n",
                "importance_df = pd.DataFrame({\n",
                "    'Feature': output_columns,\n",
                "    'Importance': feature_importances\n",
                "})\n",
                "\n",
                "# Sort the features by their importance in descending order\n",
                "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
                "\n",
                "# Display the feature importance ranking\n",
                "importance_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Other Considerations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "credit2 = credit.copy()\n",
                "credit2['risk'] = credit2['risk'].apply(lambda x: 0 if x == 'good' else 1)\n",
                "credit2.head(3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X = credit2.drop(columns=['risk'])\n",
                "y = credit2['risk']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
                "from sklearn.impute import SimpleImputer\n",
                "\n",
                "num_features = ['age', 'job', 'credit_amount', 'duration']\n",
                "cat_features = ['sex', 'housing', 'saving_accounts', 'checking_account', 'purpose']\n",
                "\n",
                "num_transformer = Pipeline(steps=[\n",
                "    ('imputer', SimpleImputer(strategy='mean')),  # Fill missing numeric values with mean\n",
                "    ('scaler', StandardScaler())\n",
                "])\n",
                "\n",
                "cat_transformer = Pipeline(steps=[\n",
                "    ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),  # Replace NaN with 'unknown'\n",
                "    ('encoder', OneHotEncoder(handle_unknown='ignore'))  # Encode categorical variables\n",
                "])\n",
                "\n",
                "preprocessor = ColumnTransformer(transformers=[\n",
                "    ('num', num_transformer, num_features),\n",
                "    ('cat', cat_transformer, cat_features)\n",
                "], remainder='passthrough')  # Keep unlisted columns unchanged\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
                "X_train_transformed = preprocessor.fit_transform(X_train)\n",
                "lr = LogisticRegression()\n",
                "lr.fit(X_train_transformed, y_train)\n",
                "X_test_transformed = preprocessor.transform(X_test)\n",
                "y_test_pred = lr.predict(X_test_transformed)\n",
                "\n",
                "# calculate accuracy, recall, precision, f1\n",
                "accuracy = accuracy_score(y_test, y_test_pred)\n",
                "precision = precision_score(y_test, y_test_pred)\n",
                "recall = recall_score(y_test, y_test_pred)\n",
                "f1 = f1_score(y_test, y_test_pred)\n",
                "\n",
                "print(\"Accuracy:\", accuracy)\n",
                "print(\"Precision:\", precision)\n",
                "print(\"Recall:\", recall)\n",
                "print(\"F1 Score:\", f1)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_train_pred = lr.predict(X_train_transformed)\n",
                "\n",
                "# calculate accuracy, recall, precision, f1\n",
                "accuracy = accuracy_score(y_train, y_train_pred)\n",
                "precision = precision_score(y_train, y_train_pred)\n",
                "recall = recall_score(y_train, y_train_pred)\n",
                "f1 = f1_score(y_train, y_train_pred)\n",
                "\n",
                "print(\"Accuracy: {:.2f}\".format(accuracy))\n",
                "print(\"Precision: {:.2f}\".format(precision))\n",
                "print(\"Recall: {:.2f}\".format(recall))\n",
                "print(\"F1-score: {:.2f}\".format(f1))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_train = pd.concat([X_train, y_train], axis=1)\n",
                "df_train['risk_pred'] = y_train_pred\n",
                "df_train['accuracy'] = y_train_pred == y_train\n",
                "df_train['accuracy'] = df_train['accuracy'].astype(int)\n",
                "df_train.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_train.groupby('purpose').agg(accuracy_mean=('accuracy', 'mean'), row_count=('accuracy', 'count')).reset_index()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_test = pd.concat([X_test, y_test], axis=1)\n",
                "df_test['risk_pred'] = y_test_pred\n",
                "df_test['accuracy'] = y_test_pred == y_test\n",
                "df_test['accuracy'] = df_test['accuracy'].astype(int)\n",
                "df_test.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_test.groupby('purpose').agg(accuracy_mean=('accuracy', 'mean'), row_count=('accuracy', 'count')).reset_index()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "summary_train = df_train.groupby('purpose').agg(accuracy_mean=('accuracy', 'mean'), row_count=('accuracy', 'count')).reset_index()\n",
                "summary_test = df_test.groupby('purpose').agg(accuracy_mean=('accuracy','mean'), row_count=('accuracy', 'count')).reset_index()\n",
                "summary = pd.merge(\n",
                "    summary_train.rename(columns={'accuracy_mean': 'accuracy_mean_train', 'row_count': 'row_count_train'}),\n",
                "    summary_test.rename(columns={'accuracy_mean': 'accuracy_mean_test', 'row_count': 'row_count_test'}),\n",
                "    on='purpose',\n",
                ")\n",
                "summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.6"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
