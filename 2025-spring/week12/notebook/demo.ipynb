{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "(**Click the icon below to open this notebook in Colab**)\n",
                "\n",
                "[![Open InColab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/xiangshiyin/machine-learning-for-actuarial-science/blob/main/2025-spring/week12/notebook/demo.ipynb)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Ensemble Learning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.svm import SVC\n",
                "\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.datasets import make_moons\n",
                "import numpy as np\n",
                "\n",
                "\n",
                "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "plt.scatter(X[y==0,0],X[y==0,1],c='r',marker='o',edgecolors='black')\n",
                "plt.scatter(X[y==1,0],X[y==1,1],c='b',marker='^',edgecolors='black')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from matplotlib.colors import ListedColormap\n",
                "\n",
                "def plot_decision_boundary(clf, X, y, axes=[-1.5, 2.45, -1, 1.5], alpha=0.5, contour=True):\n",
                "    x1s = np.linspace(axes[0], axes[1], 100)\n",
                "    x2s = np.linspace(axes[2], axes[3], 100)\n",
                "    x1, x2 = np.meshgrid(x1s, x2s)\n",
                "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
                "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
                "    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
                "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n",
                "    if contour:\n",
                "        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n",
                "        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n",
                "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", alpha=alpha)\n",
                "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", alpha=alpha)\n",
                "    plt.axis(axes)\n",
                "    plt.xlabel(r\"$x_1$\", fontsize=18)\n",
                "    plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Boosting\n",
                "\n",
                "**Boosting** (originally called hypothesis boosting) refers to any ensemble method that can combine several weak learners into a strong learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### AdaBoost\n",
                "\n",
                "$$\n",
                "\\begin{aligned}\n",
                "&\\text{Error Rate of Weak Classifier:} \\quad\n",
                "\\epsilon_t = \\sum_{i=1}^{N} w_i I(y_i \\neq h_t(x_i)) \\\\\n",
                "\n",
                "&\\text{Weight of Weak Classifier:} \\quad\n",
                "\\alpha_t = \\frac{1}{2} \\eta \\ln \\left(\\frac{1 - \\epsilon_t}{\\epsilon_t} \\right) \\\\\n",
                "\n",
                "&\\text{Final Prediction:} \\quad\n",
                "H(x) = \\text{sign} \\left\\{ \\sum_{t=1}^{T} \\alpha_t h_t(x) \\right\\}\n",
                "\\end{aligned}\n",
                "$$\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "m = len(X_train)\n",
                "\n",
                "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
                "for subplot, learning_rate in ((0, 1), (1, 0.5)):\n",
                "    print(learning_rate)\n",
                "    sample_weights = np.ones(m) / m\n",
                "    plt.sca(axes[subplot])\n",
                "    for i in range(5):\n",
                "        svm_clf = SVC(C=0.2, gamma=0.6, random_state=42)\n",
                "        svm_clf.fit(X_train, y_train, sample_weight=sample_weights * m)\n",
                "        y_pred = svm_clf.predict(X_train)\n",
                "\n",
                "        error_weights = sample_weights[y_pred != y_train].sum()\n",
                "        r = error_weights / sample_weights.sum()  # equation 7-1\n",
                "        alpha = learning_rate * np.log((1 - r) / r)  # equation 7-2\n",
                "        sample_weights[y_pred != y_train] *= np.exp(alpha)  # equation 7-3\n",
                "        sample_weights /= sample_weights.sum()  # normalization step\n",
                "\n",
                "        plot_decision_boundary(svm_clf, X_train, y_train, alpha=0.4)\n",
                "        plt.title(f\"learning_rate = {learning_rate}\")\n",
                "    if subplot == 0:\n",
                "        plt.text(-0.75, -0.95, \"1\", fontsize=16)\n",
                "        plt.text(-1.05, -0.95, \"2\", fontsize=16)\n",
                "        plt.text(1.0, -0.95, \"3\", fontsize=16)\n",
                "        plt.text(-1.45, -0.5, \"4\", fontsize=16)\n",
                "        plt.text(1.36,  -0.95, \"5\", fontsize=16)\n",
                "    else:\n",
                "        plt.ylabel(\"\")\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.ensemble import AdaBoostClassifier\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.metrics import accuracy_score\n",
                "\n",
                "ada_clf = AdaBoostClassifier(\n",
                "    DecisionTreeClassifier(max_depth=1), n_estimators=100,\n",
                "    learning_rate=0.5, random_state=42)\n",
                "ada_clf.fit(X_train, y_train)\n",
                "\n",
                "y_pred_ada = ada_clf.predict(X_test)\n",
                "accuracy_score(y_test, y_pred_ada)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_decision_boundary(ada_clf, X_train, y_train, alpha=0.4)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Gradient Boosting\n",
                "Just like AdaBoost, gradient boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every iteration like AdaBoost does, this method tries to fit the new predictor to the residual errors made by the previous predictor.\n",
                "\n",
                "In case of a gradient boosting regressor, this yields predictions with the following form:\n",
                "$$\n",
                "\\hat{y}_i = \\sum_{k=1}^{n_{iter}} h_m(x_i)\n",
                "$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "from sklearn.tree import DecisionTreeRegressor\n",
                "\n",
                "np.random.seed(42)\n",
                "X = np.random.rand(100, 1) - 0.5\n",
                "y = 3 * X[:, 0] ** 2 + 0.05 * np.random.randn(100)  # y = 3xÂ² + Gaussian noise\n",
                "\n",
                "tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
                "tree_reg1.fit(X, y)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y2 = y - tree_reg1.predict(X)\n",
                "tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=43)\n",
                "tree_reg2.fit(X, y2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y3 = y2 - tree_reg2.predict(X)\n",
                "tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=44)\n",
                "tree_reg3.fit(X, y3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_new = np.array([[-0.4], [0.], [0.5]])\n",
                "sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_predictions(regressors, X, y, axes, style,\n",
                "                     label=None, data_style=\"b.\", data_label=None):\n",
                "    x1 = np.linspace(axes[0], axes[1], 500)\n",
                "    y_pred = sum(regressor.predict(x1.reshape(-1, 1))\n",
                "                 for regressor in regressors)\n",
                "    plt.plot(X[:, 0], y, data_style, label=data_label)\n",
                "    plt.plot(x1, y_pred, style, linewidth=2, label=label)\n",
                "    if label or data_label:\n",
                "        plt.legend(loc=\"upper center\")\n",
                "    plt.axis(axes)\n",
                "\n",
                "plt.figure(figsize=(11, 11))\n",
                "\n",
                "plt.subplot(3, 2, 1)\n",
                "plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.2, 0.8], style=\"g-\",\n",
                "                 label=\"$h_1(x_1)$\", data_label=\"Training set\")\n",
                "plt.ylabel(\"$y$  \", rotation=0)\n",
                "plt.title(\"Residuals and tree predictions\")\n",
                "\n",
                "plt.subplot(3, 2, 2)\n",
                "plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.2, 0.8], style=\"r-\",\n",
                "                 label=\"$h(x_1) = h_1(x_1)$\", data_label=\"Training set\")\n",
                "plt.title(\"Ensemble predictions\")\n",
                "\n",
                "plt.subplot(3, 2, 3)\n",
                "plot_predictions([tree_reg2], X, y2, axes=[-0.5, 0.5, -0.4, 0.6], style=\"g-\",\n",
                "                 label=\"$h_2(x_1)$\", data_style=\"k+\",\n",
                "                 data_label=\"Residuals: $y - h_1(x_1)$\")\n",
                "plt.ylabel(\"$y$  \", rotation=0)\n",
                "\n",
                "plt.subplot(3, 2, 4)\n",
                "plot_predictions([tree_reg1, tree_reg2], X, y, axes=[-0.5, 0.5, -0.2, 0.8],\n",
                "                  style=\"r-\", label=\"$h(x_1) = h_1(x_1) + h_2(x_1)$\")\n",
                "\n",
                "plt.subplot(3, 2, 5)\n",
                "plot_predictions([tree_reg3], X, y3, axes=[-0.5, 0.5, -0.4, 0.6], style=\"g-\",\n",
                "                 label=\"$h_3(x_1)$\", data_style=\"k+\",\n",
                "                 data_label=\"Residuals: $y - h_1(x_1) - h_2(x_1)$\")\n",
                "plt.xlabel(\"$x_1$\")\n",
                "plt.ylabel(\"$y$  \", rotation=0)\n",
                "\n",
                "plt.subplot(3, 2, 6)\n",
                "plot_predictions([tree_reg1, tree_reg2, tree_reg3], X, y,\n",
                "                 axes=[-0.5, 0.5, -0.2, 0.8], style=\"r-\",\n",
                "                 label=\"$h(x_1) = h_1(x_1) + h_2(x_1) + h_3(x_1)$\")\n",
                "plt.xlabel(\"$x_1$\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Directly apply a gradient boosting regressor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.ensemble import GradientBoostingRegressor\n",
                "\n",
                "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3,\n",
                "                                 learning_rate=1.0, random_state=42)\n",
                "gbrt.fit(X, y)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "gbrt_best = GradientBoostingRegressor(\n",
                "    max_depth=2, learning_rate=0.05, n_estimators=500,\n",
                "    n_iter_no_change=10, random_state=42)\n",
                "gbrt_best.fit(X, y)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "gbrt_best.n_estimators_"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
                "\n",
                "plt.sca(axes[0])\n",
                "plot_predictions([gbrt], X, y, axes=[-0.5, 0.5, -0.1, 0.8], style=\"r-\",\n",
                "                 label=\"Ensemble predictions\")\n",
                "plt.title(f\"learning_rate={gbrt.learning_rate}, \"\n",
                "          f\"n_estimators={gbrt.n_estimators_}\")\n",
                "plt.xlabel(\"$x_1$\")\n",
                "plt.ylabel(\"$y$\", rotation=0)\n",
                "\n",
                "plt.sca(axes[1])\n",
                "plot_predictions([gbrt_best], X, y, axes=[-0.5, 0.5, -0.1, 0.8], style=\"r-\")\n",
                "plt.title(f\"learning_rate={gbrt_best.learning_rate}, \"\n",
                "          f\"n_estimators={gbrt_best.n_estimators_}\")\n",
                "plt.xlabel(\"$x_1$\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### `XgBoost`\n",
                "\n",
                "#### Example 1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X = np.random.rand(100, 1) - 0.5\n",
                "y = 3 * X[:, 0] ** 2 + 0.05 * np.random.randn(100) "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from xgboost import XGBRegressor\n",
                "\n",
                "xgb_reg_50 = XGBRegressor(n_estimators=50, max_depth=3, learning_rate=0.1, random_state=42)\n",
                "xgb_reg_50.fit(X, y)\n",
                "\n",
                "xgb_reg_10 = XGBRegressor(n_estimators=10, max_depth=3, learning_rate=0.1, random_state=42)\n",
                "xgb_reg_10.fit(X, y)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
                "\n",
                "plt.sca(axes[0])\n",
                "plot_predictions([xgb_reg_10], X, y, axes=[-0.5, 0.5, -0.1, 0.8], style=\"r-\",\n",
                "                 label=\"n_estimators=10\")\n",
                "plt.xlabel(\"$x_1$\")\n",
                "plt.ylabel(\"$y$\", rotation=0)\n",
                "\n",
                "plt.sca(axes[1])\n",
                "plot_predictions([xgb_reg_50], X, y, axes=[-0.5, 0.5, -0.1, 0.8], style=\"r-\",\n",
                "                 label=\"n_estimators=50\")\n",
                "plt.xlabel(\"$x_1$\")\n",
                "plt.ylabel(\"$y$\", rotation=0)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "xgb_reg_50 = XGBRegressor(n_estimators=50, max_depth=3, learning_rate=0.1, random_state=42)\n",
                "xgb_reg_50.fit(X, y)\n",
                "\n",
                "xgb_reg_50_v2 = XGBRegressor(n_estimators=50, max_depth=3, learning_rate=0.1, gamma=0.1, random_state=42)\n",
                "xgb_reg_50_v2.fit(X, y)\n",
                "\n",
                "xgb_reg_50_v3 = XGBRegressor(n_estimators=50, max_depth=3, learning_rate=0.1, gamma=0.01, random_state=42)\n",
                "xgb_reg_50_v3.fit(X, y)\n",
                "\n",
                "## visulize\n",
                "fig, axes = plt.subplots(ncols=3, figsize=(10, 4), sharey=True)\n",
                "plt.sca(axes[0])\n",
                "plot_predictions([xgb_reg_50], X, y, axes=[-0.5, 0.5, -0.1, 0.8], style=\"r-\",\n",
                "                 label=\"n_estimators=50\")\n",
                "plt.xlabel(\"$x_1$\")\n",
                "plt.ylabel(\"$y$\", rotation=0)\n",
                "plt.sca(axes[1])\n",
                "plot_predictions([xgb_reg_50_v2], X, y, axes=[-0.5, 0.5, -0.1, 0.8], style=\"r-\",\n",
                "                 label=\"n_estimators=50, gamma=0.1\")\n",
                "plt.xlabel(\"$x_1$\")\n",
                "plt.ylabel(\"$y$\", rotation=0)\n",
                "plt.sca(axes[2])\n",
                "plot_predictions([xgb_reg_50_v3], X, y, axes=[-0.5, 0.5, -0.1, 0.8], style=\"r-\",\n",
                "                 label=\"n_estimators=50, gamma=0.01\")\n",
                "plt.xlabel(\"$x_1$\")\n",
                "plt.ylabel(\"$y$\", rotation=0)\n",
                "\n",
                "plt.legend(loc=\"upper center\")\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Example 2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import xgboost as xgb\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.datasets import load_breast_cancer\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import accuracy_score, classification_report\n",
                "\n",
                "# Load dataset\n",
                "data = load_breast_cancer()\n",
                "X, y = data.data, data.target\n",
                "feature_names = data.feature_names  # Get feature names\n",
                "\n",
                "# Split into train and test sets\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "# Train an XGBoost classifier\n",
                "model = xgb.XGBClassifier(eval_metric='logloss', random_state=42)\n",
                "model.fit(X_train, y_train)\n",
                "\n",
                "# Make predictions\n",
                "y_pred = model.predict(X_test)\n",
                "\n",
                "# Evaluate model\n",
                "accuracy = accuracy_score(y_test, y_pred)\n",
                "print(f'Accuracy: {accuracy:.4f}')\n",
                "print(classification_report(y_test, y_pred))\n",
                "\n",
                "# Feature importance visualization\n",
                "importances = model.feature_importances_\n",
                "sorted_idx = np.argsort(importances)[-10:]\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.barh(range(10), importances[sorted_idx], align='center')\n",
                "plt.yticks(range(10), [feature_names[i] for i in sorted_idx])\n",
                "plt.xlabel(\"Feature Importance\")\n",
                "plt.title(\"Top 10 Feature Importances\")\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### LightGBM\n",
                "- https://lightgbm.readthedocs.io/en/stable/pythonapi/lightgbm.LGBMClassifier.html#lightgbm.LGBMClassifier"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import lightgbm as lgb\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.datasets import load_breast_cancer\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import accuracy_score, classification_report\n",
                "\n",
                "# Load dataset\n",
                "data = load_breast_cancer()\n",
                "X, y = data.data, data.target\n",
                "feature_names = data.feature_names  # Get feature names\n",
                "\n",
                "# Split into train and test sets\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "# Train a LightGBM classifier\n",
                "model = lgb.LGBMClassifier(random_state=42)\n",
                "model.fit(X_train, y_train)\n",
                "\n",
                "# Make predictions\n",
                "y_pred = model.predict(X_test)\n",
                "\n",
                "# Evaluate model\n",
                "accuracy = accuracy_score(y_test, y_pred)\n",
                "print(f'Accuracy: {accuracy:.4f}')\n",
                "print(classification_report(y_test, y_pred))\n",
                "\n",
                "# Feature importance visualization\n",
                "importances = model.feature_importances_\n",
                "sorted_idx = np.argsort(importances)[-10:]\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.barh(range(10), importances[sorted_idx], align='center')\n",
                "plt.yticks(range(10), [feature_names[i] for i in sorted_idx])\n",
                "plt.xlabel(\"Feature Importance\")\n",
                "plt.title(\"Top 10 Feature Importances\")\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Stacking"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.ensemble import StackingClassifier\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.svm import SVC\n",
                "\n",
                "stacking_clf = StackingClassifier(\n",
                "    estimators=[\n",
                "        ('lr', LogisticRegression(random_state=42)),\n",
                "        ('rf', RandomForestClassifier(random_state=42)),\n",
                "        ('svc', SVC(probability=True, random_state=42))\n",
                "    ],\n",
                "    final_estimator=RandomForestClassifier(random_state=43),\n",
                "    cv=5  # number of cross-validation folds\n",
                ")\n",
                "stacking_clf.fit(X_train, y_train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "stacking_clf.score(X_test,y_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import accuracy_score\n",
                "\n",
                "# Get individual base estimators\n",
                "base_models = stacking_clf.estimators_\n",
                "names = [\n",
                "    'LogisticRegression',\n",
                "    'RandomForestRegressor',\n",
                "    'SupportVectorClassifier'\n",
                "]\n",
                "\n",
                "# Evaluate each base model\n",
                "for name, model in zip(names, base_models):\n",
                "    y_pred = model.predict(X_test)  # Make predictions\n",
                "    acc = accuracy_score(y_test, y_pred)  # Compute accuracy\n",
                "    print(f\"Accuracy of {name}: {acc:.4f}\")\n",
                "\n",
                "# Evaluate the final stacked model\n",
                "stacked_acc = accuracy_score(y_test, stacking_clf.predict(X_test))\n",
                "print(f\"Accuracy of StackingClassifier (Final Model): {stacked_acc:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Model Serving\n",
                "\n",
                "## `Pickle`\n",
                "Serialize a model for later usage"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Example 1 - Save a Python object to local storage"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pickle\n",
                "\n",
                "data = {\n",
                "    'name': 'John Doe',\n",
                "    'age': 30,\n",
                "    'city': 'New York',\n",
                "    'occupation': 'Software Engineer'\n",
                "}\n",
                "\n",
                "# Save this dictionary to a file\n",
                "with open('data.pickle', 'wb') as f:\n",
                "    pickle.dump(data, f)\n",
                "\n",
                "print(\"Data saved to file data.pickle.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the dictionary back from the pickle file\n",
                "with open('data.pickle', 'rb') as f:\n",
                "    data2 = pickle.load(f)\n",
                "print(\"Dictionary loaded successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data2 == data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "id(data2), id(data)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Example 2 - Save a ML model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pickle\n",
                "from sklearn.datasets import load_iris\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "\n",
                "# Load the Iris dataset\n",
                "iris = load_iris()\n",
                "X, y = iris.data, iris.target\n",
                "\n",
                "# Split into training and testing sets\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "print(\"Dataset loaded and split successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train a RandomForest classifier\n",
                "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
                "model.fit(X_train, y_train)\n",
                "\n",
                "print(\"Model trained successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save the trained model to a file\n",
                "with open(\"iris_model.pkl\", \"wb\") as file:\n",
                "    pickle.dump(model, file)\n",
                "\n",
                "print(\"Model saved successfully as 'iris_model.pkl'!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Predict with the in-memory model\n",
                "from sklearn.metrics import accuracy_score\n",
                "\n",
                "y_pred = model.predict(X_test)\n",
                "# Calculate the accuracy\n",
                "accuracy = accuracy_score(y_test, y_pred)\n",
                "# Print the accuracy\n",
                "print(\"Accuracy:\", accuracy)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_pred[:20]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_test[:20]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Predict with the saved model\n",
                "with open('iris_model.pkl', 'rb') as f:\n",
                "    model2 = pickle.load(f)\n",
                "\n",
                "y_pred2 = model2.predict(X_test)\n",
                "accuracy2 = accuracy_score(y_test, y_pred2)\n",
                "print(\"Accuracy:\", accuracy2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_pred2[:20]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_test[:20]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Single data point (sepal length, sepal width, petal length, petal width)\n",
                "single_data_point = [[5.1, 3.5, 1.4, 0.2]]\n",
                "\n",
                "# Predict class for this single data point\n",
                "single_prediction = model.predict(single_data_point)\n",
                "\n",
                "# Get class name from iris dataset\n",
                "predicted_class = iris.target_names[single_prediction[0]]\n",
                "\n",
                "print(f\"Predicted Class for the Single Data Point: {predicted_class}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Predict class for this single data point\n",
                "single_prediction2 = model.predict(single_data_point)\n",
                "\n",
                "# Get class name from iris dataset\n",
                "predicted_class2 = iris.target_names[single_prediction2[0]]\n",
                "\n",
                "print(f\"Predicted Class for the Single Data Point: {predicted_class2}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "single_prediction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "iris.target_names"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## `FastAPI`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from fastapi import FastAPI\n",
                "from pydantic import BaseModel\n",
                "import pickle\n",
                "import numpy as np\n",
                "\n",
                "app = FastAPI()\n",
                "\n",
                "# Load trained model\n",
                "with open(\"iris_model.pkl\", \"rb\") as file:\n",
                "    model = pickle.load(file)\n",
                "\n",
                "\n",
                "# Define the expected request format\n",
                "class FeaturesInput(BaseModel):\n",
                "    features: list[float]\n",
                "\n",
                "\n",
                "@app.post(\"/predict\")\n",
                "def predict(data: FeaturesInput):\n",
                "    prediction = model.predict([data.features])\n",
                "    return {\"prediction\": int(prediction[0])}\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Run command `uvicorn fastapi_demo:app --host 0.0.0.0 --port 8000 --reload` to expose the API endpoints.\n",
                "- Or run `fastapi dev fastapi_demo.py --reload`\n",
                "Run `curl` command below to test the API\n",
                "```\n",
                "curl -X POST \"http://127.0.0.1:8000/predict\" \\\n",
                "      -H \"Content-Type: application/json\" \\\n",
                "      -d '{\"features\": [5.1, 3.5, 1.4, 0.2]}'\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## `Streamlit` example"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import streamlit as st\n",
                "import pickle\n",
                "import numpy as np\n",
                "\n",
                "# Load model\n",
                "with open(\"iris_model.pkl\", \"rb\") as file:\n",
                "    model = pickle.load(file)\n",
                "\n",
                "st.title(\"Iris Flower Classifier\")\n",
                "\n",
                "# User input\n",
                "sepal_length = st.number_input(\"Sepal Length\")\n",
                "sepal_width = st.number_input(\"Sepal Width\")\n",
                "petal_length = st.number_input(\"Petal Length\")\n",
                "petal_width = st.number_input(\"Petal Width\")\n",
                "\n",
                "if st.button(\"Predict\"):\n",
                "    features = np.array([[sepal_length, sepal_width, petal_length, petal_width]])\n",
                "    prediction = model.predict(features)[0]\n",
                "    st.write(f\"Predicted Class: {prediction}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Run `streamlit run streamlit_demo.py`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.6"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
