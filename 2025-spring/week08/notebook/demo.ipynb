{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "(**Click the icon below to open this notebook in Colab**)\n",
                "\n",
                "[![Open InColab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/xiangshiyin/machine-learning-for-actuarial-science/blob/main/2025-spring/week08/notebook/demo.ipynb)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%sh\n",
                "\n",
                "ls -l work/2025-spring/week08/"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Recap of Last Class"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Example - `prophet`\n",
                "\n",
                "Prophet is an enhanced statistical tool because it extends traditional time series models with automated trend and seasonality detection, making it user-friendly and effective for business applications.\n",
                "\n",
                "## Key Differences Between Holiday Effect and Seasonality  \n",
                "\n",
                "| Feature            | Seasonality  | Holiday Effect |\n",
                "|--------------------|-------------|---------------|\n",
                "| **Definition**     | A **repeating pattern** that occurs at a fixed frequency (e.g., daily, weekly, yearly). | An impact on the time series caused by specific holidays or events. |\n",
                "| **Regularity**     | **Strictly periodic** (e.g., weekly sales cycles, annual weather patterns). | **Not necessarily periodic** (holidays can fall on different weekdays each year). |\n",
                "| **Examples**       | Increased ice cream sales in summer (yearly seasonality). Higher website traffic on weekends (weekly seasonality). | Lower stock market activity on Christmas. Higher online sales on Black Friday. |\n",
                "| **Modeling in Prophet** | Modeled using a **Fourier series** to capture repeating cycles. | Modeled using **dummy variables** for specific dates, allowing deviations from normal patterns. |\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from prophet import Prophet\n",
                "\n",
                "playoffs = pd.DataFrame({\n",
                "  'holiday': 'playoff',\n",
                "  'ds': pd.to_datetime(['2008-01-13', '2009-01-03', '2010-01-16',\n",
                "                        '2010-01-24', '2010-02-07', '2011-01-08',\n",
                "                        '2013-01-12', '2014-01-12', '2014-01-19',\n",
                "                        '2014-02-02', '2015-01-11', '2016-01-17',\n",
                "                        '2016-01-24', '2016-02-07']),\n",
                "  'lower_window': 0,\n",
                "  'upper_window': 1,\n",
                "})\n",
                "superbowls = pd.DataFrame({\n",
                "  'holiday': 'superbowl',\n",
                "  'ds': pd.to_datetime(['2010-02-07', '2014-02-02', '2016-02-07']),\n",
                "  'lower_window': 0,\n",
                "  'upper_window': 1,\n",
                "})\n",
                "holidays = pd.concat((playoffs, superbowls))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "holidays"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The time series of the log daily page views for the Wikipedia page for Peyton Manning is used here\n",
                "- https://facebook.github.io/prophet/docs/quick_start.html"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.read_csv('https://raw.githubusercontent.com/facebook/prophet/main/examples/example_wp_log_peyton_manning.csv')\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "m = Prophet(holidays=holidays)\n",
                "m.fit(df)\n",
                "\n",
                "future = m.make_future_dataframe(periods=365)\n",
                "future.tail()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "forecast = m.predict(future)\n",
                "forecast[(forecast['playoff'] + forecast['superbowl']).abs() > 0][\n",
                "        ['ds', 'playoff', 'superbowl']][-10:]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig = m.plot_components(forecast)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Example - `LSTM`"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Train the LSTM model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import yfinance as yf\n",
                "\n",
                "stock_symbol = 'AAPL'\n",
                "start_date = '2020-01-01'\n",
                "end_date = '2025-01-31'\n",
                "\n",
                "# Download historical data\n",
                "df_aapl = yf.download(stock_symbol, start=start_date, end=end_date).Close.reset_index()\n",
                "df_aapl.set_index('Date', inplace=True)\n",
                "df_aapl.columns = ['Close']\n",
                "df_aapl.head(10)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.plot(df_aapl.Close, label='AAPL stock price')\n",
                "plt.plot(df_aapl.Close.rolling(20).mean(), label='20-day moving average')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset_train = df_aapl.loc[:\"2024-12-31\", :].copy()\n",
                "dataset_train.tail(3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.preprocessing import MinMaxScaler\n",
                "\n",
                "sc = MinMaxScaler(feature_range = (0, 1))\n",
                "training_set = dataset_train.Close.values.reshape(-1, 1)\n",
                "training_set_scaled = sc.fit_transform(training_set)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "\n",
                "X_train = []\n",
                "y_train = []\n",
                "\n",
                "for i in range(60, len(training_set_scaled)):\n",
                "    X_train.append(training_set_scaled[i-60:i, 0])\n",
                "    y_train.append(training_set_scaled[i, 0])\n",
                "\n",
                "X_train, y_train = np.array(X_train), np.array(y_train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import Dense\n",
                "from tensorflow.keras.layers import LSTM\n",
                "from tensorflow.keras.layers import Dropout\n",
                "\n",
                "regressor = Sequential()\n",
                "regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
                "regressor.add(Dropout(0.2))\n",
                "regressor.add(LSTM(units = 50, return_sequences = True))\n",
                "regressor.add(Dropout(0.2))\n",
                "regressor.add(LSTM(units = 50, return_sequences = True))\n",
                "regressor.add(Dropout(0.2))\n",
                "regressor.add(LSTM(units = 50))\n",
                "regressor.add(Dropout(0.2))\n",
                "regressor.add(Dense(units = 1))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "regressor.compile(optimizer='adam', loss='mean_squared_error')\n",
                "regressor.fit(X_train, y_train, epochs=100, batch_size=64)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "\n",
                "tf.__version__"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Prediction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset_train.tail(3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset_test = df_aapl.loc[\"2025-01-01\":, :].copy()\n",
                "pred_inputs = pd.concat(\n",
                "    [\n",
                "        dataset_train.iloc[-60:,:],\n",
                "        dataset_test,\n",
                "    ],\n",
                "    axis=0\n",
                ")\n",
                "pred_inputs = pred_inputs.Close.values.reshape(-1, 1)\n",
                "pred_inputs[:3]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pred_inputs = sc.transform(pred_inputs)\n",
                "X_test = []\n",
                "for i in range(60, 80):\n",
                "    X_test.append(pred_inputs[i-60:i, 0])\n",
                "X_test = np.array(X_test)\n",
                "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
                "predicted_stock_price = regressor.predict(X_test)\n",
                "predicted_stock_price = sc.inverse_transform(predicted_stock_price)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset_test.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "predicted_stock_price = pd.DataFrame(predicted_stock_price[:19], index=dataset_test.index[:19], columns=['Close'])\n",
                "predicted_stock_price.head(3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.plot(dataset_test[:19], color = 'red', label = 'Real AAPL Stock Price')\n",
                "plt.plot(predicted_stock_price, color = 'blue', label = 'Predicted AAPL Stock Price')\n",
                "plt.title('AAPL Stock Price Prediction')\n",
                "plt.xlabel('Time')\n",
                "plt.ylabel('AAPL Stock Price')\n",
                "plt.xticks(rotation=45)\n",
                "plt.legend()\n",
                "plt.grid()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "pd.options.display.max_rows=None\n",
                "pd.options.display.max_columns=None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Clustering Methods\n",
                "\n",
                "![](https://www.tutorialandexample.com/wp-content/uploads/2019/11/An-example-of-a-cluster-system.png)\n",
                "\n",
                "![](https://fullcircle-cms.com/fullcircle/storage/uploads/2019/07/03/5d1ca1b9199dbClustering-GIF-2.gif)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.datasets import load_iris\n",
                "\n",
                "data = load_iris()\n",
                "X = data.data\n",
                "y = data.target\n",
                "\n",
                "plt.figure(figsize=(9, 3.5))\n",
                "plt.subplot(121)\n",
                "plt.plot(X[y==0, 2], X[y==0, 3], \"yo\", label=\"Iris setosa\")\n",
                "plt.plot(X[y==1, 2], X[y==1, 3], \"bs\", label=\"Iris versicolor\")\n",
                "plt.plot(X[y==2, 2], X[y==2, 3], \"g^\", label=\"Iris virginica\")\n",
                "plt.xlabel(\"Petal length\")\n",
                "plt.ylabel(\"Petal width\")\n",
                "plt.grid()\n",
                "plt.legend()\n",
                "\n",
                "plt.subplot(122)\n",
                "plt.scatter(X[:, 2], X[:, 3], c=\"k\", marker=\".\")\n",
                "plt.xlabel(\"Petal length\")\n",
                "plt.tick_params(labelleft=False)\n",
                "plt.gca().set_axisbelow(True)\n",
                "plt.grid()\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "A Gaussian Mixture Model (GMM) assumes that the data is generated from a mixture of multiple Gaussian distributions (bell-shaped probability distributions). Instead of assigning data points to a single cluster with hard boundaries (like K-Means), GMM assigns a probability (soft assignment) to each data point for belonging to a cluster."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "from scipy import stats\n",
                "from sklearn.mixture import GaussianMixture\n",
                "\n",
                "y_pred = GaussianMixture(n_components=3, random_state=42).fit(X).predict(X)\n",
                "\n",
                "mapping = {}\n",
                "for cluster_id in np.unique(y_pred):\n",
                "    most_common_class_id, _ = stats.mode(y[y_pred==cluster_id])\n",
                "    mapping[cluster_id] = most_common_class_id\n",
                "\n",
                "y_pred = np.array([mapping[cluster_id] for cluster_id in y_pred])\n",
                "\n",
                "plt.plot(X[y_pred==0, 2], X[y_pred==0, 3], \"yo\", label=\"Cluster 1\")\n",
                "plt.plot(X[y_pred==1, 2], X[y_pred==1, 3], \"bs\", label=\"Cluster 2\")\n",
                "plt.plot(X[y_pred==2, 2], X[y_pred==2, 3], \"g^\", label=\"Cluster 3\")\n",
                "plt.xlabel(\"Petal length\")\n",
                "plt.ylabel(\"Petal width\")\n",
                "plt.legend(loc=\"upper left\")\n",
                "plt.grid()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "(y_pred==y).sum() / len(y_pred)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Clustering is used in a wide variety of applications**, including:\n",
                "\n",
                "- Customer segmentation\n",
                "    - You can cluster your customers based on their purchases and their activity on your website. This is useful to understand who your customers are and what they need, so you can adapt your products and marketing campaigns to each segment. For example, customer segmentation can be useful in recommender systems to suggest content that other users in the same cluster enjoyed.\n",
                "\n",
                "- Data analysis\n",
                "    - When you analyze a new dataset, it can be helpful to run a clustering algorithm, and then analyze each cluster separately.\n",
                "\n",
                "- Dimensionality reduction\n",
                "    - Once a dataset has been clustered, it is usually possible to measure each instance’s affinity with each cluster; affinity is any measure of how well an instance fits into a cluster. Each instance’s feature vector x can then be replaced with the vector of its cluster affinities. If there are k clusters, then this vector is k-dimensional. The new vector is typically much lower-dimensional than the original feature vector, but it can preserve enough information for further processing.\n",
                "\n",
                "- Feature engineering\n",
                "    - The cluster affinities can often be useful as extra features. For example, we used k-means in Chapter 2 to add geographic cluster affinity features to the California housing dataset, and they helped us get better performance.\n",
                "\n",
                "- Anomaly detection (also called outlier detection)\n",
                "    - Any instance that has a low affinity to all the clusters is likely to be an anomaly. For example, if you have clustered the users of your website based on their behavior, you can detect users with unusual behavior, such as an unusual number of requests per second.\n",
                "\n",
                "- Semi-supervised learning\n",
                "    - If you only have a few labels, you could perform clustering and propagate the labels to all the instances in the same cluster. This technique can greatly increase the number of labels available for a subsequent supervised learning algorithm, and thus improve its performance.\n",
                "\n",
                "- Search engines\n",
                "    - Some search engines let you search for images that are similar to a reference image. To build such a system, you would first apply a clustering algorithm to all the images in your database; similar images would end up in the same cluster. Then when a user provides a reference image, all you’d need to do is use the trained clustering model to find this image’s cluster, and you could then simply return all the images from this cluster.\n",
                "\n",
                "- Image segmentation\n",
                "    - By clustering pixels according to their color, then replacing each pixel’s color with the mean color of its cluster, it is possible to considerably reduce the number of different colors in an image. Image segmentation is used in many object detection and tracking systems, as it makes it easier to detect the contour of each object."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## k-Means clustering\n",
                "\n",
                "### The algorithm\n",
                "* The model training process\n",
                "![](https://stanford.edu/~cpiech/cs221/img/kmeansViz.png)\n",
                "\n",
                "* Pseudo code:\n",
                "![](https://stanford.edu/~cpiech/cs221/img/kmeansMath.png)\n",
                "\n",
                "* The model training process (dynamic demonstration)\n",
                "![](https://miro.medium.com/max/1280/1*rwYaxuY-jeiVXH0fyqC_oA.gif)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Fit and predict"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "* Reference on function `make_blobs`: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## generate some sample data\n",
                "\n",
                "from sklearn.datasets import make_blobs\n",
                "\n",
                "blob_centers = np.array(\n",
                "    [[ 0.2,  2.3],\n",
                "     [-1.5 ,  2.3],\n",
                "     [-2.8,  1.8],\n",
                "     [-2.8,  2.8],\n",
                "     [-2.8,  1.3]])\n",
                "blob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])\n",
                "\n",
                "X, y = make_blobs(n_samples=2000, centers=blob_centers, cluster_std=blob_std, random_state=123)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## plot the sample data\n",
                "\n",
                "plt.scatter(X[:,0], X[:,1], c=y)\n",
                "plt.xlabel(\"$x_1$\", fontsize=14)\n",
                "plt.ylabel(\"$x_2$\", fontsize=14)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.scatter(X[:,0], X[:,1])\n",
                "plt.xlabel(\"$x_1$\", fontsize=14)\n",
                "plt.ylabel(\"$x_2$\", fontsize=14)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## Fit a K-Means clustering model\n",
                "\n",
                "from sklearn.cluster import KMeans\n",
                "\n",
                "\n",
                "k = 5\n",
                "kmeans = KMeans(n_clusters=k, random_state=123)\n",
                "# kmenas.fit(X)\n",
                "# y_pred = kmeans.predict(X)\n",
                "y_pred = kmeans.fit_predict(X)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kmeans.labels_"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kmeans.cluster_centers_"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## plot the estimated centroids with the raw data\n",
                "\n",
                "plt.scatter(X[:,0], X[:,1], c=kmeans.labels_)\n",
                "plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], c='r')\n",
                "    \n",
                "plt.xlabel(\"$x_1$\", fontsize=14)\n",
                "plt.ylabel(\"$x_2$\", fontsize=14)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## plot the sample data\n",
                "\n",
                "plt.scatter(X[:,0], X[:,1], c=y)\n",
                "plt.xlabel(\"$x_1$\", fontsize=14)\n",
                "plt.ylabel(\"$x_2$\", fontsize=14)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## predict labels of new instances\n",
                "\n",
                "X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])\n",
                "kmeans.predict(X_new)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kmeans.transform(X_new)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## visualize the new data points\n",
                "\n",
                "plt.scatter(X[:,0], X[:,1], c=kmeans.labels_)\n",
                "plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], c='r')\n",
                "plt.scatter(X_new[:,0], X_new[:,1], marker='^', c='k')\n",
                "\n",
                "\n",
                "#### label the clusters\n",
                "centers = kmeans.cluster_centers_\n",
                "labels = kmeans.predict(centers)\n",
                "for i in range(5):\n",
                "    plt.text(centers[i][0]+0.1, centers[i][1]+0.1, labels[i], fontsize=15)\n",
                "\n",
                "plt.xlabel(\"$x_1$\", fontsize=14)\n",
                "plt.ylabel(\"$x_2$\", fontsize=14)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### The decision boundaries (the `voronoi diagram`)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_data(X):\n",
                "    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)\n",
                "\n",
                "def plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n",
                "    if weights is not None:\n",
                "        centroids = centroids[weights > weights.max() / 10]\n",
                "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
                "                marker='o', s=30, linewidths=8,\n",
                "                color=circle_color, zorder=10, alpha=0.9)\n",
                "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
                "                marker='x', s=50, linewidths=2,\n",
                "                color=cross_color, zorder=11, alpha=1)\n",
                "\n",
                "def plot_decision_boundaries(clusterer, X, resolution=1000, show_centroids=True,\n",
                "                             show_xlabels=True, show_ylabels=True):\n",
                "    mins = X.min(axis=0) - 0.1\n",
                "    maxs = X.max(axis=0) + 0.1\n",
                "    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n",
                "                         np.linspace(mins[1], maxs[1], resolution))\n",
                "    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n",
                "    Z = Z.reshape(xx.shape)\n",
                "\n",
                "    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n",
                "                cmap=\"Pastel2\")\n",
                "    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n",
                "                linewidths=1, colors='k')\n",
                "    plot_data(X)\n",
                "    if show_centroids:\n",
                "        plot_centroids(clusterer.cluster_centers_)\n",
                "\n",
                "    if show_xlabels:\n",
                "        plt.xlabel(\"$x_1$\", fontsize=14)\n",
                "    else:\n",
                "        plt.tick_params(labelbottom=False)\n",
                "    if show_ylabels:\n",
                "        plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n",
                "    else:\n",
                "        plt.tick_params(labelleft=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(8, 4))\n",
                "plot_decision_boundaries(kmeans, X)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### The algorithm demonstration"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "* Initialize the centroids randomly"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## run the kmeans algorithm 1, 2, and 3 iterations, see how the centroids move around\n",
                "\n",
                "kmeans_iter1 = KMeans(n_clusters=5, init=\"random\", n_init=1,\n",
                "                     algorithm=\"lloyd\", max_iter=1, random_state=123)\n",
                "kmeans_iter2 = KMeans(n_clusters=5, init=\"random\", n_init=1,\n",
                "                     algorithm=\"lloyd\", max_iter=2, random_state=123)\n",
                "kmeans_iter3 = KMeans(n_clusters=5, init=\"random\", n_init=1,\n",
                "                     algorithm=\"lloyd\", max_iter=3, random_state=123)\n",
                "kmeans_iter1.fit(X)\n",
                "kmeans_iter2.fit(X)\n",
                "kmeans_iter3.fit(X)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10, 8))\n",
                "\n",
                "plt.subplot(321)\n",
                "plot_data(X)\n",
                "plot_centroids(kmeans_iter1.cluster_centers_, circle_color='r', cross_color='w')\n",
                "plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n",
                "plt.tick_params(labelbottom=False)\n",
                "plt.title(\"Update the centroids (initially randomly)\", fontsize=14)\n",
                "\n",
                "plt.subplot(322)\n",
                "plot_decision_boundaries(kmeans_iter1, X, show_xlabels=False, show_ylabels=False)\n",
                "plt.title(\"Label the instances\", fontsize=14)\n",
                "\n",
                "plt.subplot(323)\n",
                "plot_decision_boundaries(kmeans_iter1, X, show_centroids=False, show_xlabels=False)\n",
                "plot_centroids(kmeans_iter2.cluster_centers_)\n",
                "\n",
                "plt.subplot(324)\n",
                "plot_decision_boundaries(kmeans_iter2, X, show_xlabels=False, show_ylabels=False)\n",
                "\n",
                "plt.subplot(325)\n",
                "plot_decision_boundaries(kmeans_iter2, X, show_centroids=False)\n",
                "plot_centroids(kmeans_iter3.cluster_centers_)\n",
                "\n",
                "plt.subplot(326)\n",
                "plot_decision_boundaries(kmeans_iter3, X, show_ylabels=False)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "* Depending on the centroids initialization, the model might converge to different solutions (different local optimums)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_clusterer_comparison(clusterer1, clusterer2, X, title1=None, title2=None):\n",
                "    clusterer1.fit(X)\n",
                "    clusterer2.fit(X)\n",
                "\n",
                "    plt.figure(figsize=(10, 3.2))\n",
                "\n",
                "    plt.subplot(121)\n",
                "    plot_decision_boundaries(clusterer1, X)\n",
                "    if title1:\n",
                "        plt.title(title1, fontsize=14)\n",
                "\n",
                "    plt.subplot(122)\n",
                "    plot_decision_boundaries(clusterer2, X, show_ylabels=False)\n",
                "    if title2:\n",
                "        plt.title(title2, fontsize=14)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kmeans_rnd_init1 = KMeans(n_clusters=5, init=\"random\", n_init=1,\n",
                "                         algorithm=\"lloyd\", random_state=123)\n",
                "kmeans_rnd_init2 = KMeans(n_clusters=5, init=\"random\", n_init=1,\n",
                "                         algorithm=\"lloyd\", random_state=1)\n",
                "\n",
                "plot_clusterer_comparison(kmeans_rnd_init1, kmeans_rnd_init2, X,\n",
                "                          \"Solution 1\", \"Solution 2 (with a different random init)\")\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "* Random number generator and seed"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "np.random.randint(low=0, high=3, size=5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Inertia\n",
                "\n",
                "To select the best model, we will need a way to evaluate a K-Mean model's performance. Unfortunately, clustering is an unsupervised task, so we do not have the targets. But at least we can measure the distance between each instance and its centroid. This is the idea behind the `inertia` metric: **sum of the squared distances between each training instance and its closest centroid**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.cluster import KMeans\n",
                "\n",
                "\n",
                "k = 5\n",
                "kmeans = KMeans(n_clusters=k, random_state=123)\n",
                "kmeans.fit(X)\n",
                "kmeans.inertia_"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kmeans_rnd_init1 = KMeans(n_clusters=5, init=\"random\", n_init=1,\n",
                "                         algorithm=\"lloyd\", random_state=123)\n",
                "kmeans_rnd_init2 = KMeans(n_clusters=5, init=\"random\", n_init=1,\n",
                "                         algorithm=\"lloyd\", random_state=1)\n",
                "\n",
                "kmeans_rnd_init1.fit(X)\n",
                "kmeans_rnd_init2.fit(X)\n",
                "\n",
                "kmeans_rnd_init1.inertia_, kmeans_rnd_init2.inertia_"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kmeans.score(X) # the negative inertia"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## multiple randomization\n",
                "\n",
                "k = 5\n",
                "kmeans_10 = KMeans(n_clusters=k, n_init=10)\n",
                "kmeans_10.fit(X)\n",
                "kmeans_10.inertia_"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## manually assign initial centroids\n",
                "\n",
                "good_init = np.array([[-3, 3], [-3, 2], [-3, 1], [-1, 2], [0, 2]])\n",
                "kmeans_m = KMeans(n_clusters=5, init=good_init, n_init=1)\n",
                "kmeans_m.fit(X)\n",
                "kmeans_m.inertia_"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## visualize the best model\n",
                "\n",
                "plt.figure(figsize=(8, 4))\n",
                "plot_decision_boundaries(kmeans_10, X)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Find the optimal number of clusters - the \"elbow\" plot"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## visualize k = 4\n",
                "kmeans_3 = KMeans(n_clusters=3)\n",
                "kmeans_3.fit(X)\n",
                "\n",
                "\n",
                "plt.figure(figsize=(8, 4))\n",
                "plot_decision_boundaries(kmeans_3, X)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kmeans_k3 = KMeans(n_clusters=3)\n",
                "kmeans_k8 = KMeans(n_clusters=8)\n",
                "\n",
                "plot_clusterer_comparison(kmeans_k3, kmeans_k8, X, \"$k=3$\", \"$k=8$\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kmeans_k3.inertia_"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kmeans_k8.inertia_"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## try different k values\n",
                "\n",
                "kmeans_per_k = [KMeans(n_clusters=k).fit(X)\n",
                "                for k in range(1, 10)]\n",
                "inertias = [model.inertia_ for model in kmeans_per_k]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(8, 3.5))\n",
                "plt.plot(range(1, 10), inertias, \"bo-\")\n",
                "plt.xlabel(\"$k$\", fontsize=14)\n",
                "plt.ylabel(\"Inertia\", fontsize=14)\n",
                "plt.annotate('Elbow',\n",
                "             xy=(4, inertias[3]),\n",
                "             xytext=(0.55, 0.55),\n",
                "             textcoords='figure fraction',\n",
                "             fontsize=16,\n",
                "             arrowprops=dict(facecolor='black', shrink=0.1)\n",
                "            )\n",
                "# plt.ylim(0, 1300)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kmeans_k4 = KMeans(n_clusters=4)\n",
                "kmeans_k5 = KMeans(n_clusters=5)\n",
                "\n",
                "plot_clusterer_comparison(kmeans_k4, kmeans_k5, X, \"$k=4$\", \"$k=5$\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kmeans_k4.inertia_, kmeans_k5.inertia_"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As you can see, there is an elbow at $k=4$, which means that less clusters than that would be bad, and more clusters would not help much and might cut clusters in half. So $k=4$ is a pretty good choice. Of course in this example it is not perfect since it means that the two blobs in the lower left will be considered as just a single cluster, but it's a pretty good clustering nonetheless."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_decision_boundaries(kmeans_per_k[3], X)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Find the optimal number of clusters - the `silhouette score`"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The `silhouette score`, which is the mean silhouette coefficient over all the instances. An instance's silhouette coefficient is equal to $\\frac{(b - a)}{\\max(a, b)}$ where $a$ is the mean distance to the other instances in the same cluster (it is the mean intra-cluster distance), and $b$ is the mean nearest-cluster distance, that is the mean distance to the instances of the next closest cluster (defined as the one that minimizes $b$, excluding the instance's own cluster). The silhouette coefficient can vary between -1 and +1: a coefficient close to +1 means that the instance is well inside its own cluster and far from other clusters, while a coefficient close to 0 means that it is close to a cluster boundary, and finally a coefficient close to -1 means that the instance may have been assigned to the wrong cluster.\n",
                "\n",
                "![](https://www.researchgate.net/profile/Frans_Coenen/publication/221570710/figure/fig1/AS:670029003644935@1536758771429/Derivation-of-the-Overall-Silhouette-Coefficient-OverallSil.png)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import silhouette_score\n",
                "\n",
                "silhouette_score(X, kmeans.labels_)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kmeans.get_params"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "silhouette_scores = [silhouette_score(X, model.labels_)\n",
                "                     for model in kmeans_per_k[1:]]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(8, 3))\n",
                "plt.plot(range(2, 10), silhouette_scores, \"bo-\")\n",
                "plt.xlabel(\"$k$\", fontsize=14)\n",
                "plt.ylabel(\"Silhouette score\", fontsize=14)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**It confirms that $k=4$ is a very good choice, and it also underlines the fact that $k=5$ is quite good as well, and much better than $k=6$ or $7$.**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Limit of K-Means model\n",
                "\n",
                "* Multiple run is needed to avoid suboptimal solutions\n",
                "* Need to feed in a fixed `k` value\n",
                "* Doesn't work well when clusters have varying sizes, different densities, or nonspherical shapes\n",
                "    * It is important to scale the input features before you run the K-Means"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X1, y1 = make_blobs(n_samples=1000, centers=((4, -4), (0, 0)), random_state=42)\n",
                "X1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]]))\n",
                "X2, y2 = make_blobs(n_samples=250, centers=1, random_state=42)\n",
                "X2 = X2 + [6, -8]\n",
                "X = np.r_[X1, X2]\n",
                "y = np.r_[y1, y2]\n",
                "\n",
                "plt.scatter(X[:,0], X[:,1], s=2)\n",
                "plt.xlabel(\"$x_1$\", fontsize=14)\n",
                "plt.ylabel(\"$x_2$\", fontsize=14)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kmeans_good = KMeans(n_clusters=3, init=np.array([[-1.5, 2.5], [0.5, 0], [4, 0]]), n_init=1, random_state=123)\n",
                "kmeans_bad = KMeans(n_clusters=3, random_state=123)\n",
                "kmeans_good.fit(X)\n",
                "kmeans_bad.fit(X)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10, 3.2))\n",
                "\n",
                "plt.subplot(121)\n",
                "plot_decision_boundaries(kmeans_good, X)\n",
                "plt.title(\"'Good' Inertia = {:.1f}\".format(kmeans_good.inertia_), fontsize=14)\n",
                "\n",
                "plt.subplot(122)\n",
                "plot_decision_boundaries(kmeans_bad, X, show_ylabels=False)\n",
                "plt.title(\"'Bad' Inertia = {:.1f}\".format(kmeans_bad.inertia_), fontsize=14)\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "`Gaussian mixture models` works better in this case\n",
                "* Reference: https://scikit-learn.org/stable/modules/mixture.html"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Model Evaluation on Imbalanced Dataset"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ROC-AUC"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# example of a roc curve for a predictive model\n",
                "from sklearn.datasets import make_classification\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import roc_curve, roc_auc_score\n",
                "import matplotlib.pyplot as plt\n",
                "# generate 2 class dataset\n",
                "X, y = make_classification(n_samples=1000, n_classes=2, random_state=1)\n",
                "# split into train/test sets\n",
                "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=2)\n",
                "# fit a model\n",
                "lr = LogisticRegression(solver='lbfgs')\n",
                "lr.fit(trainX, trainy)\n",
                "# predict probabilities\n",
                "yhat = lr.predict_proba(testX)\n",
                "# retrieve just the probabilities for the positive class\n",
                "pos_probs = yhat[:, 1]\n",
                "# plot no skill roc curve\n",
                "plt.plot([0, 1], [0, 1], linestyle='--', label='No Skill')\n",
                "# calculate roc curve for the model\n",
                "fpr, tpr, thresholds = roc_curve(testy, pos_probs)\n",
                "roc_auc_lr = roc_auc_score(testy, pos_probs)\n",
                "# plot lr roc curve\n",
                "plt.plot(fpr, tpr, marker='.', label='Logistic')\n",
                "# axis labels\n",
                "plt.xlabel('False Positive Rate')\n",
                "plt.ylabel('True Positive Rate')\n",
                "# show the legend\n",
                "plt.legend()\n",
                "# show the plot\n",
                "plt.title(f'ROC curve for Logistic Regression Model. AUC = {roc_auc_lr:.2f}')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.dummy import DummyClassifier\n",
                "\n",
                "dc = DummyClassifier(strategy='stratified')\n",
                "dc.fit(trainX, trainy)\n",
                "yhat_dummy = dc.predict_proba(testX)\n",
                "pos_probs_dummy = yhat_dummy[:, 1]\n",
                "# calculate roc auc\n",
                "roc_auc_dummy = roc_auc_score(testy, pos_probs_dummy)\n",
                "print(f'ROC AUC = {roc_auc_dummy:.2f}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pos_probs_dummy = yhat_dummy[:, 1]\n",
                "\n",
                "plt.hist(pos_probs_dummy, bins=10, label='Positive Class Distribution', density=True)\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pos_probs = yhat[:, 1]\n",
                "neg_probs = yhat[:, 0]\n",
                "\n",
                "plt.hist(pos_probs, bins=100, label='Positive Class Distribution', density=True)\n",
                "plt.axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='Threshold = 0.5')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "np.sum(pos_probs>=0.5), np.sum(pos_probs<0.5)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
                "\n",
                "\n",
                "|               | Negative Prediction | Positive Prediction |\n",
                "|--------------|--------------------|--------------------|\n",
                "| **Negative Class** | True Negative (TN)  | False Positive (FP) |\n",
                "| **Positive Class** | False Negative (FN) | True Positive (TP) |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import confusion_matrix\n",
                "\n",
                "matrix = confusion_matrix(testy, lr.predict(testX))\n",
                "print(matrix)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## PR-AUC"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import precision_recall_curve\n",
                "\n",
                "# predict probabilities\n",
                "yhat = lr.predict_proba(testX)\n",
                "# retrieve just the probabilities for the positive class\n",
                "pos_probs = yhat[:, 1]\n",
                "# calculate the no skill line as the proportion of the positive class\n",
                "no_skill = len(y[y==1]) / len(y)\n",
                "# plot the no skill precision-recall curve\n",
                "plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
                "# calculate model precision-recall curve\n",
                "precision, recall, _ = precision_recall_curve(testy, pos_probs)\n",
                "# plot the model precision-recall curve\n",
                "plt.plot(recall, precision, marker='.', label='Logistic')\n",
                "# axis labels\n",
                "plt.xlabel('Recall')\n",
                "plt.ylabel('Precision')\n",
                "# show the legend\n",
                "plt.legend()\n",
                "# show the plot\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import auc\n",
                "\n",
                "auc_score = auc(recall, precision)\n",
                "print('PR AUC: %.3f' % auc_score)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n",
                "precision_dummy, recall_dummy, _ = precision_recall_curve(testy, pos_probs_dummy)\n",
                "auc_score = auc(recall_dummy, precision_dummy)\n",
                "print('PR AUC: %.3f' % auc_score)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Severly Imbalanced Data\n",
                "\n",
                "### ROC-AUC"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.datasets import make_classification\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.dummy import DummyClassifier\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import roc_curve\n",
                "from sklearn.metrics import roc_auc_score\n",
                "import matplotlib.pyplot as plt\n",
                " \n",
                "# plot no skill and model roc curves\n",
                "def plot_roc_curve(test_y, naive_probs, model_probs):\n",
                "\t# plot naive skill roc curve\n",
                "\tfpr, tpr, _ = roc_curve(test_y, naive_probs)\n",
                "\tplt.plot(fpr, tpr, linestyle='--', label='No Skill')\n",
                "\t# plot model roc curve\n",
                "\tfpr, tpr, _ = roc_curve(test_y, model_probs)\n",
                "\tplt.plot(fpr, tpr, marker='.', label='Logistic')\n",
                "\t# axis labels\n",
                "\tplt.xlabel('False Positive Rate')\n",
                "\tplt.ylabel('True Positive Rate')\n",
                "\t# show the legend\n",
                "\tplt.legend()\n",
                "\t# show the plot\n",
                "\tplt.show()\n",
                " \n",
                "# generate 2 class dataset\n",
                "X, y = make_classification(n_samples=1000, n_classes=2, weights=[0.99, 0.01], random_state=1)\n",
                "# split into train/test sets with same class ratio\n",
                "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=2, stratify=y)\n",
                "# no skill model, stratified random class predictions\n",
                "dc = DummyClassifier(strategy='stratified')\n",
                "dc.fit(trainX, trainy)\n",
                "yhat_dc = dc.predict_proba(testX)\n",
                "naive_probs = yhat_dc[:, 1]\n",
                "# calculate roc auc\n",
                "roc_auc_dc = roc_auc_score(testy, naive_probs)\n",
                "print('No Skill ROC AUC %.3f' % roc_auc_dc)\n",
                "# skilled model\n",
                "lr = LogisticRegression(solver='lbfgs')\n",
                "lr.fit(trainX, trainy)\n",
                "yhat = lr.predict_proba(testX)\n",
                "lr_probs = yhat[:, 1]\n",
                "# calculate roc auc\n",
                "roc_auc = roc_auc_score(testy, lr_probs)\n",
                "print('Logistic ROC AUC %.3f' % roc_auc)\n",
                "# plot roc curves\n",
                "plot_roc_curve(testy, naive_probs, lr_probs)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### PR-AUC"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import precision_recall_curve\n",
                "from sklearn.metrics import auc\n",
                " \n",
                "# plot no skill and model precision-recall curves\n",
                "def plot_pr_curve(test_y, model_probs):\n",
                "\t# calculate the no skill line as the proportion of the positive class\n",
                "\tno_skill = len(test_y[test_y==1]) / len(test_y)\n",
                "\t# plot the no skill precision-recall curve\n",
                "\tplt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
                "\t# plot model precision-recall curve\n",
                "\tprecision, recall, _ = precision_recall_curve(testy, model_probs)\n",
                "\tplt.plot(recall, precision, marker='.', label='Logistic')\n",
                "\t# axis labels\n",
                "\tplt.xlabel('Recall')\n",
                "\tplt.ylabel('Precision')\n",
                "\t# show the legend\n",
                "\tplt.legend()\n",
                "\t# show the plot\n",
                "\tplt.show()\n",
                " \n",
                "naive_probs = yhat_dc[:, 1]\n",
                "# calculate the precision-recall auc\n",
                "precision_dc, recall_dc, _ = precision_recall_curve(testy, naive_probs)\n",
                "auc_score_dc = auc(recall_dc, precision_dc)\n",
                "print('No Skill PR AUC: %.3f' % auc_score_dc)\n",
                "\n",
                "model_probs = yhat[:, 1]\n",
                "# calculate the precision-recall auc\n",
                "precision_lr, recall_lr, _ = precision_recall_curve(testy, model_probs)\n",
                "auc_score_lr = auc(recall_lr, precision_lr)\n",
                "print('Logistic PR AUC: %.3f' % auc_score_lr)\n",
                "# plot precision-recall curves\n",
                "plot_pr_curve(testy, model_probs)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To explain why the ROC and PR curves tell a different story, recall that the PR curve focuses on the minority class, whereas the ROC curve covers both classes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pos_probs = yhat[:, 1]\n",
                "plt.hist(pos_probs, bins=100)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.6"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
