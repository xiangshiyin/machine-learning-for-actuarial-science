{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(**Click the icon below to open this notebook in Colab**)\n",
    "\n",
    "[![Open InColab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/xiangshiyin/machine-learning-for-actuarial-science/blob/main/2025-spring/week06/notebook/demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In our last class, we explored the Titanic dataset, examined it from multiple perspectives, and applied various feature engineering techniques to enhance its explanatory variables. Today, we will continue working with the Titanic dataset, focusing on model training and evaluation techniques to gain deeper insights into predictive modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset\n",
    "\n",
    "https://www.kaggle.com/competitions/titanic/data\n",
    "- **The Titanic** https://en.wikipedia.org/wiki/Titanic\n",
    "\n",
    "| Variable   | Definition                                | Key                                  |\n",
    "|------------|-------------------------------------------|--------------------------------------|\n",
    "| survival   | Survival                                 | 0 = No, 1 = Yes                     |\n",
    "| pclass     | Ticket class                             | 1 = 1st, 2 = 2nd, 3 = 3rd           |\n",
    "| sex        | Sex                                      |                                      |\n",
    "| Age        | Age in years                             |                                      |\n",
    "| sibsp      | # of siblings / spouses aboard the Titanic |                                      |\n",
    "| parch      | # of parents / children aboard the Titanic |                                      |\n",
    "| ticket     | Ticket number                            |                                      |\n",
    "| fare       | Passenger fare                           |                                      |\n",
    "| cabin      | Cabin number                             |                                      |\n",
    "| embarked   | Port of Embarkation                     | C = Cherbourg, Q = Queenstown, S = Southampton |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/titanic/train.csv')\n",
    "test = pd.read_csv('../data/titanic/test.csv')\n",
    "\n",
    "# convert all column names to lower cases\n",
    "train.columns = train.columns.str.lower()\n",
    "test.columns = test.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Streamline the data transformations\n",
    "\n",
    "Here are the data exploration and transformation strategies we used so far:\n",
    "* Quick survey across key variables\n",
    "* Detect and address data anomalies\n",
    "  * Missing values\n",
    "  * Outliers\n",
    "* Feature engineering\n",
    "  * Encode categorical variables\n",
    "  * Normalize numerical variables\n",
    "  * Create new features with stronger predictive power\n",
    "\n",
    "Data exploration process is typically iterative and complex. Once we have a good understanding of the data and some potential strategies to apply in the feature engineering process, we need to make sure these transformation strategies can be easily and consistently applied to new datasets, such as the test set and new batches of data for model retraining. This requires a systematic approach to streamline the data transformations so that we don't need to start from scratch and repeat the same steps for each new dataset. This is especially important in the real-world scenario where we want to productionalize and automate the data transformation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing value imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_num_values(df):\n",
    "    \"\"\"\n",
    "    Impute missing values in numerical columns of a DataFrame using the median of each column.\n",
    "\n",
    "    Args:\n",
    "    df (pandas.DataFrame): The DataFrame to impute missing values in.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The DataFrame with missing values imputed.\n",
    "    \"\"\"\n",
    "    # Select only the numerical columns\n",
    "    num_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    # Impute missing values with the median of each column\n",
    "    for col in num_cols:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = impute_missing_num_values(train)\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical variables could have missing values too\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.mode.html\n",
    "\n",
    "def impute_missing_cat_values(df, ignore_list):\n",
    "    \"\"\"\n",
    "    Impute missing categorical values with the most frequent value.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the data.\n",
    "        ignore_list (list): List of column names to ignore.    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with imputed missing categorical values.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if col not in ignore_list:\n",
    "            if df[col].dtype == 'object':\n",
    "                df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = impute_missing_cat_values(train, ignore_list=['cabin'])\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ **Attention:** We will treat the missing values in `cabin` in the feature engineering step!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "- Encode categorical features\n",
    "- Normalize numerical features\n",
    "- Create new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The categorical variables in the datasets\n",
    "\n",
    "cat_cols = [\n",
    "    col\n",
    "    for col in train.columns if train[col].dtype == \"object\"\n",
    "] \n",
    "print(cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply onehot encoding to the categorical columns\n",
    "# use the sklearn library\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def onehot_encode(df, ignore_list):\n",
    "    cat_cols = [\n",
    "        col for col in df.columns if col not in ignore_list and df[col].dtype == 'object'\n",
    "    ]\n",
    "    encoder = OneHotEncoder()\n",
    "    encoded = encoder.fit_transform(df[cat_cols])\n",
    "    encoded_df = pd.DataFrame(encoded.toarray(), columns=encoder.get_feature_names_out(cat_cols))\n",
    "    df = pd.concat([df, encoded_df], axis=1)\n",
    "    df = df.drop(cat_cols, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = onehot_encode(train, ignore_list=['cabin', 'name', 'ticket'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform numeric features, log transform `fare`\n",
    "import numpy as np\n",
    "\n",
    "def log_transform(df, features, drop=False):\n",
    "    for feature in features:\n",
    "        df[feature+'_log'] = np.log1p(df[feature]) \n",
    "    if drop:\n",
    "        df = df.drop(features, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = log_transform(train, features=['fare'])\n",
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new features\n",
    "import re\n",
    "\n",
    "def create_features(df):\n",
    "    df['has_cabin'] = df['cabin'].apply(lambda x: 0 if type(x) == float else 1)\n",
    "    df['family_size'] = df['sibsp'] + df['parch'] + 1\n",
    "    df['is_alone'] = df['family_size'].apply(lambda x: 1 if x == 1 else 0)\n",
    "    df['title'] = df['name'].apply(lambda x: re.search('([A-Z][a-z]+)\\\\.', x).group(1))\n",
    "    df['cabin'] = df['cabin'].fillna('U0')\n",
    "    df['deck'] = df['cabin'].apply(lambda x: re.search('([A-Z]+)', x).group(1))\n",
    "    df['name_len_cat'] = df['name'].apply(lambda x: 0 if len(x) <= 23 else 1 if len(x) <= 28 else 2 if len(x) <= 40 else 3)\n",
    "    df['age_cat'] = df['age'].apply(lambda x: 0 if x <= 14 else 1 if x <= 30 else 2 if x <= 40 else 3 if x <= 50 else 4 if x <= 60 else 5)\n",
    "    df['fare_log_cat'] = df['fare_log'].apply(lambda x: 0 if x <= 2.7 else 1 if x <= 3.2 else 2 if x <= 3.6 else 3)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = create_features(train)\n",
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def impute_missing_num_values(df):\n",
    "    \"\"\"\n",
    "    Impute missing values in numerical columns of a DataFrame using the median of each column.\n",
    "\n",
    "    Args:\n",
    "    df (pandas.DataFrame): The DataFrame to impute missing values in.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The DataFrame with missing values imputed.\n",
    "    \"\"\"\n",
    "    # Select only the numerical columns\n",
    "    num_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    # Impute missing values with the median of each column\n",
    "    for col in num_cols:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "    return df\n",
    "\n",
    "def impute_missing_cat_values(df, ignore_list):\n",
    "    \"\"\"\n",
    "    Impute missing categorical values with the most frequent value.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the data.\n",
    "        ignore_list (list): List of column names to ignore.    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with imputed missing categorical values.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if col not in ignore_list:\n",
    "            if df[col].dtype == 'object':\n",
    "                df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    return df\n",
    "\n",
    "def log_transform(df, features, drop=False):\n",
    "    for feature in features:\n",
    "        df[feature+'_log'] = np.log1p(df[feature]) \n",
    "    if drop:\n",
    "        df = df.drop(features, axis=1)\n",
    "    return df\n",
    "\n",
    "def create_features(df):\n",
    "    df['has_cabin'] = df['cabin'].apply(lambda x: 0 if type(x) == float else 1)\n",
    "    df['family_size'] = df['sibsp'] + df['parch'] + 1\n",
    "    df['is_alone'] = df['family_size'].apply(lambda x: 1 if x == 1 else 0)\n",
    "    df['title'] = df['name'].apply(lambda x: re.search('([A-Z][a-z]+)\\\\.', x).group(1))\n",
    "    df['cabin'] = df['cabin'].fillna('U0')\n",
    "    df['deck'] = df['cabin'].apply(lambda x: re.search('([A-Z]+)', x).group(1))\n",
    "    df['name_len_cat'] = df['name'].apply(lambda x: 0 if len(x) <= 23 else 1 if len(x) <= 28 else 2 if len(x) <= 40 else 3)\n",
    "    df['age_cat'] = df['age'].apply(lambda x: 0 if x <= 14 else 1 if x <= 30 else 2 if x <= 40 else 3 if x <= 50 else 4 if x <= 60 else 5)\n",
    "    df['fare_log_cat'] = df['fare_log'].apply(lambda x: 0 if x <= 2.7 else 1 if x <= 3.2 else 2 if x <= 3.6 else 3)\n",
    "    return df\n",
    "\n",
    "def load_data():\n",
    "    train = pd.read_csv('../data/titanic/train.csv')\n",
    "    test = pd.read_csv('../data/titanic/test.csv')\n",
    "    # convert all column names to lower cases\n",
    "    train.columns = train.columns.str.lower()\n",
    "    test.columns = test.columns.str.lower()    \n",
    "    return train, test\n",
    "\n",
    "def transform_data(df, encoder=None):\n",
    "    df = impute_missing_num_values(df)\n",
    "    df = impute_missing_cat_values(df, ['cabin', 'embarked'])\n",
    "    df = log_transform(df, ['fare'])\n",
    "    df = create_features(df)\n",
    "    \n",
    "    cat_attributes = ['sex', 'embarked', 'title', 'deck']\n",
    "    if not encoder:\n",
    "        encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "        encoder.fit(df[cat_attributes])\n",
    "    encoded = encoder.transform(df[cat_attributes])\n",
    "    df = pd.concat([df, pd.DataFrame(encoded, columns=encoder.get_feature_names_out(cat_attributes))], axis=1)\n",
    "    # drop columns that are not needed\n",
    "    df = df.drop([\n",
    "        'name', 'ticket', 'cabin', 'fare'\n",
    "        # , 'age', 'fare', 'sibsp', 'parch'\n",
    "    ] + cat_attributes, axis=1)\n",
    "    return df, encoder\n",
    "\n",
    "train, test = load_data()\n",
    "train, encoder = transform_data(train)\n",
    "test, _ = transform_data(test, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a simple logistic regression model to predict the survival label\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(\n",
    "    train.drop(columns=['survived', 'passengerid']), # everything except the survival label\n",
    "    train['survived'] # the survival label\n",
    ")\n",
    "\n",
    "pred = lr.predict(test.drop(columns=['passengerid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.concat([test['passengerid'], pd.DataFrame(pred, columns=['survived'])], axis=1)\n",
    "df_submission.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.to_csv('../data/titanic/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the prediction results\n",
    "\n",
    "https://www.kaggle.com/competitions/titanic/overview/evaluation\n",
    "![](https://almablog-media.s3.ap-south-1.amazonaws.com/image_14_4f4fc2cf7d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(columns=['survived', 'passengerid'])\n",
    "y_train = train['survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FLAML** - https://github.com/microsoft/FLAML/tree/main\n",
    "- `pip install flaml[automl]`\n",
    "- [Documentation](https://microsoft.github.io/FLAML/docs/Getting-Started)\n",
    "- Best practices [[link](https://learn.microsoft.com/en-us/fabric/data-science/automated-machine-learning-fabric#automl-workflow)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flaml import AutoML\n",
    "\n",
    "automl = AutoML()\n",
    "settings = {\n",
    "    \"time_budget\": 60,  # total running time in seconds\n",
    "    \"metric\": 'accuracy', \n",
    "                        # check the documentation for options of metrics (https://microsoft.github.io/FLAML/docs/Use-Cases/Task-Oriented-AutoML#optimization-metric)\n",
    "    \"task\": 'classification',  # task type\n",
    "    \"log_file_name\": 'airlines_experiment.log',  # flaml log file\n",
    "    \"seed\": 7654321,    # random seed\n",
    "    \"ensemble\": False,\n",
    "}\n",
    "automl.fit(X_train, y_train, **settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test.drop(columns=['passengerid'])\n",
    "y_test_pred = automl.predict(X_test)\n",
    "df_submission = pd.concat([test['passengerid'], pd.DataFrame(y_test_pred, columns=['survived'])], axis=1)\n",
    "df_submission.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.to_csv('../data/titanic/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl.best_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl.best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl.model.estimator.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance values\n",
    "importances = automl.model.estimator.feature_importances_\n",
    "# Get feature names from the dataset\n",
    "feature_names = X_train.columns\n",
    "# Create a DataFrame for better visualization\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "# Sort by importance (highest first)\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))  # Adjust figure size for better readability\n",
    "plt.bar(feature_importance_df['Feature'], feature_importance_df['Importance'], color='skyblue')\n",
    "\n",
    "# Rotate feature names for readability\n",
    "plt.xticks(rotation=45, ha='right')  # Tilt labels 45 degrees and align to the right\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('Feature Name')\n",
    "plt.ylabel('Importance Score')\n",
    "plt.title('Feature Importance')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluations\n",
    "\n",
    "Model evaluations can be approached from multiple perspectives:\n",
    "\n",
    "- From the Perspective of Evaluation Metrics:\n",
    "  - **Prediction Quality**: How well does the model predict or classify new data?\n",
    "  - **Interpretability**: How easily can the model’s predictions be understood and explained?\n",
    "- From the Perspective of the ML Workflow:\n",
    "  - **Offline Evaluations**: Assessment of model performance on the training and test datasets.\n",
    "  - **Online Evaluations**: Evaluation of model performance using live, production data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Quality\n",
    "\n",
    "### Classification Problems\n",
    "#### Accuracy\n",
    "$$\n",
    "\\mathrm{Accuracy} = \\frac{\\mathrm{TP} + \\mathrm{TN}}{\\mathrm{TP} + \\mathrm{FP} + \\mathrm{FN} + \\mathrm{TN}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation Classification Accuracy\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pd.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=7)\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, Y_train)\n",
    "\n",
    "Y_test_pred = lr.predict(X_test)\n",
    "accuracy = accuracy_score(Y_test, Y_test_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/1200px-Precisionrecall.svg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision = precision_score(Y_test, Y_test_pred)\n",
    "recall = recall_score(Y_test, Y_test_pred)\n",
    "\n",
    "print(f\"precision: {precision}\")\n",
    "print(f\"recall: {recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](data:image/jpeg;base64,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)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score = f1_score(Y_test, Y_test_pred)\n",
    "print(f\"f1 score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "precision = np.linspace(0, 1, 1000)\n",
    "recalls = np.linspace(0, 1, 10)\n",
    "f1s = [\n",
    "    2 * precision * recall / (precision + recall)\n",
    "    for recall in recalls\n",
    "]\n",
    "for f1, recall in zip(f1s, recalls):\n",
    "    plt.plot(precision, f1, label=f\"recall={recall:.1f}%\")\n",
    "\n",
    "plt.xlabel(\"Precision\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Loss (Log Loss)\n",
    "\n",
    "For binary classification problem, the log loss is defined as:\n",
    "$$\n",
    "L(y, p) = - \\frac{1}{N} \\sum_{i=1}^N \\left[ y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the score from scratch\n",
    "import numpy as np\n",
    "Y_test_pred_prob = lr.predict_proba(X_test)[:, 1]\n",
    "log_loss_score = np.sum(-np.log(np.where(Y_test == 1, Y_test_pred_prob, 1 - Y_test_pred_prob))) / len(Y_test)\n",
    "print(f\"Calculated Log Loss Score: {log_loss_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "log_loss_score2 = log_loss(Y_test, Y_test_pred_prob)\n",
    "print(f\"Log Loss Score: {log_loss_score2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Area under ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, _ = roc_curve(Y_test, Y_test_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# plot the ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line for random guessing\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "- https://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "matrix = confusion_matrix(Y_test, Y_test_pred)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Report\n",
    "- https://scikit-learn.org/stable/modules/model_evaluation.html#classification-report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(Y_test, Y_test_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate a synthetic regression dataset\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Print the result\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "\n",
    "# Plot the regression line\n",
    "plt.scatter(X_test, y_test, color='blue', label=\"Actual values\")\n",
    "plt.plot(X_test, y_pred, color='red', linewidth=2, label=\"Predicted values\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Target\")\n",
    "plt.title(\"Linear Regression with MSE Calculation\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R2 score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `k-fold` cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Generate a synthetic regression dataset\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "\n",
    "# Define the number of folds\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store MSE for each fold\n",
    "mse_values = []\n",
    "\n",
    "# Perform K-Fold Cross-Validation\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(X), 1):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Train a linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate Mean Squared Error\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mse_values.append(mse)\n",
    "\n",
    "    print(f\"Fold {fold}: MSE = {mse:.2f}\")\n",
    "\n",
    "# Calculate overall mean MSE\n",
    "overall_mse = np.mean(mse_values)\n",
    "print(f\"\\nOverall Mean Squared Error across all folds: {overall_mse:.2f}\")\n",
    "\n",
    "# Plot MSE values for each fold\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(mse_values) + 1), mse_values, marker='o', linestyle='-', color='b', label=\"MSE per fold\")\n",
    "plt.axhline(y=overall_mse, color='r', linestyle='--', label=f\"Overall Mean MSE = {overall_mse:.2f}\")\n",
    "plt.xlabel(\"Fold Number\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.title(\"K-Fold Cross-Validation MSE\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "# Define the number of folds\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "LR = LinearRegression()\n",
    "k_fold_mse = model_selection.cross_val_score(\n",
    "    LR, X, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "print(\"MSE: %.3f (%.3f)\" % (k_fold_mse.mean(), k_fold_mse.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train.drop(columns=['survived', 'passengerid'])\n",
    "y = train['survived']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=123)\n",
    "rf = RandomForestClassifier(n_estimators=5, max_depth=3, random_state=234)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf.feature_importances_\n",
    "df_importances = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': importances\n",
    "})\n",
    "df_importances = df_importances.sort_values(by='Importance', ascending=False)\n",
    "df_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test2 = test.drop(columns=['passengerid'])\n",
    "y_pred2 = automl.predict(X_test2)\n",
    "df_submission = pd.concat([test['passengerid'], pd.DataFrame(y_pred2, columns=['survived'])], axis=1)\n",
    "df_submission.to_csv('../data/titanic/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.Explainer(rf)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Extract SHAP values for the positive class (class 1)\n",
    "shap_values_class1 = shap_values[:,:,1]\n",
    "\n",
    "# Visualize global feature importance for class 1\n",
    "shap.summary_plot(shap_values_class1, X_test, feature_names=feature_names, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(rf)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "shap_values_class1 = shap_values[:,:,1]\n",
    "explanation = shap.Explanation(\n",
    "    values=shap_values_class1,\n",
    "    base_values=explainer.expected_value[1],  # Base value for class 1\n",
    "    data=X_test.values,  # Input data (as a NumPy array)\n",
    "    feature_names=X_test.columns.tolist()  # Feature names\n",
    ")\n",
    "shap.plots.waterfall(explanation[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = rf.predict_proba(X_test)\n",
    "y_pred_prob[1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
