{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifiers\n",
    "\n",
    "`VotingClassifier`: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads_proba = 0.51\n",
    "coin_tosses = (np.random.rand(10000, 1) < heads_proba).astype(np.int32)\n",
    "cumulative_heads_ratio = np.cumsum(coin_tosses, axis=0) / np.arange(1, 10001).reshape(-1, 1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(cumulative_heads_ratio)\n",
    "plt.plot([0, 10000], [0.51, 0.51], \"k--\", linewidth=2, label=\"51%\")\n",
    "plt.plot([0, 10000], [0.5, 0.5], \"k-\", label=\"50%\")\n",
    "plt.xlabel(\"Number of coin tosses\")\n",
    "plt.ylabel(\"Heads ratio\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.axis([0, 10000, 0.42, 0.58])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads_proba = 0.51\n",
    "coin_tosses = (np.random.rand(10000, 10) < heads_proba).astype(np.int32)\n",
    "cumulative_heads_ratio = np.cumsum(coin_tosses, axis=0) / np.arange(1, 10001).reshape(-1, 1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(cumulative_heads_ratio)\n",
    "plt.plot([0, 10000], [0.51, 0.51], \"k--\", linewidth=2, label=\"51%\")\n",
    "plt.plot([0, 10000], [0.5, 0.5], \"k-\", label=\"50%\")\n",
    "plt.xlabel(\"Number of coin tosses\")\n",
    "plt.ylabel(\"Heads ratio\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.axis([0, 10000, 0.42, 0.58])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coin_tosses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X[y==0,0],X[y==0,1],c='r',marker='o',edgecolors='black')\n",
    "plt.scatter(X[y==1,0],X[y==1,1],c='b',marker='^',edgecolors='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm_clf = SVC(gamma=\"scale\", random_state=42)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='hard'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_decision_boundary(clf, X, y, axes=[-1.5, 2.45, -1, 1.5], alpha=0.5, contour=True):\n",
    "    x1s = np.linspace(axes[0], axes[1], 100)\n",
    "    x2s = np.linspace(axes[2], axes[3], 100)\n",
    "    x1, x2 = np.meshgrid(x1s, x2s)\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n",
    "    if contour:\n",
    "        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n",
    "        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n",
    "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", alpha=alpha)\n",
    "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", alpha=alpha)\n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(r\"$x_1$\", fontsize=18)\n",
    "    plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 10), sharey=True)\n",
    "for idx, clf in enumerate((log_clf, rnd_clf, svm_clf, voting_clf)):\n",
    "    plt.sca(axes[idx//2][idx%2])\n",
    "    plot_decision_boundary(clf, X, y)\n",
    "    plt.title(clf.__class__.__name__, fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "\n",
    "### Bagging Classifier\n",
    "A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "y_pred_tree = tree_clf.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_pred_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(random_state=42), \n",
    "    n_estimators=20,\n",
    "    max_samples=100, \n",
    "    bootstrap=True, \n",
    "    random_state=42\n",
    ")\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get accuracy of each individual estimator\n",
    "accuracies = []\n",
    "for estimator in bag_clf.estimators_:\n",
    "    y_pred = estimator.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(range(1, len(accuracies) + 1), accuracies, color='skyblue', label='Base Estimator Accuracy')\n",
    "plt.xlabel('Estimator Index')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy of Each Base Estimator')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get accuracy of each individual estimator\n",
    "accuracies = []\n",
    "for estimator in bag_clf.estimators_:\n",
    "    y_pred = estimator.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(range(1, len(accuracies) + 1), accuracies, color='skyblue', label='Base Estimator Accuracy')\n",
    "plt.axhline(y=bag_clf.score(X_test, y_test), color='red', linestyle='--', label='Bagging Classifier Accuracy')\n",
    "plt.xlabel('Estimator Index')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy of Each Base Estimator')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, axes = plt.subplots(ncols=2, figsize=(10,4), sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plot_decision_boundary(tree_clf, X, y)\n",
    "plt.title(\"Decision Tree\", fontsize=14)\n",
    "plt.sca(axes[1])\n",
    "plot_decision_boundary(bag_clf, X, y)\n",
    "plt.title(\"Decision Trees with Bagging\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Ensemble`'s predictions will likely generalize much better (with smaller variance) than a single Decision Tree!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest \n",
    "\n",
    "`Random Forest` is essentially an extension of Bagging, but with an additional layer of feature randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(\n",
    "    n_estimators=20, \n",
    "    max_leaf_nodes=16, \n",
    "    max_features=\"sqrt\",\n",
    "    random_state=42\n",
    ")\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_pred_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s say we have **1000 training samples**:\n",
    "- Each tree gets **1000 samples**, but some are repeated.\n",
    "- The probability that a specific sample **is not selected** in one draw is:  \n",
    "  $$\n",
    "  P(\\text{not selected}) = 1 - \\frac{1}{N} = 1 - \\frac{1}{1000} = 0.999\n",
    "  $$\n",
    "- The probability that a sample is **never selected in N draws** is:  \n",
    "  $$\n",
    "  (1 - \\frac{1}{N})^N \\approx e^{-1} \\approx 0.3679\n",
    "  $$\n",
    "- This means **about 37% of the training data is left out per tree**, forming the **out-of-bag (OOB) samples**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "et_clf = ExtraTreesClassifier(\n",
    "    n_estimators=20, \n",
    "    max_leaf_nodes=16, \n",
    "    max_features=\"sqrt\",\n",
    "    random_state=42\n",
    ")\n",
    "et_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_et = et_clf.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_pred_et)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, axes = plt.subplots(ncols=2, figsize=(10,4), sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plot_decision_boundary(rnd_clf, X, y)\n",
    "plt.title(\"Random Forest\", fontsize=14)\n",
    "plt.sca(axes[1])\n",
    "plot_decision_boundary(et_clf, X, y)\n",
    "plt.title(\"Extra Tree\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "**Boosting** (originally called hypothesis boosting) refers to any ensemble method that can combine several weak learners into a strong learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\text{Error Rate of Weak Classifier:} \\quad\n",
    "\\epsilon_t = \\sum_{i=1}^{N} w_i I(y_i \\neq h_t(x_i)) \\\\\n",
    "\n",
    "&\\text{Weight of Weak Classifier:} \\quad\n",
    "\\alpha_t = \\frac{1}{2} \\eta \\ln \\left(\\frac{1 - \\epsilon_t}{\\epsilon_t} \\right) \\\\\n",
    "\n",
    "&\\text{Final Prediction:} \\quad\n",
    "H(x) = \\text{sign} \\left\\{ \\sum_{t=1}^{T} \\alpha_t h_t(x) \\right\\}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(X_train)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "for subplot, learning_rate in ((0, 1), (1, 0.5)):\n",
    "    print(learning_rate)\n",
    "    sample_weights = np.ones(m) / m\n",
    "    plt.sca(axes[subplot])\n",
    "    for i in range(5):\n",
    "        svm_clf = SVC(C=0.2, gamma=0.6, random_state=42)\n",
    "        svm_clf.fit(X_train, y_train, sample_weight=sample_weights * m)\n",
    "        y_pred = svm_clf.predict(X_train)\n",
    "\n",
    "        error_weights = sample_weights[y_pred != y_train].sum()\n",
    "        r = error_weights / sample_weights.sum()  # equation 7-1\n",
    "        alpha = learning_rate * np.log((1 - r) / r)  # equation 7-2\n",
    "        sample_weights[y_pred != y_train] *= np.exp(alpha)  # equation 7-3\n",
    "        sample_weights /= sample_weights.sum()  # normalization step\n",
    "\n",
    "        plot_decision_boundary(svm_clf, X_train, y_train, alpha=0.4)\n",
    "        plt.title(f\"learning_rate = {learning_rate}\")\n",
    "    if subplot == 0:\n",
    "        plt.text(-0.75, -0.95, \"1\", fontsize=16)\n",
    "        plt.text(-1.05, -0.95, \"2\", fontsize=16)\n",
    "        plt.text(1.0, -0.95, \"3\", fontsize=16)\n",
    "        plt.text(-1.45, -0.5, \"4\", fontsize=16)\n",
    "        plt.text(1.36,  -0.95, \"5\", fontsize=16)\n",
    "    else:\n",
    "        plt.ylabel(\"\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=20,\n",
    "    learning_rate=0.5, random_state=42)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_ada = ada_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred_ada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(ada_clf, X_train, y_train, alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "Just like AdaBoost, gradient boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every iteration like AdaBoost does, this method tries to fit the new predictor to the residual errors made by the previous predictor.\n",
    "\n",
    "In case of a gradient boosting regressor, this yields predictions with the following form:\n",
    "$$\n",
    "\\hat{y}_i = \\sum_{k=1}^{n_{iter}} h_m(x_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3 * X[:, 0] ** 2 + 0.05 * np.random.randn(100)  # y = 3x² + Gaussian noise\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = y - tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=43)\n",
    "tree_reg2.fit(X, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=44)\n",
    "tree_reg3.fit(X, y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([[-0.4], [0.], [0.5]])\n",
    "sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(regressors, X, y, axes, style,\n",
    "                     label=None, data_style=\"b.\", data_label=None):\n",
    "    x1 = np.linspace(axes[0], axes[1], 500)\n",
    "    y_pred = sum(regressor.predict(x1.reshape(-1, 1))\n",
    "                 for regressor in regressors)\n",
    "    plt.plot(X[:, 0], y, data_style, label=data_label)\n",
    "    plt.plot(x1, y_pred, style, linewidth=2, label=label)\n",
    "    if label or data_label:\n",
    "        plt.legend(loc=\"upper center\")\n",
    "    plt.axis(axes)\n",
    "\n",
    "plt.figure(figsize=(11, 11))\n",
    "\n",
    "plt.subplot(3, 2, 1)\n",
    "plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.2, 0.8], style=\"g-\",\n",
    "                 label=\"$h_1(x_1)$\", data_label=\"Training set\")\n",
    "plt.ylabel(\"$y$  \", rotation=0)\n",
    "plt.title(\"Residuals and tree predictions\")\n",
    "\n",
    "plt.subplot(3, 2, 2)\n",
    "plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.2, 0.8], style=\"r-\",\n",
    "                 label=\"$h(x_1) = h_1(x_1)$\", data_label=\"Training set\")\n",
    "plt.title(\"Ensemble predictions\")\n",
    "\n",
    "plt.subplot(3, 2, 3)\n",
    "plot_predictions([tree_reg2], X, y2, axes=[-0.5, 0.5, -0.4, 0.6], style=\"g-\",\n",
    "                 label=\"$h_2(x_1)$\", data_style=\"k+\",\n",
    "                 data_label=\"Residuals: $y - h_1(x_1)$\")\n",
    "plt.ylabel(\"$y$  \", rotation=0)\n",
    "\n",
    "plt.subplot(3, 2, 4)\n",
    "plot_predictions([tree_reg1, tree_reg2], X, y, axes=[-0.5, 0.5, -0.2, 0.8],\n",
    "                  style=\"r-\", label=\"$h(x_1) = h_1(x_1) + h_2(x_1)$\")\n",
    "\n",
    "plt.subplot(3, 2, 5)\n",
    "plot_predictions([tree_reg3], X, y3, axes=[-0.5, 0.5, -0.4, 0.6], style=\"g-\",\n",
    "                 label=\"$h_3(x_1)$\", data_style=\"k+\",\n",
    "                 data_label=\"Residuals: $y - h_1(x_1) - h_2(x_1)$\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$y$  \", rotation=0)\n",
    "\n",
    "plt.subplot(3, 2, 6)\n",
    "plot_predictions([tree_reg1, tree_reg2, tree_reg3], X, y,\n",
    "                 axes=[-0.5, 0.5, -0.2, 0.8], style=\"r-\",\n",
    "                 label=\"$h(x_1) = h_1(x_1) + h_2(x_1) + h_3(x_1)$\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directly apply a gradient boosting regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3,\n",
    "                                 learning_rate=1.0, random_state=42)\n",
    "gbrt.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt_best = GradientBoostingRegressor(\n",
    "    max_depth=2, learning_rate=0.05, n_estimators=500,\n",
    "    n_iter_no_change=10, random_state=42)\n",
    "gbrt_best.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt_best.n_estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plot_predictions([gbrt], X, y, axes=[-0.5, 0.5, -0.1, 0.8], style=\"r-\",\n",
    "                 label=\"Ensemble predictions\")\n",
    "plt.title(f\"learning_rate={gbrt.learning_rate}, \"\n",
    "          f\"n_estimators={gbrt.n_estimators_}\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$y$\", rotation=0)\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plot_predictions([gbrt_best], X, y, axes=[-0.5, 0.5, -0.1, 0.8], style=\"r-\")\n",
    "plt.title(f\"learning_rate={gbrt_best.learning_rate}, \"\n",
    "          f\"n_estimators={gbrt_best.n_estimators_}\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Key Formulas\n",
    "\n",
    "#### 1. Objective Function\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = \\sum_{i=1}^{N} l(y_i, \\hat{y}_i) + \\sum_{t=1}^{T} \\Omega(f_t)\n",
    "$$\n",
    "where:\n",
    "- $ l(y_i, \\hat{y}_i) $ is the loss function (e.g., squared error, logistic loss).\n",
    "- $ \\Omega(f_t) $ is the regularization term to control model complexity.\n",
    "\n",
    "#### 2. Regularization Term\n",
    "$$\n",
    "\\Omega(f_t) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j} w_j^2\n",
    "$$\n",
    "where:\n",
    "- $ T $ is the number of leaves in the tree.\n",
    "- $ w_j $ are the leaf weights.\n",
    "- $ \\gamma $ is the pruning penalty.\n",
    "- $ \\lambda $ is the L2 regularization term.\n",
    "\n",
    "#### 3. Gain (Split Quality Score)\n",
    "$$\n",
    "Gain = \\frac{1}{2} \\left[ \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{(G_L + G_R)^2}{H_L + H_R + \\lambda} \\right] - \\gamma\n",
    "$$\n",
    "where:\n",
    "- $ G $ (gradient) and $ H $ (Hessian) are computed from second-order Taylor expansion.\n",
    "- $ L $ and $ R $ refer to left and right child nodes.\n",
    "- $ \\gamma $ controls pruning.\n",
    "\n",
    "#### 4. Leaf Weight Calculation\n",
    "$$\n",
    "w^* = -\\frac{G}{H + \\lambda}\n",
    "$$\n",
    "where:\n",
    "- $ G $ is the sum of gradients for the leaf.\n",
    "- $ H $ is the sum of second-order gradients (Hessian).\n",
    "- $ \\lambda $ is the L2 regularization term.\n",
    "\n",
    "#### 5. Tree Structure Score (Optimal Tree)\n",
    "$$\n",
    "\\mathcal{L}_{tree} = -\\frac{1}{2} \\sum_{j=1}^{T} \\frac{G_j^2}{H_j + \\lambda} + \\gamma T\n",
    "$$\n",
    "where:\n",
    "- $ T $ is the number of leaves.\n",
    "- This balances predictive power with complexity.\n",
    "\n",
    "#### 6. Final Prediction\n",
    "$$\n",
    "\\hat{y}_i = \\sum_{t=1}^{T} f_t(x_i)\n",
    "$$\n",
    "where:\n",
    "- Each tree contributes an additive prediction to improve the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "bst = XGBClassifier(n_estimators=30, max_depth=2, learning_rate=0.3, objective='binary:logistic')\n",
    "# fit model\n",
    "bst.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_prob = bst.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)  # Convert probabilities to binary labels\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(bst, X, y)\n",
    "# plt.title(clf.__class__.__name__, fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM\n",
    "- https://lightgbm.readthedocs.io/en/stable/pythonapi/lightgbm.LGBMClassifier.html#lightgbm.LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lgbm = lgb.LGBMClassifier()\n",
    "lgbm.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', LogisticRegression(random_state=42)),\n",
    "        ('rf', RandomForestClassifier(random_state=42)),\n",
    "        ('svc', SVC(probability=True, random_state=42))\n",
    "    ],\n",
    "    final_estimator=RandomForestClassifier(random_state=43),\n",
    "    cv=5  # number of cross-validation folds\n",
    ")\n",
    "stacking_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking_clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Get individual base estimators\n",
    "base_models = stacking_clf.estimators_\n",
    "names = [\n",
    "    'LogisticRegression',\n",
    "    'RandomForestRegressor',\n",
    "    'SupportVectorClassifier'\n",
    "]\n",
    "\n",
    "# Evaluate each base model\n",
    "for name, model in zip(names, base_models):\n",
    "    y_pred = model.predict(X_test)  # Make predictions\n",
    "    acc = accuracy_score(y_test, y_pred)  # Compute accuracy\n",
    "    print(f\"Accuracy of {name}: {acc:.4f}\")\n",
    "\n",
    "# Evaluate the final stacked model\n",
    "stacked_acc = accuracy_score(y_test, stacking_clf.predict(X_test))\n",
    "print(f\"Accuracy of StackingClassifier (Final Model): {stacked_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
