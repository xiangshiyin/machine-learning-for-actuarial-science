{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(**Click the icon below to open this notebook in Colab**)\n",
    "\n",
    "[![Open InColab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/xiangshiyin/machine-learning-for-actuarial-science/blob/main/2025-spring/week14/notebook/demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Serving\n",
    "\n",
    "## `Pickle`\n",
    "Serialize a model for later usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1 - Save a Python object to local storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "data = {\n",
    "    'name': 'John Doe',\n",
    "    'age': 30,\n",
    "    'city': 'New York',\n",
    "    'occupation': 'Software Engineer'\n",
    "}\n",
    "\n",
    "# Save this dictionary to a file\n",
    "with open('data.pickle', 'wb') as f:\n",
    "    pickle.dump(data, f)\n",
    "\n",
    "print(\"Data saved to file data.pickle.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictionary back from the pickle file\n",
    "with open('data.pickle', 'rb') as f:\n",
    "    data2 = pickle.load(f)\n",
    "print(\"Dictionary loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 == data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id(data2), id(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2 - Save a ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Dataset loaded and split successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a RandomForest classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to a file\n",
    "with open(\"iris_model.pkl\", \"wb\") as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "print(\"Model saved successfully as 'iris_model.pkl'!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with the in-memory model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "# Print the accuracy\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with the saved model\n",
    "with open('iris_model.pkl', 'rb') as f:\n",
    "    model2 = pickle.load(f)\n",
    "\n",
    "y_pred2 = model2.predict(X_test)\n",
    "accuracy2 = accuracy_score(y_test, y_pred2)\n",
    "print(\"Accuracy:\", accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single data point (sepal length, sepal width, petal length, petal width)\n",
    "single_data_point = [[5.1, 3.5, 1.4, 0.2]]\n",
    "\n",
    "# Predict class for this single data point\n",
    "single_prediction = model.predict(single_data_point)\n",
    "\n",
    "# Get class name from iris dataset\n",
    "predicted_class = iris.target_names[single_prediction[0]]\n",
    "\n",
    "print(f\"Predicted Class for the Single Data Point: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class for this single data point\n",
    "single_prediction2 = model.predict(single_data_point)\n",
    "\n",
    "# Get class name from iris dataset\n",
    "predicted_class2 = iris.target_names[single_prediction2[0]]\n",
    "\n",
    "print(f\"Predicted Class for the Single Data Point: {predicted_class2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `FastAPI`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Load trained model\n",
    "with open(\"iris_model.pkl\", \"rb\") as file:\n",
    "    model = pickle.load(file)\n",
    "\n",
    "\n",
    "# Define the expected request format\n",
    "class FeaturesInput(BaseModel):\n",
    "    features: list[float]\n",
    "\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "def predict(data: FeaturesInput):\n",
    "    prediction = model.predict([data.features])\n",
    "    return {\"prediction\": int(prediction[0])}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run command `uvicorn fastapi_demo:app --host 0.0.0.0 --port 8000 --reload` to expose the API endpoints.\n",
    "- Or run `fastapi dev fastapi_demo.py --reload`\n",
    "Run `curl` command below to test the API\n",
    "```\n",
    "curl -X POST \"http://127.0.0.1:8000/predict\" \\\n",
    "      -H \"Content-Type: application/json\" \\\n",
    "      -d '{\"features\": [5.1, 3.5, 1.4, 0.2]}'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Streamlit` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Load model\n",
    "with open(\"iris_model.pkl\", \"rb\") as file:\n",
    "    model = pickle.load(file)\n",
    "\n",
    "st.title(\"Iris Flower Classifier\")\n",
    "\n",
    "# User input\n",
    "sepal_length = st.number_input(\"Sepal Length\")\n",
    "sepal_width = st.number_input(\"Sepal Width\")\n",
    "petal_length = st.number_input(\"Petal Length\")\n",
    "petal_width = st.number_input(\"Petal Width\")\n",
    "\n",
    "if st.button(\"Predict\"):\n",
    "    features = np.array([[sepal_length, sepal_width, petal_length, petal_width]])\n",
    "    prediction = model.predict(features)[0]\n",
    "    st.write(f\"Predicted Class: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `streamlit run streamlit_demo.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/xiangshiyin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/xiangshiyin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/xiangshiyin/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "data = \"This is a simple example to demonstrate removing stopwords using NLTK.\"\n",
    "stopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = word_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: This is a simple example to demonstrate removing stopwords using NLTK.\n",
      "Tokenized text: This|is|a|simple|example|to|demonstrate|removing|stopwords|using|NLTK|.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original text: {data}\")\n",
    "print(f\"Tokenized text: {\"|\".join(tokenized_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing stopwords: ['This', 'simple', 'example', 'demonstrate', 'removing', 'stopwords', 'using', 'NLTK', '.']\n"
     ]
    }
   ],
   "source": [
    "filtered_tokenized_data = [\n",
    "    w\n",
    "    for w in tokenized_data\n",
    "    if w not in stopWords\n",
    "]\n",
    "print(f\"After removing stopwords: {filtered_tokenized_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: This is a simple example to demonstrate removing stopwords using NLTK.\n",
      "Tokenized text: This|is|a|simple|example|to|demonstrate|removing|stopwords|using|NLTK|.\n",
      "After removing stopwords: This|simple|example|demonstrate|removing|stopwords|using|NLTK|.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original text: {data}\")\n",
    "print(f\"Tokenized text: {\"|\".join(tokenized_data)}\")\n",
    "print(f\"After removing stopwords: {\"|\".join(filtered_tokenized_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names (Vocabulary):\n",
      "['all' 'amazing' 'at' 'ever' 'experience' 'happy' 'hate' 'is' 'love' 'not'\n",
      " 'product' 'result' 'satisfied' 'the' 'this' 'very' 'with' 'worst']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Sample dataset\n",
    "texts = [\n",
    "    \"I love this product\",         # positive\n",
    "    \"This is amazing\",             # positive\n",
    "    \"Very happy with the result\",  # positive\n",
    "    \"I hate this\",                 # negative\n",
    "    \"Worst experience ever\",       # negative\n",
    "    \"Not satisfied at all\"         # negative\n",
    "]\n",
    "\n",
    "labels = [1, 1, 1, 0, 0, 0]  # 1 = positive, 0 = negative\n",
    "\n",
    "# 2. Convert text to bag-of-words vectors\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# 3. Show feature names\n",
    "print(\"Feature Names (Vocabulary):\")\n",
    "print(vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/xiangshiyin/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8275\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.81      0.82       199\n",
      "           1       0.82      0.84      0.83       201\n",
      "\n",
      "    accuracy                           0.83       400\n",
      "   macro avg       0.83      0.83      0.83       400\n",
      "weighted avg       0.83      0.83      0.83       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import nltk\n",
    "\n",
    "# Download NLTK movie_reviews data\n",
    "nltk.download('movie_reviews')\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# Prepare dataset\n",
    "docs = []\n",
    "labels = []\n",
    "\n",
    "for fileid in movie_reviews.fileids():\n",
    "    docs.append(movie_reviews.raw(fileid))\n",
    "    labels.append(movie_reviews.categories(fileid)[0])  # 'pos' or 'neg'\n",
    "\n",
    "# Convert labels to binary format\n",
    "y = [1 if label == 'pos' else 0 for label in labels]\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(docs, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train classifier\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '0009f', '007', '00s', '03', '04', '05', '05425',\n",
       "       '10', '100', '1000', '10000', '100m', '101', '102', '103', '104',\n",
       "       '105', '106'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "feature_names[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top TF-IDF features in sample test document:\n",
      "                 tfidf\n",
      "bates         0.401204\n",
      "annie         0.253364\n",
      "caan          0.169454\n",
      "kathy         0.148969\n",
      "sledgehammer  0.127790\n",
      "realises      0.117562\n",
      "french        0.114832\n",
      "nerve         0.107711\n",
      "cake          0.104894\n",
      "misery        0.102454\n",
      "masterful     0.100301\n",
      "basically     0.093828\n",
      "prisoner      0.092226\n",
      "arm           0.088148\n",
      "tense         0.086170\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Choose a sample document from the test set\n",
    "sample_idx = 0\n",
    "sample_vector = X_test_tfidf[sample_idx]\n",
    "\n",
    "# Convert sparse vector to dense and create DataFrame\n",
    "df_features = pd.DataFrame(\n",
    "    data=sample_vector.toarray()[0],\n",
    "    index=feature_names,\n",
    "    columns=[\"tfidf\"]\n",
    ")\n",
    "\n",
    "# Filter non-zero features and sort\n",
    "df_nonzero = df_features[df_features.tfidf > 0].sort_values(by=\"tfidf\", ascending=False)\n",
    "\n",
    "# Show top 15 features by TF-IDF weight\n",
    "print(\"\\nTop TF-IDF features in sample test document:\")\n",
    "print(df_nonzero.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hand-craft implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Sample corpus\n",
    "corpus = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# Preprocessing: Tokenization and vocabulary building\n",
    "tokens = re.findall(r'\\b\\w+\\b', corpus.lower())\n",
    "vocab = set(tokens)\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dog': 0,\n",
       " 'brown': 1,\n",
       " 'fox': 2,\n",
       " 'over': 3,\n",
       " 'lazy': 4,\n",
       " 'the': 5,\n",
       " 'jumps': 6,\n",
       " 'quick': 7}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'dog',\n",
       " 1: 'brown',\n",
       " 2: 'fox',\n",
       " 3: 'over',\n",
       " 4: 'lazy',\n",
       " 5: 'the',\n",
       " 6: 'jumps',\n",
       " 7: 'quick'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data\n",
    "def generate_training_data(tokens, window_size):\n",
    "    training_data = []\n",
    "    for idx, target_word in enumerate(tokens):\n",
    "        target_idx = word_to_idx[target_word]\n",
    "        context_range = list(range(max(0, idx - window_size), idx)) + \\\n",
    "                        list(range(idx + 1, min(len(tokens), idx + window_size + 1)))\n",
    "        for context_idx in context_range:\n",
    "            context_word = tokens[context_idx]\n",
    "            context_word_idx = word_to_idx[context_word]\n",
    "            training_data.append((target_idx, context_word_idx))\n",
    "    return training_data\n",
    "\n",
    "window_size = 2\n",
    "training_data = generate_training_data(tokens, window_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: The quick brown fox jumps over the lazy dog\n",
      "[('the', 'quick'), ('the', 'brown'), ('quick', 'the'), ('quick', 'brown'), ('quick', 'fox'), ('brown', 'the'), ('brown', 'quick'), ('brown', 'fox'), ('brown', 'jumps'), ('fox', 'quick'), ('fox', 'brown'), ('fox', 'jumps'), ('fox', 'over'), ('jumps', 'brown'), ('jumps', 'fox'), ('jumps', 'over'), ('jumps', 'the'), ('over', 'fox'), ('over', 'jumps'), ('over', 'the'), ('over', 'lazy'), ('the', 'jumps'), ('the', 'over'), ('the', 'lazy'), ('the', 'dog'), ('lazy', 'over'), ('lazy', 'the'), ('lazy', 'dog'), ('dog', 'the'), ('dog', 'lazy')]\n"
     ]
    }
   ],
   "source": [
    "# Inspect the training data\n",
    "print(f\"Corpus: {corpus}\")\n",
    "print([\n",
    "    (idx_to_word[t[0]], idx_to_word[t[1]])\n",
    "    for t in training_data\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 41.6399\n",
      "Epoch 200, Loss: 43.7115\n",
      "Epoch 300, Loss: 46.7961\n",
      "Epoch 400, Loss: 40.2588\n",
      "Epoch 500, Loss: 41.3046\n",
      "Epoch 600, Loss: 36.7148\n",
      "Epoch 700, Loss: 42.1316\n",
      "Epoch 800, Loss: 45.5240\n",
      "Epoch 900, Loss: 40.6802\n",
      "Epoch 1000, Loss: 43.0892\n"
     ]
    }
   ],
   "source": [
    "# Initialize parameters\n",
    "embedding_dim = 10\n",
    "W1 = np.random.randn(vocab_size, embedding_dim)\n",
    "W2 = np.random.randn(embedding_dim, vocab_size)\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Training parameters\n",
    "epochs = 1000\n",
    "learning_rate = 0.01\n",
    "num_negative_samples = 2\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    for target_idx, context_idx in training_data:\n",
    "        # Positive sample\n",
    "        h = W1[target_idx]\n",
    "        u = np.dot(h, W2[:, context_idx])\n",
    "        pred = sigmoid(u)\n",
    "        error = pred - 1\n",
    "        loss += -np.log(pred + 1e-7)\n",
    "        # Gradients\n",
    "        grad_W2 = error * h\n",
    "        grad_W1 = error * W2[:, context_idx]\n",
    "        # Update weights\n",
    "        W2[:, context_idx] -= learning_rate * grad_W2\n",
    "        W1[target_idx] -= learning_rate * grad_W1\n",
    "\n",
    "        # Negative sampling\n",
    "        negative_samples = random.sample([i for i in range(vocab_size) if i != context_idx], num_negative_samples)\n",
    "        for neg_idx in negative_samples:\n",
    "            u_neg = np.dot(h, W2[:, neg_idx])\n",
    "            pred_neg = sigmoid(u_neg)\n",
    "            error_neg = pred_neg\n",
    "            loss += -np.log(1 - pred_neg + 1e-7)\n",
    "            # Gradients\n",
    "            grad_W2_neg = error_neg * h\n",
    "            grad_W1_neg = error_neg * W2[:, neg_idx]\n",
    "            # Update weights\n",
    "            W2[:, neg_idx] -= learning_rate * grad_W2_neg\n",
    "            W1[target_idx] -= learning_rate * grad_W1_neg\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Words similar to 'fox':\n",
      "jumps: 0.4859\n",
      "dog: 0.0744\n",
      "lazy: 0.0424\n"
     ]
    }
   ],
   "source": [
    "# Retrieve word embeddings\n",
    "word_embeddings = W1\n",
    "\n",
    "# Example: Find similar words\n",
    "def find_similar(word, top_n=3):\n",
    "    if word not in word_to_idx:\n",
    "        print(f\"'{word}' not in vocabulary.\")\n",
    "        return\n",
    "    idx = word_to_idx[word]\n",
    "    vec = word_embeddings[idx]\n",
    "    similarities = []\n",
    "    for i in range(vocab_size):\n",
    "        if i == idx:\n",
    "            continue\n",
    "        sim = np.dot(vec, word_embeddings[i]) / (np.linalg.norm(vec) * np.linalg.norm(word_embeddings[i]))\n",
    "        similarities.append((idx_to_word[i], sim))\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    for word, sim in similarities[:top_n]:\n",
    "        print(f\"{word}: {sim:.4f}\")\n",
    "\n",
    "# Test the model\n",
    "print(\"\\nWords similar to 'fox':\")\n",
    "find_similar('fox')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With `Gensim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample corpus\n",
    "sentences = [\n",
    "    [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"],\n",
    "    [\"i\", \"love\", \"natural\", \"language\", \"processing\"],\n",
    "    [\"word2vec\", \"is\", \"a\", \"technique\", \"for\", \"natural\", \"language\", \"processing\"],\n",
    "    [\"the\", \"dog\", \"is\", \"lazy\", \"but\", \"the\", \"brown\", \"fox\", \"is\", \"quick\"]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the model\n",
    "model = Word2Vec(\n",
    "    sentences,\n",
    "    vector_size=100,  # Dimensionality of the word vectors\n",
    "    window=5,         # Maximum distance between the current and predicted word\n",
    "    min_count=1,      # Ignores all words with total frequency lower than this\n",
    "    workers=4,        # Use these many worker threads to train the model\n",
    "    sg=1              # 1 for Skip-gram; 0 for CBOW\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('over', 0.16699621081352234), ('brown', 0.1388736069202423), ('quick', 0.13150502741336823)]\n",
      "Similarity between 'dog' and 'fox': -0.1052\n"
     ]
    }
   ],
   "source": [
    "# Find most similar words\n",
    "similar_words = model.wv.most_similar(\"fox\", topn=3)\n",
    "print(similar_words)\n",
    "\n",
    "# Compute similarity between two words\n",
    "similarity = model.wv.similarity(\"dog\", \"fox\")\n",
    "print(f\"Similarity between 'dog' and 'fox': {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"\"\"\n",
    "Mr. Dursley was the director of a firm called Grunnings, which made\n",
    "drills. He was a big, beefy man with hardly any neck, although he did\n",
    "have a very large mustache. Mrs. Dursley was thin and blonde and had\n",
    "nearly twice the usual amount of neck, which came in very useful as she\n",
    "spent so much of her time craning over garden fences, spying on the\n",
    "neighbors. The Dursleys had a small son called Dudley and in their\n",
    "opinion there was no finer boy anywhere.\n",
    "\n",
    "\n",
    "The Dursleys had everything they wanted, but they also had a secret, and\n",
    "their greatest fear was that somebody would discover it. They didn't\n",
    "think they could bear it if anyone found out about the Potters. Mrs.\n",
    "Potter was Mrs. Dursley's sister, but they hadn't met for several years;\n",
    "in fact, Mrs. Dursley pretended she didn't have a sister, because her\n",
    "sister and her good-for-nothing husband were as unDursleyish as it was\n",
    "possible to be. The Dursleys shuddered to think what the neighbors would\n",
    "say if the Potters arrived in the street. The Dursleys knew that the\n",
    "Potters had a small son, too, but they had never even seen him. This boy\n",
    "was another good reason for keeping the Potters away; they didn't want\n",
    "Dudley mixing with a child like that.\n",
    "\n",
    "\n",
    "When Mr. and Mrs. Dursley woke up on the dull, gray Tuesday our story\n",
    "starts, there was nothing about the cloudy sky outside to suggest that\n",
    "strange and mysterious things would soon be happening all over the\n",
    "country. Mr. Dursley hummed as he picked out his most boring tie for\n",
    "work, and Mrs. Dursley gossiped away happily as she wrestled a screaming\n",
    "Dudley into his high chair.\n",
    "\"\"\"\n",
    "\n",
    "sentences = [\n",
    "    gensim.utils.simple_preprocess(sentence)\n",
    "    for sentence in sample.split(\"\\n\\n\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(\n",
    "    sentences,\n",
    "    vector_size=100,  # Dimensionality of the word vectors\n",
    "    window=5,         # Maximum distance between the current and predicted word\n",
    "    min_count=1,      # Ignores all words with total frequency lower than this\n",
    "    workers=4,        # Use these many worker threads to train the model\n",
    "    sg=1              # 1 for Skip-gram; 0 for CBOW\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dursley', 0.2640940845012665), ('on', 0.2523151636123657), ('starts', 0.24110931158065796)]\n"
     ]
    }
   ],
   "source": [
    "# Find most similar words\n",
    "similar_words = model.wv.most_similar(\"potter\", topn=3)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
